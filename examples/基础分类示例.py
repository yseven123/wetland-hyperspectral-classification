#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ç³»ç»Ÿ - åŸºç¡€åˆ†ç±»ç¤ºä¾‹
Wetland Hyperspectral Classification System - Basic Classification Example

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ç³»ç»Ÿè¿›è¡ŒåŸºç¡€çš„åˆ†ç±»ä»»åŠ¡ï¼Œ
åŒ…æ‹¬æ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€ç‰¹å¾æå–ã€æ¨¡å‹è®­ç»ƒå’Œç»“æœè¯„ä¼°çš„å®Œæ•´æµç¨‹ã€‚

ä½œè€…: ç ”ç©¶å›¢é˜Ÿ
æ—¥æœŸ: 2024-06-30
ç‰ˆæœ¬: 1.0.0
"""

import os
import sys
import warnings
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import time
import logging

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå¿½ç•¥è­¦å‘Š
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¹¿åœ°åˆ†ç±»ç³»ç»Ÿæ¨¡å—
try:
    from wetland_classification import Pipeline
    from wetland_classification.config import Config
    from wetland_classification.data import DataLoader
    from wetland_classification.preprocessing import Preprocessor
    from wetland_classification.features import FeatureExtractor
    from wetland_classification.classification import Classifier
    from wetland_classification.evaluation import ModelEvaluator
    from wetland_classification.utils.visualization import Visualizer
    from wetland_classification.utils.logger import get_logger
    from wetland_classification.utils.io_utils import IOUtils
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…æ¹¿åœ°åˆ†ç±»ç³»ç»Ÿ")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logger = get_logger(__name__)

def setup_directories():
    """
    è®¾ç½®å¿…è¦çš„ç›®å½•ç»“æ„
    """
    directories = [
        'output/basic_classification',
        'output/basic_classification/results',
        'output/basic_classification/figures',
        'output/basic_classification/models',
        'logs'
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        logger.info(f"åˆ›å»ºç›®å½•: {directory}")

def check_data_availability():
    """
    æ£€æŸ¥æ¼”ç¤ºæ•°æ®æ˜¯å¦å¯ç”¨
    """
    demo_data_path = 'data/samples/demo_scene'
    
    if not os.path.exists(demo_data_path):
        logger.warning(f"æ¼”ç¤ºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {demo_data_path}")
        logger.info("æ­£åœ¨åˆ›å»ºæ¨¡æ‹Ÿæ¼”ç¤ºæ•°æ®...")
        create_demo_data()
    else:
        logger.info(f"æ‰¾åˆ°æ¼”ç¤ºæ•°æ®: {demo_data_path}")

def create_demo_data():
    """
    åˆ›å»ºæ¨¡æ‹Ÿçš„æ¼”ç¤ºæ•°æ®ç”¨äºæµ‹è¯•
    """
    logger.info("å¼€å§‹åˆ›å»ºæ¨¡æ‹Ÿæ¼”ç¤ºæ•°æ®...")
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®ç›®å½•
    demo_dir = Path('data/samples/demo_scene')
    demo_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿé«˜å…‰è°±æ•°æ®
    np.random.seed(42)
    height, width, bands = 128, 128, 100
    
    # æ¨¡æ‹Ÿä¸åŒçš„åœ°ç‰©ç±»å‹
    hyperspectral_data = np.zeros((height, width, bands))
    labels = np.zeros((height, width), dtype=int)
    
    # åˆ›å»ºä¸åŒåŒºåŸŸçš„å…‰è°±ç‰¹å¾
    for i in range(height):
        for j in range(width):
            # æ ¹æ®ä½ç½®ç¡®å®šç±»åˆ«
            if i < height // 3:  # æ°´ä½“åŒºåŸŸ
                class_id = 1
                base_spectrum = np.random.normal(0.1, 0.02, bands)
                base_spectrum[40:60] = np.random.normal(0.05, 0.01, 20)  # æ°´ä½“ä½åå°„
            elif i < 2 * height // 3:  # æ¤è¢«åŒºåŸŸ
                class_id = 2
                base_spectrum = np.random.normal(0.3, 0.05, bands)
                base_spectrum[60:80] = np.random.normal(0.7, 0.1, 20)  # æ¤è¢«çº¢è¾¹
            else:  # åœŸå£¤åŒºåŸŸ
                class_id = 3
                base_spectrum = np.random.normal(0.4, 0.08, bands)
            
            # æ·»åŠ å™ªå£°
            noise = np.random.normal(0, 0.01, bands)
            hyperspectral_data[i, j, :] = np.clip(base_spectrum + noise, 0, 1)
            labels[i, j] = class_id
    
    # ä¿å­˜æ¨¡æ‹Ÿæ•°æ®
    np.save(demo_dir / 'hyperspectral_data.npy', hyperspectral_data)
    np.save(demo_dir / 'ground_truth.npy', labels)
    
    # åˆ›å»ºæ³¢é•¿ä¿¡æ¯
    wavelengths = np.linspace(400, 2500, bands)
    np.save(demo_dir / 'wavelengths.npy', wavelengths)
    
    # åˆ›å»ºç±»åˆ«ä¿¡æ¯
    class_info = {
        1: {'name': 'æ°´ä½“', 'color': '#0000FF'},
        2: {'name': 'æ¤è¢«', 'color': '#00FF00'},
        3: {'name': 'åœŸå£¤', 'color': '#8B4513'}
    }
    
    import json
    with open(demo_dir / 'class_info.json', 'w', encoding='utf-8') as f:
        json.dump(class_info, f, ensure_ascii=False, indent=2)
    
    logger.info(f"æ¨¡æ‹Ÿæ¼”ç¤ºæ•°æ®åˆ›å»ºå®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {hyperspectral_data.shape}")

def basic_classification_workflow():
    """
    åŸºç¡€åˆ†ç±»å·¥ä½œæµç¨‹ç¤ºä¾‹
    """
    logger.info("="*60)
    logger.info("å¼€å§‹æ¹¿åœ°é«˜å…‰è°±åŸºç¡€åˆ†ç±»ç¤ºä¾‹")
    logger.info("="*60)
    
    start_time = time.time()
    
    try:
        # æ­¥éª¤1: åˆ›å»ºé»˜è®¤é…ç½®
        logger.info("æ­¥éª¤1: åˆ›å»ºé…ç½®")
        config = create_basic_config()
        logger.info("é…ç½®åˆ›å»ºå®Œæˆ")
        
        # æ­¥éª¤2: æ•°æ®åŠ è½½
        logger.info("æ­¥éª¤2: åŠ è½½æ•°æ®")
        hyperspectral_data, labels, wavelengths, class_info = load_demo_data()
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ - æ•°æ®å½¢çŠ¶: {hyperspectral_data.shape}, ç±»åˆ«æ•°: {len(np.unique(labels[labels>0]))}")
        
        # æ­¥éª¤3: æ•°æ®é¢„å¤„ç†
        logger.info("æ­¥éª¤3: æ•°æ®é¢„å¤„ç†")
        processed_data = preprocess_data(hyperspectral_data)
        logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        # æ­¥éª¤4: ç‰¹å¾æå–
        logger.info("æ­¥éª¤4: ç‰¹å¾æå–")
        features, feature_names = extract_features(processed_data, wavelengths)
        logger.info(f"ç‰¹å¾æå–å®Œæˆ - ç‰¹å¾ç»´åº¦: {features.shape[1]}")
        
        # æ­¥éª¤5: å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("æ­¥éª¤5: å‡†å¤‡è®­ç»ƒæ•°æ®")
        X_train, X_test, y_train, y_test, train_indices, test_indices = prepare_training_data(
            features, labels
        )
        logger.info(f"æ•°æ®åˆ’åˆ†å®Œæˆ - è®­ç»ƒæ ·æœ¬: {len(X_train)}, æµ‹è¯•æ ·æœ¬: {len(X_test)}")
        
        # æ­¥éª¤6: æ¨¡å‹è®­ç»ƒ
        logger.info("æ­¥éª¤6: æ¨¡å‹è®­ç»ƒ")
        trained_models = train_multiple_models(X_train, y_train, X_test, y_test)
        logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ - å…±è®­ç»ƒ {len(trained_models)} ä¸ªæ¨¡å‹")
        
        # æ­¥éª¤7: æ¨¡å‹è¯„ä¼°
        logger.info("æ­¥éª¤7: æ¨¡å‹è¯„ä¼°")
        evaluation_results = evaluate_models(trained_models, X_test, y_test, class_info)
        
        # æ­¥éª¤8: é€‰æ‹©æœ€ä½³æ¨¡å‹è¿›è¡Œåœºæ™¯åˆ†ç±»
        logger.info("æ­¥éª¤8: åœºæ™¯åˆ†ç±»")
        best_model_name = max(evaluation_results, key=lambda x: evaluation_results[x]['accuracy'])
        best_model = trained_models[best_model_name]
        logger.info(f"é€‰æ‹©æœ€ä½³æ¨¡å‹: {best_model_name}")
        
        classification_map = classify_full_scene(best_model, features, hyperspectral_data.shape[:2])
        
        # æ­¥éª¤9: ç»“æœå¯è§†åŒ–
        logger.info("æ­¥éª¤9: ç»“æœå¯è§†åŒ–")
        create_visualizations(
            hyperspectral_data, labels, classification_map, 
            evaluation_results, wavelengths, class_info
        )
        
        # æ­¥éª¤10: ä¿å­˜ç»“æœ
        logger.info("æ­¥éª¤10: ä¿å­˜ç»“æœ")
        save_results(
            classification_map, evaluation_results, trained_models, 
            config, class_info
        )
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        total_time = time.time() - start_time
        display_final_results(evaluation_results, total_time)
        
    except Exception as e:
        logger.error(f"åŸºç¡€åˆ†ç±»æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
        raise

def create_basic_config():
    """
    åˆ›å»ºåŸºç¡€é…ç½®
    """
    config = {
        'data': {
            'input_path': 'data/samples/demo_scene',
            'output_path': 'output/basic_classification',
            'file_format': 'npy'
        },
        'preprocessing': {
            'normalization': True,
            'noise_reduction': False,
            'bad_bands_removal': False
        },
        'features': {
            'spectral_features': True,
            'vegetation_indices': ['NDVI', 'NDWI', 'EVI'],
            'texture_features': False,  # ç®€åŒ–ç¤ºä¾‹ï¼Œä¸ä½¿ç”¨çº¹ç†ç‰¹å¾
            'spatial_features': False
        },
        'classification': {
            'models': ['svm', 'random_forest', 'xgboost'],
            'test_ratio': 0.3,
            'random_state': 42
        },
        'evaluation': {
            'metrics': ['accuracy', 'kappa', 'f1_score'],
            'cross_validation': False  # ç®€åŒ–ç¤ºä¾‹
        }
    }
    
    return config

def load_demo_data():
    """
    åŠ è½½æ¼”ç¤ºæ•°æ®
    """
    demo_dir = Path('data/samples/demo_scene')
    
    # åŠ è½½é«˜å…‰è°±æ•°æ®
    hyperspectral_data = np.load(demo_dir / 'hyperspectral_data.npy')
    
    # åŠ è½½æ ‡ç­¾
    labels = np.load(demo_dir / 'ground_truth.npy')
    
    # åŠ è½½æ³¢é•¿ä¿¡æ¯
    wavelengths = np.load(demo_dir / 'wavelengths.npy')
    
    # åŠ è½½ç±»åˆ«ä¿¡æ¯
    import json
    with open(demo_dir / 'class_info.json', 'r', encoding='utf-8') as f:
        class_info = json.load(f)
    
    # è½¬æ¢ç±»åˆ«ä¿¡æ¯çš„é”®ä¸ºæ•´æ•°
    class_info = {int(k): v for k, v in class_info.items()}
    
    return hyperspectral_data, labels, wavelengths, class_info

def preprocess_data(hyperspectral_data):
    """
    æ•°æ®é¢„å¤„ç†
    """
    # ç®€å•çš„å½’ä¸€åŒ–é¢„å¤„ç†
    processed_data = np.copy(hyperspectral_data)
    
    # æŒ‰æ³¢æ®µè¿›è¡Œå½’ä¸€åŒ–
    for band in range(processed_data.shape[2]):
        band_data = processed_data[:, :, band]
        band_min = np.min(band_data)
        band_max = np.max(band_data)
        
        if band_max > band_min:
            processed_data[:, :, band] = (band_data - band_min) / (band_max - band_min)
        else:
            processed_data[:, :, band] = 0
    
    logger.info("æ•°æ®å½’ä¸€åŒ–å®Œæˆ")
    return processed_data

def extract_features(hyperspectral_data, wavelengths):
    """
    ç‰¹å¾æå–
    """
    height, width, bands = hyperspectral_data.shape
    
    # é‡å¡‘æ•°æ®ä¸º2Dæ ¼å¼ (n_pixels, n_bands)
    reshaped_data = hyperspectral_data.reshape(-1, bands)
    
    features = []
    feature_names = []
    
    # 1. åŸå§‹å…‰è°±ç‰¹å¾ï¼ˆä¸‹é‡‡æ ·ï¼‰
    spectral_step = max(1, bands // 20)  # é€‰æ‹©çº¦20ä¸ªå…‰è°±æ³¢æ®µ
    selected_bands = range(0, bands, spectral_step)
    spectral_features = reshaped_data[:, selected_bands]
    features.append(spectral_features)
    feature_names.extend([f'Band_{wavelengths[i]:.0f}nm' for i in selected_bands])
    
    # 2. æ¤è¢«æŒ‡æ•°
    vegetation_indices = calculate_vegetation_indices(reshaped_data, wavelengths)
    features.append(vegetation_indices)
    feature_names.extend(['NDVI', 'NDWI', 'EVI', 'SAVI'])
    
    # 3. å…‰è°±ç»Ÿè®¡ç‰¹å¾
    spectral_stats = calculate_spectral_statistics(reshaped_data)
    features.append(spectral_stats)
    feature_names.extend(['Mean', 'Std', 'Skewness', 'Kurtosis'])
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    all_features = np.column_stack(features)
    
    logger.info(f"ç‰¹å¾æå–å®Œæˆ - æ€»ç‰¹å¾æ•°: {all_features.shape[1]}")
    
    return all_features, feature_names

def calculate_vegetation_indices(data, wavelengths):
    """
    è®¡ç®—æ¤è¢«æŒ‡æ•°
    """
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ³¢æ®µ
    def find_nearest_band(target_wavelength):
        return np.argmin(np.abs(wavelengths - target_wavelength))
    
    # å®šä¹‰å…³é”®æ³¢æ®µ
    red_band = find_nearest_band(670)     # çº¢å…‰
    nir_band = find_nearest_band(850)     # è¿‘çº¢å¤–
    blue_band = find_nearest_band(470)    # è“å…‰
    swir_band = find_nearest_band(1600)   # çŸ­æ³¢çº¢å¤–
    
    red = data[:, red_band]
    nir = data[:, nir_band]
    blue = data[:, blue_band]
    swir = data[:, swir_band]
    
    # é¿å…é™¤é›¶é”™è¯¯
    epsilon = 1e-8
    
    # NDVI
    ndvi = (nir - red) / (nir + red + epsilon)
    
    # NDWI
    ndwi = (nir - swir) / (nir + swir + epsilon)
    
    # EVI
    evi = 2.5 * (nir - red) / (nir + 6 * red - 7.5 * blue + 1 + epsilon)
    
    # SAVI
    L = 0.5  # åœŸå£¤è°ƒèŠ‚å› å­
    savi = (1 + L) * (nir - red) / (nir + red + L + epsilon)
    
    return np.column_stack([ndvi, ndwi, evi, savi])

def calculate_spectral_statistics(data):
    """
    è®¡ç®—å…‰è°±ç»Ÿè®¡ç‰¹å¾
    """
    from scipy import stats
    
    # è®¡ç®—å„ç§ç»Ÿè®¡é‡
    mean_values = np.mean(data, axis=1)
    std_values = np.std(data, axis=1)
    skewness_values = stats.skew(data, axis=1)
    kurtosis_values = stats.kurtosis(data, axis=1)
    
    return np.column_stack([mean_values, std_values, skewness_values, kurtosis_values])

def prepare_training_data(features, labels):
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®
    """
    from sklearn.model_selection import train_test_split
    
    # è·å–æœ‰æ•ˆåƒç´ ï¼ˆéé›¶æ ‡ç­¾ï¼‰
    valid_mask = labels.ravel() > 0
    valid_features = features[valid_mask]
    valid_labels = labels.ravel()[valid_mask]
    
    # æ•°æ®åˆ’åˆ†
    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
        valid_features, valid_labels, 
        np.where(valid_mask)[0],  # åŸå§‹ç´¢å¼•
        test_size=0.3, 
        random_state=42, 
        stratify=valid_labels
    )
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}, æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    
    return X_train, X_test, y_train, y_test, train_idx, test_idx

def train_multiple_models(X_train, y_train, X_test, y_test):
    """
    è®­ç»ƒå¤šä¸ªæ¨¡å‹
    """
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import StandardScaler
    import xgboost as xgb
    
    models = {}
    scalers = {}
    
    # 1. SVM æ¨¡å‹
    logger.info("è®­ç»ƒ SVM æ¨¡å‹...")
    scaler_svm = StandardScaler()
    X_train_scaled = scaler_svm.fit_transform(X_train)
    X_test_scaled = scaler_svm.transform(X_test)
    
    svm_model = SVC(kernel='rbf', C=100, gamma='scale', probability=True, random_state=42)
    svm_model.fit(X_train_scaled, y_train)
    
    models['SVM'] = svm_model
    scalers['SVM'] = scaler_svm
    
    # 2. Random Forest æ¨¡å‹
    logger.info("è®­ç»ƒ Random Forest æ¨¡å‹...")
    rf_model = RandomForestClassifier(
        n_estimators=100, 
        max_depth=10, 
        random_state=42, 
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    
    models['Random Forest'] = rf_model
    scalers['Random Forest'] = None  # RFä¸éœ€è¦æ ‡å‡†åŒ–
    
    # 3. XGBoost æ¨¡å‹
    logger.info("è®­ç»ƒ XGBoost æ¨¡å‹...")
    xgb_model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        n_jobs=-1
    )
    xgb_model.fit(X_train, y_train)
    
    models['XGBoost'] = xgb_model
    scalers['XGBoost'] = None  # XGBoostä¸éœ€è¦æ ‡å‡†åŒ–
    
    # å°†æ ‡å‡†åŒ–å™¨å­˜å‚¨åœ¨æ¨¡å‹ä¸­ä»¥ä¾¿åç»­ä½¿ç”¨
    for model_name in models:
        if hasattr(models[model_name], 'scaler'):
            models[model_name].scaler = scalers[model_name]
        else:
            # ä¸ºæ¨¡å‹å¯¹è±¡æ·»åŠ scalerå±æ€§
            setattr(models[model_name], 'scaler', scalers[model_name])
    
    return models

def evaluate_models(models, X_test, y_test, class_info):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    from sklearn.metrics import cohen_kappa_score, f1_score
    
    results = {}
    
    for model_name, model in models.items():
        logger.info(f"è¯„ä¼° {model_name} æ¨¡å‹...")
        
        # é¢„æµ‹
        if hasattr(model, 'scaler') and model.scaler is not None:
            X_test_scaled = model.scaler.transform(X_test)
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)
        else:
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        kappa = cohen_kappa_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        
        # åˆ†ç±»æŠ¥å‘Š
        class_names = [class_info[i]['name'] for i in sorted(class_info.keys())]
        report = classification_report(
            y_test, y_pred, 
            target_names=class_names,
            output_dict=True
        )
        
        results[model_name] = {
            'accuracy': accuracy,
            'kappa': kappa,
            'f1_score': f1,
            'confusion_matrix': cm,
            'classification_report': report,
            'predictions': y_pred,
            'probabilities': y_pred_proba
        }
        
        logger.info(f"{model_name} - ç²¾åº¦: {accuracy:.3f}, Kappa: {kappa:.3f}, F1: {f1:.3f}")
    
    return results

def classify_full_scene(model, features, original_shape):
    """
    å¯¹æ•´ä¸ªåœºæ™¯è¿›è¡Œåˆ†ç±»
    """
    logger.info("å¼€å§‹åœºæ™¯åˆ†ç±»...")
    
    # é¢„æµ‹
    if hasattr(model, 'scaler') and model.scaler is not None:
        features_scaled = model.scaler.transform(features)
        predictions = model.predict(features_scaled)
    else:
        predictions = model.predict(features)
    
    # é‡å¡‘ä¸ºåŸå§‹å½¢çŠ¶
    classification_map = predictions.reshape(original_shape)
    
    logger.info("åœºæ™¯åˆ†ç±»å®Œæˆ")
    return classification_map

def create_visualizations(hyperspectral_data, true_labels, predicted_labels, 
                         evaluation_results, wavelengths, class_info):
    """
    åˆ›å»ºå¯è§†åŒ–ç»“æœ
    """
    logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.style.use('default')
    
    # 1. åˆ†ç±»ç»“æœå¯¹æ¯”å›¾
    create_classification_comparison(true_labels, predicted_labels, class_info)
    
    # 2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
    create_performance_comparison(evaluation_results)
    
    # 3. æ··æ·†çŸ©é˜µå›¾
    create_confusion_matrices(evaluation_results, class_info)
    
    # 4. å…‰è°±ç‰¹å¾å›¾
    create_spectral_signatures(hyperspectral_data, true_labels, wavelengths, class_info)
    
    logger.info("å¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆ")

def create_classification_comparison(true_labels, predicted_labels, class_info):
    """
    åˆ›å»ºåˆ†ç±»ç»“æœå¯¹æ¯”å›¾
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # åˆ›å»ºé¢œè‰²æ˜ å°„
    colors = ['black'] + [class_info[i]['color'] for i in sorted(class_info.keys())]
    from matplotlib.colors import ListedColormap
    cmap = ListedColormap(colors)
    
    # çœŸå®æ ‡ç­¾
    im1 = axes[0].imshow(true_labels, cmap=cmap, vmin=0, vmax=len(class_info))
    axes[0].set_title('çœŸå®æ ‡ç­¾', fontsize=14)
    axes[0].axis('off')
    
    # é¢„æµ‹æ ‡ç­¾
    im2 = axes[1].imshow(predicted_labels, cmap=cmap, vmin=0, vmax=len(class_info))
    axes[1].set_title('é¢„æµ‹ç»“æœ', fontsize=14)
    axes[1].axis('off')
    
    # æ·»åŠ å›¾ä¾‹
    handles = []
    labels = []
    for class_id in sorted(class_info.keys()):
        from matplotlib.patches import Patch
        handles.append(Patch(color=class_info[class_id]['color']))
        labels.append(class_info[class_id]['name'])
    
    fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.02), ncol=len(class_info))
    
    plt.tight_layout()
    plt.savefig('output/basic_classification/figures/classification_comparison.png', 
                dpi=300, bbox_inches='tight')
    plt.show()

def create_performance_comparison(evaluation_results):
    """
    åˆ›å»ºæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
    """
    models = list(evaluation_results.keys())
    metrics = ['accuracy', 'kappa', 'f1_score']
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(models))
    width = 0.25
    
    for i, metric in enumerate(metrics):
        values = [evaluation_results[model][metric] for model in models]
        ax.bar(x + i * width, values, width, label=metric.upper())
    
    ax.set_xlabel('æ¨¡å‹')
    ax.set_ylabel('åˆ†æ•°')
    ax.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
    ax.set_xticks(x + width)
    ax.set_xticklabels(models)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, metric in enumerate(metrics):
        values = [evaluation_results[model][metric] for model in models]
        for j, v in enumerate(values):
            ax.text(j + i * width, v + 0.01, f'{v:.3f}', 
                   ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('output/basic_classification/figures/performance_comparison.png', 
                dpi=300, bbox_inches='tight')
    plt.show()

def create_confusion_matrices(evaluation_results, class_info):
    """
    åˆ›å»ºæ··æ·†çŸ©é˜µå›¾
    """
    n_models = len(evaluation_results)
    fig, axes = plt.subplots(1, n_models, figsize=(4*n_models, 4))
    
    if n_models == 1:
        axes = [axes]
    
    class_names = [class_info[i]['name'] for i in sorted(class_info.keys())]
    
    for idx, (model_name, results) in enumerate(evaluation_results.items()):
        cm = results['confusion_matrix']
        
        # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        im = axes[idx].imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)
        axes[idx].set_title(f'{model_name}\næ··æ·†çŸ©é˜µ')
        
        # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
        thresh = cm_normalized.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                axes[idx].text(j, i, f'{cm[i, j]}\n({cm_normalized[i, j]:.2f})',
                             ha="center", va="center",
                             color="white" if cm_normalized[i, j] > thresh else "black")
        
        axes[idx].set_ylabel('çœŸå®æ ‡ç­¾')
        axes[idx].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[idx].set_xticks(range(len(class_names)))
        axes[idx].set_yticks(range(len(class_names)))
        axes[idx].set_xticklabels(class_names, rotation=45)
        axes[idx].set_yticklabels(class_names)
    
    plt.tight_layout()
    plt.savefig('output/basic_classification/figures/confusion_matrices.png', 
                dpi=300, bbox_inches='tight')
    plt.show()

def create_spectral_signatures(hyperspectral_data, labels, wavelengths, class_info):
    """
    åˆ›å»ºå…‰è°±ç‰¹å¾å›¾
    """
    fig, ax = plt.subplots(figsize=(12, 8))
    
    for class_id in sorted(class_info.keys()):
        # è·å–è¯¥ç±»åˆ«çš„åƒç´ 
        class_mask = labels == class_id
        if np.sum(class_mask) == 0:
            continue
        
        class_pixels = hyperspectral_data[class_mask]
        
        # è®¡ç®—å¹³å‡å…‰è°±å’Œæ ‡å‡†å·®
        mean_spectrum = np.mean(class_pixels, axis=0)
        std_spectrum = np.std(class_pixels, axis=0)
        
        # ç»˜åˆ¶å…‰è°±æ›²çº¿
        color = class_info[class_id]['color']
        ax.plot(wavelengths, mean_spectrum, color=color, 
               label=class_info[class_id]['name'], linewidth=2)
        
        # æ·»åŠ æ ‡å‡†å·®é˜´å½±
        ax.fill_between(wavelengths, 
                       mean_spectrum - std_spectrum,
                       mean_spectrum + std_spectrum,
                       color=color, alpha=0.2)
    
    ax.set_xlabel('æ³¢é•¿ (nm)')
    ax.set_ylabel('åå°„ç‡')
    ax.set_title('å„ç±»åˆ«å…‰è°±ç‰¹å¾æ›²çº¿')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('output/basic_classification/figures/spectral_signatures.png', 
                dpi=300, bbox_inches='tight')
    plt.show()

def save_results(classification_map, evaluation_results, models, config, class_info):
    """
    ä¿å­˜ç»“æœ
    """
    logger.info("ä¿å­˜åˆ†æç»“æœ...")
    
    output_dir = Path('output/basic_classification')
    
    # 1. ä¿å­˜åˆ†ç±»ç»“æœå›¾
    np.save(output_dir / 'results' / 'classification_map.npy', classification_map)
    
    # 2. ä¿å­˜è¯„ä¼°ç»“æœ
    import json
    
    # å‡†å¤‡å¯åºåˆ—åŒ–çš„è¯„ä¼°ç»“æœ
    serializable_results = {}
    for model_name, results in evaluation_results.items():
        serializable_results[model_name] = {
            'accuracy': float(results['accuracy']),
            'kappa': float(results['kappa']),
            'f1_score': float(results['f1_score']),
            'confusion_matrix': results['confusion_matrix'].tolist(),
        }
    
    with open(output_dir / 'results' / 'evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    
    # 3. ä¿å­˜æ¨¡å‹
    import pickle
    for model_name, model in models.items():
        model_file = output_dir / 'models' / f'{model_name.lower().replace(" ", "_")}_model.pkl'
        with open(model_file, 'wb') as f:
            pickle.dump(model, f)
    
    # 4. ä¿å­˜é…ç½®
    with open(output_dir / 'results' / 'config.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # 5. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    generate_text_report(evaluation_results, class_info, output_dir)
    
    logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

def generate_text_report(evaluation_results, class_info, output_dir):
    """
    ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    """
    report_file = output_dir / 'results' / 'classification_report.txt'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ç³»ç»Ÿ - åŸºç¡€åˆ†ç±»æŠ¥å‘Š\n")
        f.write("="*50 + "\n\n")
        
        f.write("ç±»åˆ«ä¿¡æ¯:\n")
        for class_id, info in class_info.items():
            f.write(f"  {class_id}: {info['name']}\n")
        f.write("\n")
        
        f.write("æ¨¡å‹æ€§èƒ½æ±‡æ€»:\n")
        f.write("-"*50 + "\n")
        
        for model_name, results in evaluation_results.items():
            f.write(f"\n{model_name}:\n")
            f.write(f"  æ€»ä½“ç²¾åº¦: {results['accuracy']:.4f}\n")
            f.write(f"  Kappaç³»æ•°: {results['kappa']:.4f}\n")
            f.write(f"  F1åˆ†æ•°:   {results['f1_score']:.4f}\n")
        
        f.write("\n\næœ€ä½³æ¨¡å‹: ")
        best_model = max(evaluation_results, key=lambda x: evaluation_results[x]['accuracy'])
        f.write(f"{best_model} (ç²¾åº¦: {evaluation_results[best_model]['accuracy']:.4f})\n")

def display_final_results(evaluation_results, total_time):
    """
    æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    """
    logger.info("="*60)
    logger.info("åŸºç¡€åˆ†ç±»ç¤ºä¾‹å®Œæˆ!")
    logger.info("="*60)
    
    # æ˜¾ç¤ºæ€§èƒ½æ±‡æ€»
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ±‡æ€»:")
    print("-" * 50)
    for model_name, results in evaluation_results.items():
        print(f"{model_name:15s} | ç²¾åº¦: {results['accuracy']:.3f} | "
              f"Kappa: {results['kappa']:.3f} | F1: {results['f1_score']:.3f}")
    
    # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
    best_model = max(evaluation_results, key=lambda x: evaluation_results[x]['accuracy'])
    best_accuracy = evaluation_results[best_model]['accuracy']
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (ç²¾åº¦: {best_accuracy:.3f})")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.2f} ç§’")
    
    # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
    print(f"   - åˆ†ç±»ç»“æœ: output/basic_classification/results/")
    print(f"   - å¯è§†åŒ–å›¾: output/basic_classification/figures/")
    print(f"   - è®­ç»ƒæ¨¡å‹: output/basic_classification/models/")
    
    logger.info("åŸºç¡€åˆ†ç±»ç¤ºä¾‹æ‰§è¡ŒæˆåŠŸ!")

def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # è®¾ç½®ç›®å½•
        setup_directories()
        
        # æ£€æŸ¥æ•°æ®
        check_data_availability()
        
        # æ‰§è¡ŒåŸºç¡€åˆ†ç±»å·¥ä½œæµç¨‹
        basic_classification_workflow()
        
        print("\nâœ… åŸºç¡€åˆ†ç±»ç¤ºä¾‹æ‰§è¡Œå®Œæˆ!")
        print("ğŸ” è¯·æŸ¥çœ‹ output/basic_classification/ ç›®å½•ä¸‹çš„ç»“æœæ–‡ä»¶")
        print("ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åœ¨ figures/ å­ç›®å½•ä¸­")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
        raise

if __name__ == "__main__":
    main()