{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 湿地高光谱分类结果分析\n",
    "\n",
    "## 概述\n",
    "本notebook对湿地高光谱分类的结果进行深入分析，包括：\n",
    "- 分类精度详细评估\n",
    "- 混淆矩阵分析\n",
    "- 错分样本分析\n",
    "- 特征重要性分析\n",
    "- 分类结果空间可视化\n",
    "- 不确定性分析\n",
    "- 模型可解释性分析\n",
    "\n",
    "通过全面的结果分析，深入理解模型性能，识别改进方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 科学计算和统计\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_recall_fscore_support,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 可视化\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# 地理空间分析\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# 模型解释\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "# 其他工具\n",
    "import joblib\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 自定义模块\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from wetland_classification.evaluation import (\n",
    "    ModelEvaluator, UncertaintyAnalyzer, \n",
    "    ClassificationVisualizer\n",
    ")\n",
    "from wetland_classification.utils import visualization, logger\n",
    "\n",
    "# 设置绘图样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# 配置日志\n",
    "logger = logger.setup_logger('result_analysis', level='INFO')\n",
    "\n",
    "print(\"结果分析环境初始化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据和模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置路径\n",
    "feature_dir = Path('../data/features')\n",
    "model_dir = Path('../models')\n",
    "output_dir = Path('../output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 加载特征数据\n",
    "print(\"加载数据...\")\n",
    "all_features = np.load(feature_dir / 'all_features.npy')\n",
    "feature_names = np.load(feature_dir / 'feature_names.npy')\n",
    "pca_95_features = np.load(feature_dir / 'pca_95_features.npy')\n",
    "\n",
    "# 加载模型结果\n",
    "model_results = pd.read_csv(model_dir / 'model_comparison_results.csv', index_col=0)\n",
    "with open(model_dir / 'training_report.json', 'r', encoding='utf-8') as f:\n",
    "    training_report = json.load(f)\n",
    "\n",
    "print(f\"特征数据形状: {all_features.shape}\")\n",
    "print(f\"PCA特征形状: {pca_95_features.shape}\")\n",
    "print(f\"模型数量: {len(model_results)}\")\n",
    "print(f\"最佳模型: {training_report['最佳模型']['模型名称']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新生成训练/测试数据划分（保持一致性）\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print(\"重新生成数据划分...\")\n",
    "\n",
    "# 生成一致的标签\n",
    "np.random.seed(42)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "y_labels = kmeans.fit_predict(pca_95_features)\n",
    "\n",
    "# 定义类别名称\n",
    "class_names = {\n",
    "    0: '开放水面',\n",
    "    1: '挺水植物',\n",
    "    2: '浮叶植物', \n",
    "    3: '湿生草本',\n",
    "    4: '土壤/裸地'\n",
    "}\n",
    "\n",
    "# 数据划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_features, y_labels, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y_labels\n",
    ")\n",
    "\n",
    "print(f\"训练集: {X_train.shape[0]} 样本\")\n",
    "print(f\"测试集: {X_test.shape[0]} 样本\")\n",
    "print(\"类别分布:\")\n",
    "for class_id, count in zip(*np.unique(y_test, return_counts=True)):\n",
    "    print(f\"  {class_names[class_id]}: {count} ({count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型进行详细分析\n",
    "best_model_name = training_report['最佳模型']['模型名称']\n",
    "print(f\"加载最佳模型: {best_model_name}\")\n",
    "\n",
    "# 根据模型名称加载对应的模型\n",
    "model_filename = best_model_name.lower().replace(' ', '_').replace('(', '').replace(')', '') + '_model.pkl'\n",
    "if 'optimized' in best_model_name.lower():\n",
    "    model_filename = model_filename.replace('_model.pkl', '_optimized_model.pkl')\n",
    "\n",
    "try:\n",
    "    best_model = joblib.load(model_dir / model_filename)\n",
    "    print(f\"成功加载模型: {model_filename}\")\n",
    "except FileNotFoundError:\n",
    "    # 如果找不到具体文件，尝试加载第一个可用的模型\n",
    "    model_files = list(model_dir.glob('*.pkl'))\n",
    "    if model_files:\n",
    "        best_model = joblib.load(model_files[0])\n",
    "        print(f\"使用替代模型: {model_files[0].name}\")\n",
    "    else:\n",
    "        print(\"未找到可用模型，将创建一个示例模型\")\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        best_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "# 生成预测结果\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test) if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "print(f\"预测完成，测试集大小: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分类精度详细评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算详细的评估指标\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"计算详细评估指标...\")\n",
    "\n",
    "# 整体指标\n",
    "overall_metrics = {\n",
    "    '总体准确率': accuracy_score(y_test, y_pred),\n",
    "    '加权精确率': precision_score(y_test, y_pred, average='weighted'),\n",
    "    '加权召回率': recall_score(y_test, y_pred, average='weighted'),\n",
    "    '加权F1分数': f1_score(y_test, y_pred, average='weighted'),\n",
    "    'Kappa系数': cohen_kappa_score(y_test, y_pred),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# 各类别指标\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "class_metrics = pd.DataFrame({\n",
    "    '类别': [class_names[i] for i in range(len(class_names))],\n",
    "    '精确率': precision,\n",
    "    '召回率': recall,\n",
    "    'F1分数': f1,\n",
    "    '样本数': support\n",
    "})\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n=== 整体评估指标 ===\")\n",
    "for metric, value in overall_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== 各类别评估指标 ===\")\n",
    "print(class_metrics.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化评估指标\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 各类别指标对比\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x - width, class_metrics['精确率'], width, label='精确率', alpha=0.8)\n",
    "axes[0, 0].bar(x, class_metrics['召回率'], width, label='召回率', alpha=0.8)\n",
    "axes[0, 0].bar(x + width, class_metrics['F1分数'], width, label='F1分数', alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_xlabel('类别')\n",
    "axes[0, 0].set_ylabel('分数')\n",
    "axes[0, 0].set_title('各类别性能指标')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels([class_names[i] for i in range(len(class_names))], rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 各类别样本数分布\n",
    "axes[0, 1].bar(range(len(class_names)), class_metrics['样本数'], \n",
    "               color=sns.color_palette(\"husl\", len(class_names)), alpha=0.8)\n",
    "axes[0, 1].set_xlabel('类别')\n",
    "axes[0, 1].set_ylabel('样本数')\n",
    "axes[0, 1].set_title('测试集类别分布')\n",
    "axes[0, 1].set_xticks(range(len(class_names)))\n",
    "axes[0, 1].set_xticklabels([class_names[i] for i in range(len(class_names))], rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 整体指标雷达图\n",
    "metrics_for_radar = ['总体准确率', '加权精确率', '加权召回率', '加权F1分数']\n",
    "values = [overall_metrics[m] for m in metrics_for_radar]\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics_for_radar), endpoint=False)\n",
    "values += values[:1]  # 闭合图形\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "axes[1, 0].plot(angles, values, 'o-', linewidth=2, label='模型性能')\n",
    "axes[1, 0].fill(angles, values, alpha=0.25)\n",
    "axes[1, 0].set_xticks(angles[:-1])\n",
    "axes[1, 0].set_xticklabels(metrics_for_radar)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "axes[1, 0].set_title('整体性能雷达图')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 预测置信度分布\n",
    "if y_pred_proba is not None:\n",
    "    max_proba = np.max(y_pred_proba, axis=1)\n",
    "    axes[1, 1].hist(max_proba, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 1].axvline(np.mean(max_proba), color='red', linestyle='--', \n",
    "                       label=f'平均置信度: {np.mean(max_proba):.3f}')\n",
    "    axes[1, 1].set_xlabel('最大预测概率')\n",
    "    axes[1, 1].set_ylabel('频次')\n",
    "    axes[1, 1].set_title('预测置信度分布')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 混淆矩阵分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算混淆矩阵\n",
    "print(\"生成混淆矩阵分析...\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "# 计算用户精度和生产者精度\n",
    "user_accuracy = np.diag(cm) / np.sum(cm, axis=1)  # 精确率\n",
    "producer_accuracy = np.diag(cm) / np.sum(cm, axis=0)  # 召回率\n",
    "\n",
    "print(\"混淆矩阵统计:\")\n",
    "print(f\"正确分类样本数: {np.trace(cm)}\")\n",
    "print(f\"总样本数: {np.sum(cm)}\")\n",
    "print(f\"总体准确率: {np.trace(cm) / np.sum(cm):.4f}\")\n",
    "\n",
    "# 创建详细的混淆矩阵表格\n",
    "class_labels = [class_names[i] for i in range(len(class_names))]\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "cm_norm_df = pd.DataFrame(cm_normalized, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"\\n原始混淆矩阵:\")\n",
    "print(cm_df)\n",
    "print(\"\\n归一化混淆矩阵:\")\n",
    "print(cm_norm_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化混淆矩阵\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 原始混淆矩阵\n",
    "im1 = axes[0].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0].set_title('混淆矩阵 (样本数)', fontsize=14)\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "axes[0].set_xticks(tick_marks)\n",
    "axes[0].set_yticks(tick_marks)\n",
    "axes[0].set_xticklabels(class_labels, rotation=45)\n",
    "axes[0].set_yticklabels(class_labels)\n",
    "axes[0].set_xlabel('预测类别')\n",
    "axes[0].set_ylabel('真实类别')\n",
    "\n",
    "# 添加数值标注\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    axes[0].text(j, i, format(cm[i, j], 'd'),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# 归一化混淆矩阵\n",
    "im2 = axes[1].imshow(cm_normalized, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title('归一化混淆矩阵 (比例)', fontsize=14)\n",
    "axes[1].set_xticks(tick_marks)\n",
    "axes[1].set_yticks(tick_marks)\n",
    "axes[1].set_xticklabels(class_labels, rotation=45)\n",
    "axes[1].set_yticklabels(class_labels)\n",
    "axes[1].set_xlabel('预测类别')\n",
    "axes[1].set_ylabel('真实类别')\n",
    "\n",
    "# 添加百分比标注\n",
    "thresh = cm_normalized.max() / 2.\n",
    "for i, j in np.ndindex(cm_normalized.shape):\n",
    "    axes[1].text(j, i, format(cm_normalized[i, j], '.2f'),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm_normalized[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 错分样本分析\n",
    "print(\"错分样本分析...\")\n",
    "\n",
    "# 找出错分样本\n",
    "misclassified_mask = y_test != y_pred\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "correct_indices = np.where(~misclassified_mask)[0]\n",
    "\n",
    "print(f\"错分样本数: {len(misclassified_indices)} / {len(y_test)} ({len(misclassified_indices)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"正确样本数: {len(correct_indices)} / {len(y_test)} ({len(correct_indices)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# 分析错分模式\n",
    "misclassification_patterns = {}\n",
    "for true_class in range(len(class_names)):\n",
    "    for pred_class in range(len(class_names)):\n",
    "        if true_class != pred_class:\n",
    "            count = np.sum((y_test == true_class) & (y_pred == pred_class))\n",
    "            if count > 0:\n",
    "                pattern = f\"{class_names[true_class]} → {class_names[pred_class]}\"\n",
    "                misclassification_patterns[pattern] = count\n",
    "\n",
    "# 按错分数量排序\n",
    "sorted_patterns = sorted(misclassification_patterns.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n主要错分模式:\")\n",
    "for pattern, count in sorted_patterns[:10]:\n",
    "    print(f\"  {pattern}: {count} 样本\")\n",
    "\n",
    "# 可视化错分模式\n",
    "if len(sorted_patterns) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    patterns = [p[0] for p in sorted_patterns[:10]]\n",
    "    counts = [p[1] for p in sorted_patterns[:10]]\n",
    "    \n",
    "    bars = ax.barh(range(len(patterns)), counts, color='lightcoral', alpha=0.8)\n",
    "    ax.set_yticks(range(len(patterns)))\n",
    "    ax.set_yticklabels(patterns)\n",
    "    ax.set_xlabel('错分样本数')\n",
    "    ax.set_title('主要错分模式统计')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                str(count), va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征重要性分析\n",
    "print(\"分析特征重要性...\")\n",
    "\n",
    "feature_importance = None\n",
    "importance_method = \"未知\"\n",
    "\n",
    "# 获取特征重要性（根据模型类型）\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # 树模型（RF, XGBoost等）\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    importance_method = \"基于树的特征重要性\"\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # 线性模型\n",
    "    feature_importance = np.abs(best_model.coef_).mean(axis=0) if best_model.coef_.ndim > 1 else np.abs(best_model.coef_)\n",
    "    importance_method = \"基于系数的特征重要性\"\n",
    "else:\n",
    "    # 使用排列重要性作为通用方法\n",
    "    print(\"使用排列重要性计算特征重要性...\")\n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test, y_test, \n",
    "        n_repeats=5, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    feature_importance = perm_importance.importances_mean\n",
    "    importance_method = \"排列重要性\"\n",
    "\n",
    "if feature_importance is not None:\n",
    "    # 创建特征重要性DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names[:len(feature_importance)],\n",
    "        'importance': feature_importance,\n",
    "        'abs_importance': np.abs(feature_importance)\n",
    "    })\n",
    "    \n",
    "    # 按重要性排序\n",
    "    importance_df = importance_df.sort_values('abs_importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{importance_method}\")\n",
    "    print(f\"总特征数: {len(feature_importance)}\")\n",
    "    print(f\"非零重要性特征数: {np.sum(feature_importance != 0)}\")\n",
    "    print(f\"平均重要性: {np.mean(feature_importance):.6f}\")\n",
    "    print(f\"标准差: {np.std(feature_importance):.6f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 最重要特征:\")\n",
    "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}. {row['feature'][:50]:<50} : {row['importance']:.6f}\")\n",
    "else:\n",
    "    print(\"无法计算特征重要性\")\n",
    "    importance_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化特征重要性\n",
    "if importance_df is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Top 20 特征重要性\n",
    "    top_20 = importance_df.head(20)\n",
    "    y_pos = np.arange(len(top_20))\n",
    "    \n",
    "    bars = axes[0, 0].barh(y_pos, top_20['importance'], color='skyblue', alpha=0.8)\n",
    "    axes[0, 0].set_yticks(y_pos)\n",
    "    axes[0, 0].set_yticklabels([f[:30] + '...' if len(f) > 30 else f \n",
    "                               for f in top_20['feature']], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('重要性分数')\n",
    "    axes[0, 0].set_title('Top 20 特征重要性')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 特征重要性分布\n",
    "    axes[0, 1].hist(importance_df['abs_importance'], bins=50, alpha=0.7, \n",
    "                   color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].axvline(np.mean(importance_df['abs_importance']), \n",
    "                      color='red', linestyle='--', label='平均值')\n",
    "    axes[0, 1].axvline(np.median(importance_df['abs_importance']), \n",
    "                      color='orange', linestyle='--', label='中位数')\n",
    "    axes[0, 1].set_xlabel('重要性分数')\n",
    "    axes[0, 1].set_ylabel('频次')\n",
    "    axes[0, 1].set_title('特征重要性分布')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 累积重要性\n",
    "    cumsum_importance = np.cumsum(importance_df['abs_importance']) / np.sum(importance_df['abs_importance'])\n",
    "    axes[1, 0].plot(range(1, len(cumsum_importance) + 1), cumsum_importance, 'b-', linewidth=2)\n",
    "    axes[1, 0].axhline(y=0.8, color='red', linestyle='--', label='80%')\n",
    "    axes[1, 0].axhline(y=0.9, color='orange', linestyle='--', label='90%')\n",
    "    axes[1, 0].set_xlabel('特征数量')\n",
    "    axes[1, 0].set_ylabel('累积重要性比例')\n",
    "    axes[1, 0].set_title('累积特征重要性')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 特征类型重要性分析（如果可以区分特征类型）\n",
    "    feature_types = {\n",
    "        '原始波段': [],\n",
    "        '植被指数': [],\n",
    "        '纹理特征': [],\n",
    "        '空间特征': [],\n",
    "        '其他': []\n",
    "    }\n",
    "    \n",
    "    # 根据特征名称分类\n",
    "    for idx, feature_name in enumerate(importance_df['feature']):\n",
    "        if 'Band_' in feature_name:\n",
    "            feature_types['原始波段'].append(importance_df.iloc[idx]['abs_importance'])\n",
    "        elif any(veg in feature_name for veg in ['NDVI', 'EVI', 'SAVI', 'GNDVI', 'NDWI']):\n",
    "            feature_types['植被指数'].append(importance_df.iloc[idx]['abs_importance'])\n",
    "        elif 'Texture' in feature_name:\n",
    "            feature_types['纹理特征'].append(importance_df.iloc[idx]['abs_importance'])\n",
    "        elif 'Spatial' in feature_name:\n",
    "            feature_types['空间特征'].append(importance_df.iloc[idx]['abs_importance'])\n",
    "        else:\n",
    "            feature_types['其他'].append(importance_df.iloc[idx]['abs_importance'])\n",
    "    \n",
    "    # 计算各类型平均重要性\n",
    "    type_avg_importance = {}\n",
    "    for ftype, importances in feature_types.items():\n",
    "        if importances:\n",
    "            type_avg_importance[ftype] = np.mean(importances)\n",
    "    \n",
    "    if type_avg_importance:\n",
    "        types = list(type_avg_importance.keys())\n",
    "        values = list(type_avg_importance.values())\n",
    "        \n",
    "        axes[1, 1].pie(values, labels=types, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 1].set_title('各类型特征平均重要性分布')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 计算80%和90%重要性所需的特征数\n",
    "    idx_80 = np.argmax(cumsum_importance >= 0.8) + 1\n",
    "    idx_90 = np.argmax(cumsum_importance >= 0.9) + 1\n",
    "    print(f\"\\n80%重要性需要前 {idx_80} 个特征\")\n",
    "    print(f\"90%重要性需要前 {idx_90} 个特征\")\n",
    "    print(f\"特征选择建议: 可考虑保留前 {idx_80}-{idx_90} 个最重要特征\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 分类结果空间可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成全图分类结果\n",
    "print(\"生成分类结果空间可视化...\")\n",
    "\n",
    "# 对所有像素进行预测\n",
    "print(\"对整个影像进行分类预测...\")\n",
    "all_predictions = best_model.predict(all_features)\n",
    "all_probabilities = best_model.predict_proba(all_features) if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# 假设原始影像尺寸（需要根据实际数据调整）\n",
    "# 这里使用合理的尺寸推算\n",
    "total_pixels = len(all_features)\n",
    "img_height = int(np.sqrt(total_pixels * 0.75))  # 假设长宽比为4:3\n",
    "img_width = int(total_pixels / img_height)\n",
    "\n",
    "# 调整到实际像素数\n",
    "if img_height * img_width != total_pixels:\n",
    "    img_height = int(np.sqrt(total_pixels))\n",
    "    img_width = img_height\n",
    "    if img_height * img_width < total_pixels:\n",
    "        img_width += 1\n",
    "\n",
    "# 重塑预测结果为二维数组\n",
    "classification_map = all_predictions[:img_height * img_width].reshape(img_height, img_width)\n",
    "\n",
    "print(f\"分类图尺寸: {img_height} x {img_width}\")\n",
    "print(f\"总像素数: {img_height * img_width}\")\n",
    "\n",
    "# 计算各类别面积统计\n",
    "class_counts = np.bincount(all_predictions, minlength=len(class_names))\n",
    "class_percentages = class_counts / len(all_predictions) * 100\n",
    "\n",
    "print(\"\\n分类结果统计:\")\n",
    "for i, (class_name, count, percentage) in enumerate(zip(class_names.values(), class_counts, class_percentages)):\n",
    "    print(f\"{class_name}: {count} 像素 ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分类结果\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 分类结果图\n",
    "# 创建颜色映射\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']  # 5种不同颜色\n",
    "cmap = plt.matplotlib.colors.ListedColormap(colors[:len(class_names)])\n",
    "\n",
    "im1 = axes[0, 0].imshow(classification_map, cmap=cmap, vmin=0, vmax=len(class_names)-1)\n",
    "axes[0, 0].set_title('湿地分类结果图', fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# 添加图例\n",
    "legend_elements = [plt.matplotlib.patches.Patch(color=colors[i], label=class_names[i]) \n",
    "                  for i in range(len(class_names))]\n",
    "axes[0, 0].legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# 各类别面积饼图\n",
    "axes[0, 1].pie(class_percentages, labels=[class_names[i] for i in range(len(class_names))], \n",
    "               autopct='%1.1f%%', colors=colors[:len(class_names)])\n",
    "axes[0, 1].set_title('各类别面积比例', fontsize=14)\n",
    "\n",
    "# 预测置信度图（如果有概率预测）\n",
    "if all_probabilities is not None:\n",
    "    max_prob_map = np.max(all_probabilities, axis=1)[:img_height * img_width].reshape(img_height, img_width)\n",
    "    im3 = axes[1, 0].imshow(max_prob_map, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    axes[1, 0].set_title('预测置信度图', fontsize=14)\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[1, 0], fraction=0.046, pad=0.04, label='置信度')\n",
    "    \n",
    "    # 置信度统计\n",
    "    axes[1, 1].hist(np.max(all_probabilities, axis=1), bins=50, alpha=0.7, \n",
    "                   color='lightblue', edgecolor='black')\n",
    "    axes[1, 1].axvline(np.mean(np.max(all_probabilities, axis=1)), \n",
    "                      color='red', linestyle='--', label='平均置信度')\n",
    "    axes[1, 1].set_xlabel('预测置信度')\n",
    "    axes[1, 1].set_ylabel('像素数量')\n",
    "    axes[1, 1].set_title('预测置信度分布', fontsize=14)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    # 如果没有概率预测，显示类别分布柱状图\n",
    "    axes[1, 0].bar(range(len(class_names)), class_counts, \n",
    "                   color=colors[:len(class_names)], alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('类别')\n",
    "    axes[1, 0].set_ylabel('像素数量')\n",
    "    axes[1, 0].set_title('各类别像素统计', fontsize=14)\n",
    "    axes[1, 0].set_xticks(range(len(class_names)))\n",
    "    axes[1, 0].set_xticklabels([class_names[i] for i in range(len(class_names))], rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 空白subplot\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 不确定性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不确定性分析\n",
    "print(\"进行分类不确定性分析...\")\n",
    "\n",
    "if all_probabilities is not None:\n",
    "    # 计算不确定性指标\n",
    "    \n",
    "    # 1. 预测熵（信息熵）\n",
    "    prediction_entropy = -np.sum(all_probabilities * np.log(all_probabilities + 1e-10), axis=1)\n",
    "    \n",
    "    # 2. 最大概率\n",
    "    max_probability = np.max(all_probabilities, axis=1)\n",
    "    \n",
    "    # 3. 第一和第二高概率的差值\n",
    "    sorted_probs = np.sort(all_probabilities, axis=1)\n",
    "    prob_margin = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "    \n",
    "    # 4. 方差\n",
    "    prob_variance = np.var(all_probabilities, axis=1)\n",
    "    \n",
    "    print(f\"预测熵统计:\")\n",
    "    print(f\"  平均值: {np.mean(prediction_entropy):.4f}\")\n",
    "    print(f\"  标准差: {np.std(prediction_entropy):.4f}\")\n",
    "    print(f\"  范围: {np.min(prediction_entropy):.4f} - {np.max(prediction_entropy):.4f}\")\n",
    "    \n",
    "    print(f\"\\n最大概率统计:\")\n",
    "    print(f\"  平均值: {np.mean(max_probability):.4f}\")\n",
    "    print(f\"  标准差: {np.std(max_probability):.4f}\")\n",
    "    print(f\"  范围: {np.min(max_probability):.4f} - {np.max(max_probability):.4f}\")\n",
    "    \n",
    "    print(f\"\\n概率边际统计:\")\n",
    "    print(f\"  平均值: {np.mean(prob_margin):.4f}\")\n",
    "    print(f\"  标准差: {np.std(prob_margin):.4f}\")\n",
    "    print(f\"  范围: {np.min(prob_margin):.4f} - {np.max(prob_margin):.4f}\")\n",
    "    \n",
    "    # 分析高不确定性区域\n",
    "    high_uncertainty_threshold = np.percentile(prediction_entropy, 90)\n",
    "    high_uncertainty_mask = prediction_entropy > high_uncertainty_threshold\n",
    "    high_uncertainty_classes = all_predictions[high_uncertainty_mask]\n",
    "    \n",
    "    print(f\"\\n高不确定性分析 (熵 > {high_uncertainty_threshold:.3f}):\")\n",
    "    print(f\"  高不确定性像素数: {np.sum(high_uncertainty_mask)} ({np.sum(high_uncertainty_mask)/len(all_predictions)*100:.1f}%)\")\n",
    "    \n",
    "    if len(high_uncertainty_classes) > 0:\n",
    "        print(\"  高不确定性区域的类别分布:\")\n",
    "        uncertainty_class_counts = np.bincount(high_uncertainty_classes, minlength=len(class_names))\n",
    "        for i, (class_name, count) in enumerate(zip(class_names.values(), uncertainty_class_counts)):\n",
    "            if count > 0:\n",
    "                percentage = count / len(high_uncertainty_classes) * 100\n",
    "                print(f\"    {class_name}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"模型不支持概率预测，无法进行详细的不确定性分析\")\n",
    "    prediction_entropy = None\n",
    "    max_probability = None\n",
    "    prob_margin = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化不确定性分析\n",
    "if prediction_entropy is not None:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 预测熵空间分布\n",
    "    entropy_map = prediction_entropy[:img_height * img_width].reshape(img_height, img_width)\n",
    "    im1 = axes[0, 0].imshow(entropy_map, cmap='Reds')\n",
    "    axes[0, 0].set_title('预测熵分布图')\n",
    "    axes[0, 0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], fraction=0.046, pad=0.04, label='熵值')\n",
    "    \n",
    "    # 最大概率空间分布\n",
    "    max_prob_map = max_probability[:img_height * img_width].reshape(img_height, img_width)\n",
    "    im2 = axes[0, 1].imshow(max_prob_map, cmap='RdYlGn')\n",
    "    axes[0, 1].set_title('最大概率分布图')\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[0, 1], fraction=0.046, pad=0.04, label='概率')\n",
    "    \n",
    "    # 概率边际空间分布\n",
    "    margin_map = prob_margin[:img_height * img_width].reshape(img_height, img_width)\n",
    "    im3 = axes[0, 2].imshow(margin_map, cmap='RdYlGn')\n",
    "    axes[0, 2].set_title('概率边际分布图')\n",
    "    axes[0, 2].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[0, 2], fraction=0.046, pad=0.04, label='边际')\n",
    "    \n",
    "    # 预测熵分布直方图\n",
    "    axes[1, 0].hist(prediction_entropy, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[1, 0].axvline(np.mean(prediction_entropy), color='blue', linestyle='--', label='平均值')\n",
    "    axes[1, 0].axvline(high_uncertainty_threshold, color='orange', linestyle='--', label='90%分位数')\n",
    "    axes[1, 0].set_xlabel('预测熵')\n",
    "    axes[1, 0].set_ylabel('频次')\n",
    "    axes[1, 0].set_title('预测熵分布')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 最大概率分布直方图\n",
    "    axes[1, 1].hist(max_probability, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1, 1].axvline(np.mean(max_probability), color='blue', linestyle='--', label='平均值')\n",
    "    axes[1, 1].set_xlabel('最大概率')\n",
    "    axes[1, 1].set_ylabel('频次')\n",
    "    axes[1, 1].set_title('最大概率分布')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 不确定性与类别关系\n",
    "    class_entropy_means = []\n",
    "    class_entropy_stds = []\n",
    "    \n",
    "    for class_id in range(len(class_names)):\n",
    "        class_mask = all_predictions == class_id\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_entropies = prediction_entropy[class_mask]\n",
    "            class_entropy_means.append(np.mean(class_entropies))\n",
    "            class_entropy_stds.append(np.std(class_entropies))\n",
    "        else:\n",
    "            class_entropy_means.append(0)\n",
    "            class_entropy_stds.append(0)\n",
    "    \n",
    "    x_pos = np.arange(len(class_names))\n",
    "    bars = axes[1, 2].bar(x_pos, class_entropy_means, yerr=class_entropy_stds, \n",
    "                         capsize=5, alpha=0.7, color=colors[:len(class_names)])\n",
    "    axes[1, 2].set_xlabel('类别')\n",
    "    axes[1, 2].set_ylabel('平均预测熵')\n",
    "    axes[1, 2].set_title('各类别预测不确定性')\n",
    "    axes[1, 2].set_xticks(x_pos)\n",
    "    axes[1, 2].set_xticklabels([class_names[i] for i in range(len(class_names))], rotation=45)\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 分析不确定性与分类错误的关系\n",
    "    if len(y_test) > 0:\n",
    "        test_entropy = prediction_entropy[len(X_train):len(X_train)+len(X_test)]\n",
    "        correct_entropy = test_entropy[y_test == y_pred]\n",
    "        incorrect_entropy = test_entropy[y_test != y_pred]\n",
    "        \n",
    "        print(f\"\\n不确定性与分类准确性关系:\")\n",
    "        print(f\"正确分类样本平均熵: {np.mean(correct_entropy):.4f} ± {np.std(correct_entropy):.4f}\")\n",
    "        print(f\"错误分类样本平均熵: {np.mean(incorrect_entropy):.4f} ± {np.std(incorrect_entropy):.4f}\")\n",
    "        \n",
    "        # 统计检验\n",
    "        from scipy import stats\n",
    "        t_stat, p_value = stats.ttest_ind(correct_entropy, incorrect_entropy)\n",
    "        print(f\"t检验结果: t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"结论: 错误分类样本的不确定性显著高于正确分类样本\")\n",
    "        else:\n",
    "            print(\"结论: 错误分类与正确分类样本的不确定性无显著差异\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模型可解释性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP值分析（如果可能）\n",
    "print(\"进行模型可解释性分析...\")\n",
    "\n",
    "try:\n",
    "    # 选择一个子集进行SHAP分析（计算量大）\n",
    "    n_shap_samples = min(100, len(X_test))\n",
    "    shap_indices = np.random.choice(len(X_test), n_shap_samples, replace=False)\n",
    "    X_shap = X_test[shap_indices]\n",
    "    y_shap = y_test[shap_indices]\n",
    "    \n",
    "    print(f\"使用 {n_shap_samples} 个样本进行SHAP分析...\")\n",
    "    \n",
    "    # 创建SHAP解释器\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        # 对于支持概率预测的模型\n",
    "        explainer = shap.Explainer(best_model.predict_proba, X_train[:100])  # 使用少量背景样本\n",
    "        shap_values = explainer(X_shap)\n",
    "        \n",
    "        print(\"SHAP分析完成\")\n",
    "        print(f\"SHAP值形状: {shap_values.values.shape}\")\n",
    "        \n",
    "        # 特征重要性总结\n",
    "        feature_importance_shap = np.mean(np.abs(shap_values.values).mean(axis=0), axis=0)\n",
    "        \n",
    "        # 创建SHAP重要性DataFrame\n",
    "        shap_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names[:len(feature_importance_shap)],\n",
    "            'shap_importance': feature_importance_shap\n",
    "        })\n",
    "        shap_importance_df = shap_importance_df.sort_values('shap_importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 SHAP重要特征:\")\n",
    "        for i, (_, row) in enumerate(shap_importance_df.head(10).iterrows()):\n",
    "            print(f\"{i+1:2d}. {row['feature'][:50]:<50} : {row['shap_importance']:.6f}\")\n",
    "        \n",
    "        shap_analysis_available = True\n",
    "        \n",
    "    else:\n",
    "        print(\"模型不支持概率预测，无法进行SHAP分析\")\n",
    "        shap_analysis_available = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"SHAP分析失败: {e}\")\n",
    "    shap_analysis_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 局部可解释性分析：查看具体样本的决策过程\n",
    "print(\"\\n局部可解释性分析...\")\n",
    "\n",
    "# 选择几个有代表性的样本\n",
    "sample_indices = []\n",
    "for class_id in range(len(class_names)):\n",
    "    class_mask = y_test == class_id\n",
    "    if np.sum(class_mask) > 0:\n",
    "        # 选择该类别中置信度最高的样本\n",
    "        if y_pred_proba is not None:\n",
    "            class_indices = np.where(class_mask)[0]\n",
    "            class_probs = y_pred_proba[class_indices, class_id]\n",
    "            best_idx = class_indices[np.argmax(class_probs)]\n",
    "            sample_indices.append(best_idx)\n",
    "        else:\n",
    "            # 随机选择一个样本\n",
    "            class_indices = np.where(class_mask)[0]\n",
    "            sample_indices.append(np.random.choice(class_indices))\n",
    "\n",
    "print(f\"选择了 {len(sample_indices)} 个代表性样本进行分析\")\n",
    "\n",
    "# 分析每个样本\n",
    "for i, sample_idx in enumerate(sample_indices):\n",
    "    true_class = y_test[sample_idx]\n",
    "    pred_class = y_pred[sample_idx]\n",
    "    \n",
    "    print(f\"\\n样本 {i+1}:\")\n",
    "    print(f\"  真实类别: {class_names[true_class]}\")\n",
    "    print(f\"  预测类别: {class_names[pred_class]}\")\n",
    "    print(f\"  预测正确: {'是' if true_class == pred_class else '否'}\")\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        sample_proba = y_pred_proba[sample_idx]\n",
    "        print(f\"  预测概率:\")\n",
    "        for j, prob in enumerate(sample_proba):\n",
    "            print(f\"    {class_names[j]}: {prob:.4f}\")\n",
    "        print(f\"  置信度: {np.max(sample_proba):.4f}\")\n",
    "        print(f\"  预测熵: {-np.sum(sample_proba * np.log(sample_proba + 1e-10)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策边界分析（在主成分空间中）\n",
    "print(\"\\n决策边界可视化分析...\")\n",
    "\n",
    "# 使用PCA降维到2D进行可视化\n",
    "if X_test.shape[1] > 2:\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    X_test_2d = pca_2d.fit_transform(X_test)\n",
    "    \n",
    "    print(f\"PCA降维到2D，解释方差比: {pca_2d.explained_variance_ratio_}\")\n",
    "    print(f\"累积解释方差比: {np.sum(pca_2d.explained_variance_ratio_):.4f}\")\n",
    "else:\n",
    "    X_test_2d = X_test\n",
    "\n",
    "# 可视化决策边界和样本分布\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 真实类别分布\n",
    "for class_id in range(len(class_names)):\n",
    "    mask = y_test == class_id\n",
    "    if np.sum(mask) > 0:\n",
    "        axes[0].scatter(X_test_2d[mask, 0], X_test_2d[mask, 1], \n",
    "                       c=colors[class_id], label=class_names[class_id], \n",
    "                       alpha=0.7, s=50
 

# 3. 面积-功能关系散点图 (继续)
scatter = axes[1, 0].scatter(areas, habitat_values, c=colors[:len(class_names)], 
                            s=100, alpha=0.8, edgecolors='black')
axes[1, 0].set_xlabel('类别面积 (像素)')
axes[1, 0].set_ylabel('栖息地功能值')
axes[1, 0].set_title('面积与栖息地功能关系')
axes[1, 0].grid(True, alpha=0.3)

# 添加类别标签
for i, (x, y) in enumerate(zip(areas, habitat_values)):
    axes[1, 0].annotate(class_names[i][:4], (x, y), xytext=(5, 5), 
                       textcoords='offset points', fontsize=8)

# 预测类别分布
for class_id in range(len(class_names)):
    mask = y_pred == class_id
    if np.sum(mask) > 0:
        axes[1].scatter(X_test_2d[mask, 0], X_test_2d[mask, 1], 
                       c=colors[class_id], label=class_names[class_id], 
                       alpha=0.7, s=50)

# 标记错分样本
misclassified_mask = y_test != y_pred
if np.sum(misclassified_mask) > 0:
    axes[1].scatter(X_test_2d[misclassified_mask, 0], X_test_2d[misclassified_mask, 1], 
                   marker='x', c='red', s=100, label='错分样本', alpha=0.8)

axes[1].set_xlabel('第一主成分')
axes[1].set_ylabel('第二主成分')
axes[1].set_title('预测类别分布 (PCA空间)')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 分析主成分的物理意义
if hasattr(pca_2d, 'components_'):
    print(f"\n主成分分析:")
    print(f"第一主成分解释方差比: {pca_2d.explained_variance_ratio_[0]:.4f}")
    print(f"第二主成分解释方差比: {pca_2d.explained_variance_ratio_[1]:.4f}")
    
    # 找出对主成分贡献最大的特征
    pc1_top_features = np.argsort(np.abs(pca_2d.components_[0]))[-5:][::-1]
    pc2_top_features = np.argsort(np.abs(pca_2d.components_[1]))[-5:][::-1]
    
    print(f"\n第一主成分主要特征:")
    for i, feat_idx in enumerate(pc1_top_features):
        if feat_idx < len(feature_names):
            print(f"  {feature_names[feat_idx]}: {pca_2d.components_[0][feat_idx]:.4f}")
    
    print(f"\n第二主成分主要特征:")
    for i, feat_idx in enumerate(pc2_top_features):
        if feat_idx < len(feature_names):
            print(f"  {feature_names[feat_idx]}: {pca_2d.components_[1][feat_idx]:.4f}")

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ROC曲线和AUC分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC曲线分析（多分类）\n",
    "print(\"ROC曲线和AUC分析...\")\n",
    "\n",
    "if y_pred_proba is not None:\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from itertools import cycle\n",
    "    \n",
    "    # 将标签二值化\n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(class_names)))\n",
    "    n_classes = y_test_bin.shape[1]\n",
    "    \n",
    "    # 计算每个类别的ROC曲线\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # 计算micro-average ROC curve\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # 计算macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    print(f\"AUC分数:\")\n",
    "    print(f\"  Micro-average AUC: {roc_auc['micro']:.4f}\")\n",
    "    print(f\"  Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        print(f\"  {class_names[i]} AUC: {roc_auc[i]:.4f}\")\n",
    "    \n",
    "    # 可视化ROC曲线\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 所有类别的ROC曲线\n",
    "    colors_cycle = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
    "    for i, color in zip(range(n_classes), colors_cycle):\n",
    "        ax1.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    ax1.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', lw=4,\n",
    "            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "    \n",
    "    ax1.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle=':', lw=4,\n",
    "            label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='随机分类器')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('假正率 (False Positive Rate)')\n",
    "    ax1.set_ylabel('真正率 (True Positive Rate)')\n",
    "    ax1.set_title('多分类ROC曲线')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # AUC分数条形图\n",
    "    auc_scores = [roc_auc[i] for i in range(n_classes)]\n",
    "    class_labels = [class_names[i] for i in range(n_classes)]\n",
    "    \n",
    "    bars = ax2.bar(range(len(class_labels)), auc_scores, \n",
    "                   color=colors[:len(class_labels)], alpha=0.8)\n",
    "    ax2.axhline(y=roc_auc['micro'], color='deeppink', linestyle='--', \n",
    "               label=f'Micro-average: {roc_auc[\"micro\"]:.3f}')\n",
    "    ax2.axhline(y=roc_auc['macro'], color='navy', linestyle='--', \n",
    "               label=f'Macro-average: {roc_auc[\"macro\"]:.3f}')\n",
    "    ax2.set_xlabel('类别')\n",
    "    ax2.set_ylabel('AUC分数')\n",
    "    ax2.set_title('各类别AUC分数')\n",
    "    ax2.set_xticks(range(len(class_labels)))\n",
    "    ax2.set_xticklabels(class_labels, rotation=45)\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i, (bar, score) in enumerate(zip(bars, auc_scores)):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"模型不支持概率预测，无法进行ROC分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 类别间距离和可分性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别间距离分析\n",
    "print(\"类别间距离和可分性分析...\")\n",
    "\n",
    "# 计算各类别的中心点\n",
    "class_centers = []\n",
    "class_spreads = []\n",
    "\n",
    "for class_id in range(len(class_names)):\n",
    "    class_mask = y_test == class_id\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_samples = X_test[class_mask]\n",
    "        class_center = np.mean(class_samples, axis=0)\n",
    "        class_spread = np.mean(np.linalg.norm(class_samples - class_center, axis=1))\n",
    "        class_centers.append(class_center)\n",
    "        class_spreads.append(class_spread)\n",
    "    else:\n",
    "        class_centers.append(np.zeros(X_test.shape[1]))\n",
    "        class_spreads.append(0)\n",
    "\n",
    "class_centers = np.array(class_centers)\n",
    "class_spreads = np.array(class_spreads)\n",
    "\n",
    "# 计算类别间距离矩阵\n",
    "n_classes = len(class_names)\n",
    "inter_class_distances = np.zeros((n_classes, n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        if i != j:\n",
    "            distance = np.linalg.norm(class_centers[i] - class_centers[j])\n",
    "            inter_class_distances[i, j] = distance\n",
    "\n",
    "print(f\"类别内平均分散度:\")\n",
    "for i, (class_name, spread) in enumerate(zip(class_names.values(), class_spreads)):\n",
    "    print(f\"  {class_name}: {spread:.4f}\")\n",
    "\n",
    "print(f\"\\n类别间平均距离: {np.mean(inter_class_distances[inter_class_distances > 0]):.4f}\")\n",
    "print(f\"类别间最小距离: {np.min(inter_class_distances[inter_class_distances > 0]):.4f}\")\n",
    "print(f\"类别间最大距离: {np.max(inter_class_distances):.4f}\")\n",
    "\n",
    "# 找出最相似和最不相似的类别对\n",
    "min_dist_idx = np.unravel_index(np.argmin(inter_class_distances + np.eye(n_classes) * 1000), \n",
    "                               inter_class_distances.shape)\n",
    "max_dist_idx = np.unravel_index(np.argmax(inter_class_distances), inter_class_distances.shape)\n",
    "\n",
    "print(f\"\\n最相似类别对: {class_names[min_dist_idx[0]]} - {class_names[min_dist_idx[1]]} \"\n",
    "      f\"(距离: {inter_class_distances[min_dist_idx]:.4f})\")\n",
    "print(f\"最不相似类别对: {class_names[max_dist_idx[0]]} - {class_names[max_dist_idx[1]]} \"\n",
    "      f\"(距离: {inter_class_distances[max_dist_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化类别间距离分析\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 类别间距离热力图\n",
    "im1 = axes[0, 0].imshow(inter_class_distances, cmap='viridis')\n",
    "axes[0, 0].set_title('类别间距离矩阵')\n",
    "axes[0, 0].set_xticks(range(n_classes))\n",
    "axes[0, 0].set_yticks(range(n_classes))\n",
    "axes[0, 0].set_xticklabels([class_names[i] for i in range(n_classes)], rotation=45)\n",
    "axes[0, 0].set_yticklabels([class_names[i] for i in range(n_classes)])\n",
    "\n",
    "# 添加数值标注\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        if i != j:\n",
    "            axes[0, 0].text(j, i, f'{inter_class_distances[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"white\" if inter_class_distances[i, j] > np.mean(inter_class_distances) else \"black\")\n",
    "\n",
    "plt.colorbar(im1, ax=axes[0, 0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# 类别内分散度\n",
    "bars = axes[0, 1].bar(range(n_classes), class_spreads, \n",
    "                     color=colors[:n_classes], alpha=0.8)\n",
    "axes[0, 1].set_xlabel('类别')\n",
    "axes[0, 1].set_ylabel('平均类内距离')\n",
    "axes[0, 1].set_title('类别内分散度')\n",
    "axes[0, 1].set_xticks(range(n_classes))\n",
    "axes[0, 1].set_xticklabels([class_names[i] for i in range(n_classes)], rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 添加数值标签\n",
    "for bar, spread in zip(bars, class_spreads):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{spread:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 可分性指标：类间距离/类内距离比值\n",
    "separability_scores = []\n",
    "for i in range(n_classes):\n",
    "    if class_spreads[i] > 0:\n",
    "        other_classes = [j for j in range(n_classes) if j != i]\n",
    "        min_inter_distance = np.min([inter_class_distances[i, j] for j in other_classes])\n",
    "        separability = min_inter_distance / class_spreads[i]\n",
    "        separability_scores.append(separability)\n",
    "    else:\n",
    "        separability_scores.append(0)\n",
    "\n",
    "bars = axes[1, 0].bar(range(n_classes), separability_scores, \n",
    "                     color=colors[:n_classes], alpha=0.8)\n",
    "axes[1, 0].set_xlabel('类别')\n",
    "axes[1, 0].set_ylabel('可分性分数')\n",
    "axes[1, 0].set_title('类别可分性分析\\n(最近类间距离/类内分散度)')\n",
    "axes[1, 0].set_xticks(range(n_classes))\n",
    "axes[1, 0].set_xticklabels([class_names[i] for i in range(n_classes)], rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 添加数值标签\n",
    "for bar, score in zip(bars, separability_scores):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "                   f'{score:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 距离分布直方图\n",
    "all_distances = inter_class_distances[inter_class_distances > 0]\n",
    "axes[1, 1].hist(all_distances, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 1].axvline(np.mean(all_distances), color='red', linestyle='--', \n",
    "                  label=f'平均距离: {np.mean(all_distances):.3f}')\n",
    "axes[1, 1].set_xlabel('类别间距离')\n",
    "axes[1, 1].set_ylabel('频次')\n",
    "axes[1, 1].set_title('类别间距离分布')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n可分性分析结果:\")\n",
    "for i, (class_name, score) in enumerate(zip(class_names.values(), separability_scores)):\n",
    "    print(f\"  {class_name}: {score:.4f} ({'易分' if score > 2 else '难分' if score > 1 else '很难分'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 结果保存和报告生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存分析结果\n",
    "print(\"保存分析结果...\")\n",
    "\n",
    "# 保存分类图\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 保存分类结果图\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "cmap = plt.matplotlib.colors.ListedColormap(colors[:len(class_names)])\n",
    "im = ax.imshow(classification_map, cmap=cmap, vmin=0, vmax=len(class_names)-1)\n",
    "ax.set_title('湿地高光谱分类结果', fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# 添加图例\n",
    "legend_elements = [plt.matplotlib.patches.Patch(color=colors[i], label=class_names[i]) \n",
    "                  for i in range(len(class_names))]\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)\n",
    "\n",
    "# 添加比例尺和北向标\n",
    "ax.text(0.02, 0.98, 'N', transform=ax.transAxes, fontsize=16, fontweight='bold',\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "ax.arrow(0.02, 0.94, 0, -0.05, transform=ax.transAxes, head_width=0.01, \n",
    "         head_length=0.01, fc='black', ec='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'wetland_classification_result.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir / 'wetland_classification_result.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"分类结果图已保存到: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成综合分析报告\n",
    "analysis_report = {\n",
    "    '模型信息': {\n",
    "        '最佳模型': best_model_name,\n",
    "        '模型类型': str(type(best_model)).split('.')[-1].replace(\"'>\", \"\"),\n",
    "        '训练样本数': len(X_train),\n",
    "        '测试样本数': len(X_test),\n",
    "        '特征维度': X_train.shape[1]\n",
    "    },\n",
    "    '整体性能': {\n",
    "        '总体准确率': float(overall_metrics['总体准确率']),\n",
    "        '加权精确率': float(overall_metrics['加权精确率']),\n",
    "        '加权召回率': float(overall_metrics['加权召回率']),\n",
    "        '加权F1分数': float(overall_metrics['加权F1分数']),\n",
    "        'Kappa系数': float(overall_metrics['Kappa系数']),\n",
    "        'MCC': float(overall_metrics['MCC'])\n",
    "    },\n",
    "    '各类别性能': {},\n",
    "    '分类统计': {\n",
    "        '总像素数': int(len(all_predictions)),\n",
    "        '各类别像素数': {class_names[i]: int(count) for i, count in enumerate(class_counts)},\n",
    "        '各类别面积比例': {class_names[i]: float(percentage) for i, percentage in enumerate(class_percentages)}\n",
    "    },\n",
    "    '错分分析': {\n",
    "        '错分样本数': int(len(misclassified_indices)),\n",
    "        '错分率': float(len(misclassified_indices) / len(y_test)),\n",
    "        '主要错分模式': dict(sorted_patterns[:5]) if len(sorted_patterns) > 0 else {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# 添加各类别详细性能\n",
    "for i, class_name in enumerate([class_names[j] for j in range(len(class_names))]):\n",
    "    analysis_report['各类别性能'][class_name] = {\n",
    "        '精确率': float(class_metrics.iloc[i]['精确率']),\n",
    "        '召回率': float(class_metrics.iloc[i]['召回率']),\n",
    "        'F1分数': float(class_metrics.iloc[i]['F1分数']),\n",
    "        '样本数': int(class_metrics.iloc[i]['样本数'])\n",
    "    }\n",
    "\n",
    "# 添加ROC分析结果（如果有）\n",
    "if y_pred_proba is not None and 'roc_auc' in locals():\n",
    "    analysis_report['ROC分析'] = {\n",
    "        'Micro_average_AUC': float(roc_auc['micro']),\n",
    "        'Macro_average_AUC': float(roc_auc['macro']),\n",
    "        '各类别AUC': {class_names[i]: float(roc_auc[i]) for i in range(len(class_names))}\n",
    "    }\n",
    "\n",
    "# 添加不确定性分析结果（如果有）\n",
    "if prediction_entropy is not None:\n",
    "    analysis_report['不确定性分析'] = {\n",
    "        '平均预测熵': float(np.mean(prediction_entropy)),\n",
    "        '预测熵标准差': float(np.std(prediction_entropy)),\n",
    "        '平均最大概率': float(np.mean(max_probability)),\n",
    "        '高不确定性像素比例': float(np.sum(high_uncertainty_mask) / len(all_predictions))\n",
    "    }\n",
    "\n",
    "# 添加可分性分析结果\n",
    "analysis_report['可分性分析'] = {\n",
    "    '平均类间距离': float(np.mean(inter_class_distances[inter_class_distances > 0])),\n",
    "    '平均类内分散度': float(np.mean(class_spreads)),\n",
    "    '各类别可分性分数': {class_names[i]: float(score) for i, score in enumerate(separability_scores)},\n",
    "    '最相似类别对': f\"{class_names[min_dist_idx[0]]} - {class_names[min_dist_idx[1]]}\",\n",
    "    '最不相似类别对': f\"{class_names[max_dist_idx[0]]} - {class_names[max_dist_idx[1]]}\"\n",
    "}\n",
    "\n",
    "# 添加特征重要性分析结果（如果有）\n",
    "if importance_df is not None:\n",
    "    analysis_report['特征重要性'] = {\n",
    "        '重要性分析方法': importance_method,\n",
    "        '80%重要性所需特征数': int(idx_80) if 'idx_80' in locals() else 0,\n",
    "        '90%重要性所需特征数': int(idx_90) if 'idx_90' in locals() else 0,\n",
    "        'Top_10_重要特征': list(importance_df.head(10)['feature'].values) if len(importance_df) >= 10 else list(importance_df['feature'].values)\n",
    "    }\n",
    "\n",
    "# 保存分析报告\n",
    "with open(output_dir / 'classification_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"分析报告已保存到: {output_dir / 'classification_analysis_report.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印最终分析报告\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"湿地高光谱分类结果分析报告\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 整体性能指标:\")\n",
    "print(f\"   总体准确率: {overall_metrics['总体准确率']:.4f}\")\n",
    "print(f\"   Kappa系数:  {overall_metrics['Kappa系数']:.4f}\")\n",
    "print(f\"   加权F1分数: {overall_metrics['加权F1分数']:.4f}\")\n",
    "print(f\"   MCC:       {overall_metrics['MCC']:.4f}\")\n",
    "\n",
    "print(f\"\\n🎯 各类别表现:\")\n",
    "for _, row in class_metrics.iterrows():\n",
    "    print(f\"   {row['类别']:<12}: 精确率={row['精确率']:.3f}, 召回率={row['召回率']:.3f}, F1={row['F1分数']:.3f}\")\n",
    "\n",
    "print(f\"\\n🗺️ 分类结果统计:\")\n",
    "for i, (class_name, count, percentage) in enumerate(zip(class_names.values(), class_counts, class_percentages)):\n",
    "    print(f\"   {class_name:<12}: {count:6d} 像素 ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n❌ 错分分析:\")\n",
    "print(f\"   错分样本数: {len(misclassified_indices)} / {len(y_test)} ({len(misclassified_indices)/len(y_test)*100:.1f}%)\")\n",
    "if len(sorted_patterns) > 0:\n",
    "    print(f\"   主要错分模式:\")\n",
    "    for pattern, count in sorted_patterns[:3]:\n",
    "        print(f\"     {pattern}: {count} 样本\")\n",
    "\n",
    "if prediction_entropy is not None:\n",
    "    print(f\"\\n🔄 不确定性分析:\")\n",
    "    print(f\"   平均预测熵: {np.mean(prediction_entropy):.4f}\")\n",
    "    print(f\"   平均置信度: {np.mean(max_probability):.4f}\")\n",
    "    print(f\"   高不确定性区域: {np.sum(high_uncertainty_mask)/len(all_predictions)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n📏 类别可分性:\")\n",
    "print(f\"   平均类间距离: {np.mean(inter_class_distances[inter_class_distances > 0]):.4f}\")\n",
    "print(f\"   平均类内分散度: {np.mean(class_spreads):.4f}\")\n",
    "print(f\"   最难分类别对: {class_names[min_dist_idx[0]]} - {class_names[min_dist_idx[1]]}\")\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(f\"\\n🔍 特征重要性:\")\n",
    "    print(f\"   分析方法: {importance_method}\")\n",
    "    print(f\"   80%重要性需要: {idx_80 if 'idx_80' in locals() else 'N/A'} 个特征\")\n",
    "    print(f\"   90%重要性需要: {idx_90 if 'idx_90' in locals() else 'N/A'} 个特征\")\n",
    "\n",
    "print(f\"\\n💡 改进建议:\")\n",
    "suggestions = []\n",
    "\n",
    "# 基于分析结果生成建议\n",
    "if overall_metrics['总体准确率'] < 0.85:\n",
    "    suggestions.append(\"考虑使用更复杂的模型或增加训练数据\")\n",
    "\n",
    "if len(misclassified_indices) / len(y_test) > 0.2:\n",
    "    suggestions.append(\"分析主要错分模式，考虑增加相应的特征或样本\")\n",
    "\n",
    "if prediction_entropy is not None and np.sum(high_uncertainty_mask) / len(all_predictions) > 0.15:\n",
    "    suggestions.append(\"高不确定性区域较多，建议增加训练样本或改进特征\")\n",
    "\n",
    "if min(separability_scores) < 1.5:\n",
    "    suggestions.append(\"部分类别可分性较低，建议优化特征选择或重新定义类别\")\n",
    "\n",
    "if len(suggestions) == 0:\n",
    "    suggestions.append(\"模型性能良好，可考虑在更大数据集上验证\")\n",
    "\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    print(f\"   {i}. {suggestion}\")\n",
    "\n",
    "print(f\"\\n📁 输出文件:\")\n",
    "print(f\"   - 分类结果图: {output_dir / 'wetland_classification_result.png'}\")\n",
    "print(f\"   - 详细分析报告: {output_dir / 'classification_analysis_report.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"分析完成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本notebook完成了湿地高光谱分类结果的全面分析:\n",
    "\n",
    "### 🎯 主要成果:\n",
    "1. **精度评估**: 全面的分类精度评估，包括整体和各类别指标\n",
    "2. **混淆矩阵分析**: 详细的错分模式和原因分析\n",
    "3. **特征重要性**: 识别对分类最重要的特征\n",
    "4. **空间可视化**: 分类结果的空间分布和模式\n",
    "5. **不确定性分析**: 预测置信度和不确定性评估\n",
    "6. **可解释性分析**: 模型决策过程的解释和可视化\n",
    "7. **ROC分析**: 多分类ROC曲线和AUC评估\n",
    "8. **可分性评估**: 类别间的可分性和距离分析\n",
    "\n",
    "### 📊 关键发现:\n",
    "通过详细的结果分析，我们深入理解了模型性能、分类结果的空间分布特征、各类别的分类难度，以及模型的优势和不足。\n",
    "\n",
    "### 🔬 技术价值:\n",
    "1. **质量保证**: 确保分类结果的可靠性和准确性\n",
    "2. **模型优化**: 为模型改进提供明确方向\n",
    "3. **应用指导**: 为实际应用提供置信度评估\n",
    "4. **科学理解**: 增进对湿地光谱特征的理解\n",
    "\n",
    "这种全面的结果分析对于确保高光谱分类项目的科学性和实用性具有重要意义。\n",
    "\""
   ]
  }
 ],\n \"metadata\": {\n,
  \"kernelspec\": {\n,
   \"display_name\": \"Python 3\",\n,
   \"language\": \"python\",\n,
   \"name\": \"python3\"\n,
  },\n,
  \"language_info\": {\n,
   \"codemirror_mode\": {\n,
    \"name\": \"ipython\",\n,
    \"version\": 3\n,
   },\n,
   \"file_extension\": \".py\",\n,
   \"mimetype\": \"text/x-python\",\n,
   \"name\": \"python\",\n,
   \"nbconvert_exporter\": \"python\",\n,
   \"pygments_lexer\": \"ipython3\",\n,
   \"version\": \"3.9.0\"\n,
  }\n,
 },\n,
 \"nbformat\": 4,\n,
 \"nbformat_minor\": 4\n,
}