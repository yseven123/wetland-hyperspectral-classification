{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¹¿åœ°é«˜å…‰è°±æ•°æ®æŽ¢ç´¢åˆ†æž\n",
    "## Wetland Hyperspectral Data Exploration\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å±•ç¤ºå¦‚ä½•æŽ¢ç´¢å’Œåˆ†æžé«˜å…‰è°±é¥æ„Ÿæ•°æ®ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€å¯è§†åŒ–ã€ç»Ÿè®¡åˆ†æžç­‰ã€‚\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡ï¼š**\n",
    "- äº†è§£é«˜å…‰è°±æ•°æ®çš„ç»“æž„å’Œç‰¹ç‚¹\n",
    "- æŽŒæ¡æ•°æ®å¯è§†åŒ–çš„åŸºæœ¬æ–¹æ³•\n",
    "- å­¦ä¼šè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°\n",
    "- ç†è§£ä¸åŒåœ°ç‰©ç±»åž‹çš„å…‰è°±ç‰¹å¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. çŽ¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"çŽ¯å¢ƒå‡†å¤‡å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ¹¿åœ°åˆ†ç±»ç³»ç»Ÿæ¨¡å—\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "try:\n",
    "    from wetland_classification.data import DataLoader\n",
    "    from wetland_classification.utils.visualization import Visualizer\n",
    "    from wetland_classification.utils.logger import get_logger\n",
    "    print(\"âœ… æ¹¿åœ°åˆ†ç±»ç³»ç»Ÿæ¨¡å—å¯¼å…¥æˆåŠŸ\")\nexcept ImportError as e:\n",
    "    print(f\"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ðŸ’¡ è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…æ¹¿åœ°åˆ†ç±»ç³»ç»Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®è·¯å¾„é…ç½®\n",
    "data_dir = Path('../data/samples/demo_scene')\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨\n",
    "if not data_dir.exists():\n",
    "    print(\"ðŸ”„ æ¼”ç¤ºæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...\")\n",
    "    # è¿™é‡Œå¯ä»¥è°ƒç”¨æ•°æ®åˆ›å»ºå‡½æ•°\n",
    "    # create_demo_data()\n",
    "    print(\"âš ï¸ è¯·å…ˆè¿è¡Œ examples/åŸºç¡€åˆ†ç±»ç¤ºä¾‹.py åˆ›å»ºæ¼”ç¤ºæ•°æ®\")\nelse:\n",
    "    print(f\"âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½é«˜å…‰è°±æ•°æ®\n",
    "hyperspectral_file = data_dir / 'hyperspectral_data.npy'\n",
    "labels_file = data_dir / 'ground_truth.npy'\n",
    "wavelengths_file = data_dir / 'wavelengths.npy'\n",
    "\n",
    "if hyperspectral_file.exists():\n",
    "    # åŠ è½½æ•°æ®\n",
    "    hyperspectral_data = np.load(hyperspectral_file)\n",
    "    ground_truth = np.load(labels_file)\n",
    "    \n",
    "    # å°è¯•åŠ è½½æ³¢é•¿ä¿¡æ¯\n",
    "    if wavelengths_file.exists():\n",
    "        wavelengths = np.load(wavelengths_file)\n",
    "    else:\n",
    "        # å¦‚æžœæ²¡æœ‰æ³¢é•¿ä¿¡æ¯ï¼Œåˆ›å»ºæ¨¡æ‹Ÿçš„æ³¢é•¿æ•°ç»„\n",
    "        wavelengths = np.linspace(400, 2500, hyperspectral_data.shape[2])\n",
    "    \n",
    "    print(f\"ðŸ“Š æ•°æ®åŠ è½½æˆåŠŸ!\")\n",
    "    print(f\"   é«˜å…‰è°±æ•°æ®å½¢çŠ¶: {hyperspectral_data.shape}\")\n",
    "    print(f\"   æ ‡ç­¾æ•°æ®å½¢çŠ¶: {ground_truth.shape}\")\n",
    "    print(f\"   æ³¢é•¿èŒƒå›´: {wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm\")\n",
    "    print(f\"   å…‰è°±æ³¢æ®µæ•°: {len(wavelengths)}\")\nelse:\n",
    "    print(\"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºæ¼”ç¤ºæ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½ç±»åˆ«ä¿¡æ¯\n",
    "import json\n",
    "\n",
    "class_info_file = data_dir / 'class_info.json'\n",
    "if class_info_file.exists():\n",
    "    with open(class_info_file, 'r', encoding='utf-8') as f:\n",
    "        class_info = json.load(f)\n",
    "    \n",
    "    # è½¬æ¢é”®ä¸ºæ•´æ•°\n",
    "    class_info = {int(k): v for k, v in class_info.items()}\n",
    "    \n",
    "    print(\"ðŸ“‹ ç±»åˆ«ä¿¡æ¯:\")\n",
    "    for class_id, info in class_info.items():\n",
    "        print(f\"   {class_id}: {info['name']}\")\nelse:\n",
    "    # åˆ›å»ºé»˜è®¤ç±»åˆ«ä¿¡æ¯\n",
    "    class_info = {\n",
    "        1: {'name': 'æ°´ä½“', 'color': '#0000FF'},\n",
    "        2: {'name': 'æ¤è¢«', 'color': '#00FF00'},\n",
    "        3: {'name': 'åœŸå£¤', 'color': '#8B4513'}\n",
    "    }\n",
    "    print(\"ðŸ”§ ä½¿ç”¨é»˜è®¤ç±»åˆ«ä¿¡æ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®åŸºæœ¬ä¿¡æ¯æŽ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "height, width, bands = hyperspectral_data.shape\n",
    "total_pixels = height * width\n",
    "\n",
    "print(\"ðŸ“ æ•°æ®ç»´åº¦ä¿¡æ¯:\")\n",
    "print(f\"   ç©ºé—´å°ºå¯¸: {height} Ã— {width} åƒç´ \")\n",
    "print(f\"   å…‰è°±æ³¢æ®µ: {bands} ä¸ª\")\n",
    "print(f\"   æ€»åƒç´ æ•°: {total_pixels:,}\")\n",
    "print(f\"   æ•°æ®å¤§å°: {hyperspectral_data.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nðŸ“Š å…‰è°±æ•°å€¼ç»Ÿè®¡:\")\n",
    "print(f\"   æœ€å°å€¼: {np.min(hyperspectral_data):.4f}\")\n",
    "print(f\"   æœ€å¤§å€¼: {np.max(hyperspectral_data):.4f}\")\n",
    "print(f\"   å¹³å‡å€¼: {np.mean(hyperspectral_data):.4f}\")\n",
    "print(f\"   æ ‡å‡†å·®: {np.std(hyperspectral_data):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡\n",
    "unique_classes, counts = np.unique(ground_truth, return_counts=True)\n",
    "\n",
    "print(\"ðŸ·ï¸ ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:\")\n",
    "class_stats = []\n",
    "for class_id, count in zip(unique_classes, counts):\n",
    "    percentage = count / total_pixels * 100\n",
    "    class_name = class_info.get(class_id, {}).get('name', f'ç±»åˆ«{class_id}')\n",
    "    \n",
    "    if class_id == 0:\n",
    "        class_name = 'èƒŒæ™¯'\n",
    "    \n",
    "    print(f\"   {class_name}: {count:,} åƒç´  ({percentage:.1f}%)\")\n",
    "    \n",
    "    if class_id > 0:  # æŽ’é™¤èƒŒæ™¯\n",
    "        class_stats.append({\n",
    "            'class_id': class_id,\n",
    "            'class_name': class_name,\n",
    "            'count': count,\n",
    "            'percentage': percentage\n",
    "        })\n",
    "\n",
    "# è½¬æ¢ä¸ºDataFrameä¾¿äºŽåˆ†æž\n",
    "class_df = pd.DataFrame(class_stats)\nprint(f\"\\nðŸ“ˆ æœ‰æ•ˆç±»åˆ«æ•°: {len(class_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 RGBåˆæˆå›¾åƒæ˜¾ç¤º\n",
    "def create_rgb_image(hyperspectral_data, wavelengths):\n",
    "    \"\"\"åˆ›å»ºRGBåˆæˆå›¾åƒ\"\"\"\n",
    "    # é€‰æ‹©æŽ¥è¿‘çº¢ã€ç»¿ã€è“çš„æ³¢æ®µ\n",
    "    red_idx = np.argmin(np.abs(wavelengths - 670))    # çº¢å…‰ ~670nm\n",
    "    green_idx = np.argmin(np.abs(wavelengths - 550))  # ç»¿å…‰ ~550nm  \n",
    "    blue_idx = np.argmin(np.abs(wavelengths - 470))   # è“å…‰ ~470nm\n",
    "    \n",
    "    # æå–RGBæ³¢æ®µ\n",
    "    red_band = hyperspectral_data[:, :, red_idx]\n",
    "    green_band = hyperspectral_data[:, :, green_idx]\n",
    "    blue_band = hyperspectral_data[:, :, blue_idx]\n",
    "    \n",
    "    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´\n",
    "    def normalize_band(band):\n",
    "        band_min, band_max = np.percentile(band, [2, 98])  # ä½¿ç”¨2%å’Œ98%åˆ†ä½æ•°\n",
    "        normalized = (band - band_min) / (band_max - band_min)\n",
    "        return np.clip(normalized, 0, 1)\n",
    "    \n",
    "    rgb_image = np.stack([\n",
    "        normalize_band(red_band),\n",
    "        normalize_band(green_band),\n",
    "        normalize_band(blue_band)\n",
    "    ], axis=2)\n",
    "    \n",
    "    return rgb_image, (red_idx, green_idx, blue_idx)\n",
    "\n",
    "# åˆ›å»ºRGBå›¾åƒ\n",
    "rgb_image, rgb_indices = create_rgb_image(hyperspectral_data, wavelengths)\n",
    "\n",
    "print(f\"ðŸŒˆ RGBåˆæˆæ³¢æ®µ:\")\n",
    "print(f\"   çº¢å…‰: æ³¢æ®µ{rgb_indices[0]+1} ({wavelengths[rgb_indices[0]]:.1f} nm)\")\n",
    "print(f\"   ç»¿å…‰: æ³¢æ®µ{rgb_indices[1]+1} ({wavelengths[rgb_indices[1]]:.1f} nm)\")\n",
    "print(f\"   è“å…‰: æ³¢æ®µ{rgb_indices[2]+1} ({wavelengths[rgb_indices[2]]:.1f} nm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 æ˜¾ç¤ºRGBå›¾åƒå’Œæ ‡ç­¾å›¾\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# RGBå›¾åƒ\n",
    "axes[0].imshow(rgb_image)\n",
    "axes[0].set_title('RGBåˆæˆå›¾åƒ', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# æ ‡ç­¾å›¾\n",
    "# åˆ›å»ºé¢œè‰²æ˜ å°„\n",
    "colors = ['black'] + [class_info.get(i, {}).get('color', '#CCCCCC') \n",
    "                      for i in range(1, len(class_info)+1)]\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "im = axes[1].imshow(ground_truth, cmap=cmap, vmin=0, vmax=len(colors)-1)\n",
    "axes[1].set_title('åœ°é¢çœŸå®žæ ‡ç­¾', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# æ·»åŠ å›¾ä¾‹\n",
    "legend_elements = []\n",
    "for class_id, info in class_info.items():\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements.append(Patch(facecolor=info['color'], label=info['name']))\n",
    "\n",
    "axes[1].legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 ç±»åˆ«åˆ†å¸ƒé¥¼å›¾\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# é¥¼å›¾\n",
    "colors_pie = [class_info[row['class_id']]['color'] for _, row in class_df.iterrows()]\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    class_df['percentage'], \n",
    "    labels=class_df['class_name'],\n",
    "    colors=colors_pie,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90\n",
    ")\n",
    "ax1.set_title('ç±»åˆ«é¢ç§¯åˆ†å¸ƒ', fontsize=14)\n",
    "\n",
    "# æ¡å½¢å›¾\n",
    "bars = ax2.bar(class_df['class_name'], class_df['count'], color=colors_pie, alpha=0.7)\n",
    "ax2.set_ylabel('åƒç´ æ•°é‡')\n",
    "ax2.set_title('å„ç±»åˆ«åƒç´ ç»Ÿè®¡', fontsize=14)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, count in zip(bars, class_df['count']):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
    "             f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å…‰è°±ç‰¹å¾åˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 æå–å„ç±»åˆ«çš„å…‰è°±ç‰¹å¾\n",
    "def extract_class_spectra(hyperspectral_data, ground_truth, class_info, sample_size=1000):\n",
    "    \"\"\"æå–å„ç±»åˆ«çš„å…‰è°±æ ·æœ¬\"\"\"\n",
    "    class_spectra = {}\n",
    "    \n",
    "    for class_id in class_info.keys():\n",
    "        # æ‰¾åˆ°è¯¥ç±»åˆ«çš„åƒç´ ä½ç½®\n",
    "        class_mask = ground_truth == class_id\n",
    "        class_pixels = hyperspectral_data[class_mask]\n",
    "        \n",
    "        if len(class_pixels) > 0:\n",
    "            # éšæœºé‡‡æ ·å‡å°‘æ•°æ®é‡\n",
    "            if len(class_pixels) > sample_size:\n",
    "                indices = np.random.choice(len(class_pixels), sample_size, replace=False)\n",
    "                class_pixels = class_pixels[indices]\n",
    "            \n",
    "            class_spectra[class_id] = {\n",
    "                'name': class_info[class_id]['name'],\n",
    "                'color': class_info[class_id]['color'],\n",
    "                'spectra': class_pixels,\n",
    "                'mean': np.mean(class_pixels, axis=0),\n",
    "                'std': np.std(class_pixels, axis=0),\n",
    "                'count': len(class_pixels)\n",
    "            }\n",
    "    \n",
    "    return class_spectra\n",
    "\n",
    "# æå–å…‰è°±ç‰¹å¾\n",
    "np.random.seed(42)  # ä¿è¯ç»“æžœå¯é‡å¤\n",
    "class_spectra = extract_class_spectra(hyperspectral_data, ground_truth, class_info)\n",
    "\n",
    "print(\"ðŸ”¬ å…‰è°±ç‰¹å¾æå–å®Œæˆ:\")\n",
    "for class_id, spec_data in class_spectra.items():\n",
    "    print(f\"   {spec_data['name']}: {spec_data['count']} ä¸ªæ ·æœ¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 ç»˜åˆ¶å¹³å‡å…‰è°±æ›²çº¿\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for class_id, spec_data in class_spectra.items():\n",
    "    mean_spectrum = spec_data['mean']\n",
    "    std_spectrum = spec_data['std']\n",
    "    color = spec_data['color']\n",
    "    name = spec_data['name']\n",
    "    \n",
    "    # ç»˜åˆ¶å¹³å‡å…‰è°±\n",
    "    plt.plot(wavelengths, mean_spectrum, color=color, linewidth=2, label=name)\n",
    "    \n",
    "    # æ·»åŠ æ ‡å‡†å·®é˜´å½±\n",
    "    plt.fill_between(wavelengths, \n",
    "                     mean_spectrum - std_spectrum,\n",
    "                     mean_spectrum + std_spectrum,\n",
    "                     color=color, alpha=0.2)\n",
    "\n",
    "plt.xlabel('æ³¢é•¿ (nm)', fontsize=12)\n",
    "plt.ylabel('åå°„çŽ‡', fontsize=12)\n",
    "plt.title('å„ç±»åˆ«å¹³å‡å…‰è°±ç‰¹å¾æ›²çº¿', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# æ ‡æ³¨é‡è¦æ³¢æ®µ\n",
    "important_bands = {\n",
    "    'Blue': 470,\n",
    "    'Green': 550,\n",
    "    'Red': 670,\n",
    "    'NIR': 850,\n",
    "    'SWIR1': 1650,\n",
    "    'SWIR2': 2200\n",
    "}\n",
    "\n",
    "for band_name, wavelength in important_bands.items():\n",
    "    if wavelength >= wavelengths[0] and wavelength <= wavelengths[-1]:\n",
    "        plt.axvline(x=wavelength, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.text(wavelength, plt.ylim()[1]*0.9, band_name, \n",
    "                ha='center', fontsize=8, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 å…‰è°±å¯åˆ†æ€§åˆ†æž\n",
    "def calculate_spectral_separability(spectra1, spectra2):\n",
    "    \"\"\"è®¡ç®—ä¸¤ä¸ªå…‰è°±ç±»åˆ«çš„å¯åˆ†æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\"\"\"\n",
    "    mean1, mean2 = np.mean(spectra1, axis=0), np.mean(spectra2, axis=0)\n",
    "    \n",
    "    # è®¡ç®—å…‰è°±è§’è·ç¦»\n",
    "    dot_product = np.dot(mean1, mean2)\n",
    "    norm_product = np.linalg.norm(mean1) * np.linalg.norm(mean2)\n",
    "    \n",
    "    if norm_product > 0:\n",
    "        spectral_angle = np.arccos(np.clip(dot_product / norm_product, -1, 1))\n",
    "        spectral_angle = np.degrees(spectral_angle)\n",
    "    else:\n",
    "        spectral_angle = 0\n",
    "    \n",
    "    # è®¡ç®—æ¬§æ°è·ç¦»\n",
    "    euclidean_dist = np.linalg.norm(mean1 - mean2)\n",
    "    \n",
    "    return spectral_angle, euclidean_dist\n",
    "\n",
    "# è®¡ç®—ç±»åˆ«é—´å¯åˆ†æ€§\n",
    "separability_matrix = np.zeros((len(class_spectra), len(class_spectra)))\n",
    "class_ids = list(class_spectra.keys())\n",
    "class_names = [class_spectra[cid]['name'] for cid in class_ids]\n",
    "\n",
    "for i, class_id1 in enumerate(class_ids):\n",
    "    for j, class_id2 in enumerate(class_ids):\n",
    "        if i != j:\n",
    "            spectra1 = class_spectra[class_id1]['spectra']\n",
    "            spectra2 = class_spectra[class_id2]['spectra']\n",
    "            angle, _ = calculate_spectral_separability(spectra1, spectra2)\n",
    "            separability_matrix[i, j] = angle\n",
    "\n",
    "# ç»˜åˆ¶å¯åˆ†æ€§çƒ­åŠ›å›¾\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(separability_matrix, \n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot=True, \n",
    "            fmt='.1f',\n",
    "            cmap='viridis',\n",
    "            cbar_kws={'label': 'å…‰è°±è§’è·ç¦» (åº¦)'})\n",
    "\n",
    "plt.title('ç±»åˆ«é—´å…‰è°±å¯åˆ†æ€§çŸ©é˜µ', fontsize=14)\n",
    "plt.xlabel('ç±»åˆ«', fontsize=12)\n",
    "plt.ylabel('ç±»åˆ«', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š å…‰è°±å¯åˆ†æ€§åˆ†æžï¼š\")\n",
    "print(\"   - æ•°å€¼è¶Šå¤§è¡¨ç¤ºä¸¤ä¸ªç±»åˆ«çš„å…‰è°±å·®å¼‚è¶Šå¤§\")\n",
    "print(\"   - æ·±è‰²è¡¨ç¤ºç›¸ä¼¼åº¦é«˜ï¼Œæµ…è‰²è¡¨ç¤ºå·®å¼‚å¤§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å…³é”®æ³¢æ®µåˆ†æž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 è®¡ç®—æ¤è¢«æŒ‡æ•°\n",
    "def calculate_vegetation_indices(hyperspectral_data, wavelengths):\n",
    "    \"\"\"è®¡ç®—æ¤è¢«æŒ‡æ•°\"\"\"\n",
    "    # æ‰¾åˆ°æœ€æŽ¥è¿‘çš„æ³¢æ®µ\n",
    "    def find_nearest_band(target_wavelength):\n",
    "        return np.argmin(np.abs(wavelengths - target_wavelength))\n",
    "    \n",
    "    # å®šä¹‰å…³é”®æ³¢æ®µ\n",
    "    blue_idx = find_nearest_band(470)\n",
    "    green_idx = find_nearest_band(550)\n",
    "    red_idx = find_nearest_band(670)\n",
    "    red_edge_idx = find_nearest_band(720)\n",
    "    nir_idx = find_nearest_band(850)\n",
    "    \n",
    "    # æå–æ³¢æ®µæ•°æ®\n",
    "    blue = hyperspectral_data[:, :, blue_idx]\n",
    "    green = hyperspectral_data[:, :, green_idx]\n",
    "    red = hyperspectral_data[:, :, red_idx]\n",
    "    red_edge = hyperspectral_data[:, :, red_edge_idx]\n",
    "    nir = hyperspectral_data[:, :, nir_idx]\n",
    "    \n",
    "    # è®¡ç®—æ¤è¢«æŒ‡æ•°\n",
    "    epsilon = 1e-8  # é¿å…é™¤é›¶\n",
    "    \n",
    "    ndvi = (nir - red) / (nir + red + epsilon)\n",
    "    evi = 2.5 * (nir - red) / (nir + 6 * red - 7.5 * blue + 1 + epsilon)\n",
    "    savi = 1.5 * (nir - red) / (nir + red + 0.5 + epsilon)\n",
    "    ndwi = (green - nir) / (green + nir + epsilon)\n",
    "    \n",
    "    indices = {\n",
    "        'NDVI': ndvi,\n",
    "        'EVI': evi,\n",
    "        'SAVI': savi,\n",
    "        'NDWI': ndwi\n",
    "    }\n",
    "    \n",
    "    band_info = {\n",
    "        'Blue': (blue_idx, wavelengths[blue_idx]),\n",
    "        'Green': (green_idx, wavelengths[green_idx]),\n",
    "        'Red': (red_idx, wavelengths[red_idx]),\n",
    "        'Red Edge': (red_edge_idx, wavelengths[red_edge_idx]),\n",
    "        'NIR': (nir_idx, wavelengths[nir_idx])\n",
    "    }\n",
    "    \n",
    "    return indices, band_info\n",
    "\n",
    "# è®¡ç®—æ¤è¢«æŒ‡æ•°\n",
    "vegetation_indices, band_info = calculate_vegetation_indices(hyperspectral_data, wavelengths)\n",
    "\n",
    "print(\"ðŸŒ± å…³é”®æ³¢æ®µä¿¡æ¯ï¼š\")\n",
    "for name, (idx, wavelength) in band_info.items():\n",
    "    print(f\"   {name}: æ³¢æ®µ{idx+1} ({wavelength:.1f} nm)\")\n",
    "\n",
    "print(\"\\nðŸ“Š æ¤è¢«æŒ‡æ•°ç»Ÿè®¡ï¼š\")\n",
    "for name, index_data in vegetation_indices.items():\n",
    "    print(f\"   {name}: {np.min(index_data):.3f} ~ {np.max(index_data):.3f} (å‡å€¼: {np.mean(index_data):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 å¯è§†åŒ–æ¤è¢«æŒ‡æ•°\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "cmaps = ['RdYlGn', 'viridis', 'RdYlBu', 'RdBu_r']\n",
    "\n",
    "for i, (name, index_data) in enumerate(vegetation_indices.items()):\n",
    "    im = axes[i].imshow(index_data, cmap=cmaps[i])\n",
    "    axes[i].set_title(f'{name} æ¤è¢«æŒ‡æ•°', fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # æ·»åŠ é¢œè‰²æ¡\n",
    "    cbar = plt.colorbar(im, ax=axes[i], shrink=0.8)\n",
    "    cbar.set_label(name, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ•°æ®è´¨é‡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "def assess_data_quality(hyperspectral_data, ground_truth):\n",
    "    \"\"\"è¯„ä¼°æ•°æ®è´¨é‡\"\"\"\n",
    "    quality_report = {}\n",
    "    \n",
    "    # æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "    nan_pixels = np.isnan(hyperspectral_data).sum()\n",
    "    inf_pixels = np.isinf(hyperspectral_data).sum()\n",
    "    zero_pixels = (hyperspectral_data == 0).sum()\n",
    "    \n",
    "    quality_report['missing_data'] = {\n",
    "        'nan_pixels': int(nan_pixels),\n",
    "        'inf_pixels': int(inf_pixels),\n",
    "        'zero_pixels': int(zero_pixels),\n",
    "        'total_pixels': int(hyperspectral_data.size)\n",
    "    }\n",
    "    \n",
    "    # æ£€æŸ¥æ•°å€¼èŒƒå›´\n",
    "    quality_report['value_range'] = {\n",
    "        'min_value': float(np.min(hyperspectral_data)),\n",
    "        'max_value': float(np.max(hyperspectral_data)),\n",
    "        'mean_value': float(np.mean(hyperspectral_data)),\n",
    "        'std_value': float(np.std(hyperspectral_data))\n",
    "    }\n",
    "    \n",
    "    # æ£€æŸ¥å¼‚å¸¸æ³¢æ®µ\n",
    "    band_means = np.mean(hyperspectral_data, axis=(0, 1))\n",
    "    band_stds = np.std(hyperspectral_data, axis=(0, 1))\n",
    "    \n",
    "    # è¯†åˆ«å¼‚å¸¸æ³¢æ®µï¼ˆæ ‡å‡†å·®è¿‡å¤§æˆ–è¿‡å°ï¼‰\n",
    "    std_threshold_high = np.percentile(band_stds, 95)\n",
    "    std_threshold_low = np.percentile(band_stds, 5)\n",
    "    \n",
    "    noisy_bands = np.where(band_stds > std_threshold_high)[0]\n",
    "    low_variation_bands = np.where(band_stds < std_threshold_low)[0]\n",
    "    \n",
    "    quality_report['band_quality'] = {\n",
    "        'noisy_bands': noisy_bands.tolist(),\n",
    "        'low_variation_bands': low_variation_bands.tolist(),\n",
    "        'band_mean_range': [float(np.min(band_means)), float(np.max(band_means))],\n",
    "        'band_std_range': [float(np.min(band_stds)), float(np.max(band_stds))]\n",
    "    }\n",
    "    \n",
    "    # æ£€æŸ¥æ ‡ç­¾è´¨é‡\n",
    "    unique_labels = np.unique(ground_truth)\n",
    "    label_counts = np.bincount(ground_truth.ravel())\n",
    "    \n",
    "    quality_report['label_quality'] = {\n",
    "        'unique_labels': unique_labels.tolist(),\n",
    "        'label_counts': label_counts.tolist(),\n",
    "        'min_class_size': int(np.min(label_counts[unique_labels])),\n",
    "        'max_class_size': int(np.max(label_counts[unique_labels]))\n",
    "    }\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# æ‰§è¡Œè´¨é‡è¯„ä¼°\n",
    "quality_report = assess_data_quality(hyperspectral_data, ground_truth)\n",
    "\n",
    "print(\"ðŸ” æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š:\")\n",
    "print(f\"\\nðŸ“Š ç¼ºå¤±æ•°æ®æ£€æŸ¥:\")\n",
    "missing = quality_report['missing_data']\n",
    "print(f\"   NaNåƒç´ : {missing['nan_pixels']:,}\")\n",
    "print(f\"   æ— ç©·å¤§åƒç´ : {missing['inf_pixels']:,}\")\n",
    "print(f\"   é›¶å€¼åƒç´ : {missing['zero_pixels']:,} ({missing['zero_pixels']/missing['total_pixels']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ æ•°å€¼èŒƒå›´æ£€æŸ¥:\")\n",
    "value_range = quality_report['value_range']\n",
    "print(f\"   æ•°å€¼èŒƒå›´: {value_range['min_value']:.4f} ~ {value_range['max_value']:.4f}\")\n",
    "print(f\"   å¹³å‡å€¼: {value_range['mean_value']:.4f}\")\n",
    "print(f\"   æ ‡å‡†å·®: {value_range['std_value']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŒˆ æ³¢æ®µè´¨é‡æ£€æŸ¥:\")\n",
    "band_quality = quality_report['band_quality']\n",
    "print(f\"   å™ªå£°æ³¢æ®µæ•°: {len(band_quality['noisy_bands'])}\")\n",
    "print(f\"   ä½Žå˜å¼‚æ³¢æ®µæ•°: {len(band_quality['low_variation_bands'])}\")\n",
    "if band_quality['noisy_bands']:\n",
    "    print(f\"   å™ªå£°æ³¢æ®µ: {band_quality['noisy_bands'][:10]}{'...' if len(band_quality['noisy_bands']) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 å¯è§†åŒ–æ³¢æ®µè´¨é‡\n",
    "band_means = np.mean(hyperspectral_data, axis=(0, 1))\n",
    "band_stds = np.std(hyperspectral_data, axis=(0, 1))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# æ³¢æ®µå¹³å‡å€¼\n",
    "ax1.plot(wavelengths, band_means, 'b-', linewidth=1)\n",
    "ax1.set_ylabel('å¹³å‡åå°„çŽ‡', fontsize=12)\n",
    "ax1.set_title('å„æ³¢æ®µå¹³å‡åå°„çŽ‡', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# æ ‡è®°å¼‚å¸¸æ³¢æ®µ\n",
    "noisy_bands = quality_report['band_quality']['noisy_bands']\n",
    "if noisy_bands:\n",
    "    ax1.scatter(wavelengths[noisy_bands], band_means[noisy_bands], \n",
    "               color='red', s=30, label='å™ªå£°æ³¢æ®µ', zorder=5)\n",
    "    ax1.legend()\n",
    "\n",
    "# æ³¢æ®µæ ‡å‡†å·®\n",
    "ax2.plot(wavelengths, band_stds, 'g-', linewidth=1)\n",
    "ax2.set_xlabel('æ³¢é•¿ (nm)', fontsize=12)\n",
    "ax2.set_ylabel('æ ‡å‡†å·®', fontsize=12)\n",
    "ax2.set_title('å„æ³¢æ®µæ ‡å‡†å·®ï¼ˆå˜å¼‚ç¨‹åº¦ï¼‰', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# æ ‡è®°å¼‚å¸¸æ³¢æ®µ\n",
    "if noisy_bands:\n",
    "    ax2.scatter(wavelengths[noisy_bands], band_stds[noisy_bands], \n",
    "               color='red', s=30, label='å™ªå£°æ³¢æ®µ', zorder=5)\n",
    "\n",
    "low_var_bands = quality_report['band_quality']['low_variation_bands']\n",
    "if low_var_bands:\n",
    "    ax2.scatter(wavelengths[low_var_bands], band_stds[low_var_bands], \n",
    "               color='orange', s=30, label='ä½Žå˜å¼‚æ³¢æ®µ', zorder=5)\n",
    "\n",
    "if noisy_bands or low_var_bands:\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ•°æ®æŽ¢ç´¢æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 ç”Ÿæˆæ•°æ®æŽ¢ç´¢æ€»ç»“æŠ¥å‘Š\n",
    "def generate_exploration_summary(hyperspectral_data, ground_truth, class_info, \n",
    "                                quality_report, class_spectra):\n",
    "    \"\"\"ç”Ÿæˆæ•°æ®æŽ¢ç´¢æ€»ç»“\"\"\"\n",
    "    summary = {}\n",
    "    \n",
    "    # åŸºæœ¬ä¿¡æ¯\n",
    "    height, width, bands = hyperspectral_data.shape\n",
    "    summary['basic_info'] = {\n",
    "        'spatial_size': f\"{height} Ã— {width}\",\n",
    "        'spectral_bands': bands,\n",
    "        'total_pixels': height * width,\n",
    "        'data_size_mb': hyperspectral_data.nbytes / 1024**2,\n",
    "        'num_classes': len(class_info)\n",
    "    }\n",
    "    \n",
    "    # ç±»åˆ«å¹³è¡¡æ€§\n",
    "    unique_classes, counts = np.unique(ground_truth, return_counts=True)\n",
    "    valid_classes = unique_classes[unique_classes > 0]  # æŽ’é™¤èƒŒæ™¯\n",
    "    valid_counts = counts[unique_classes > 0]\n",
    "    \n",
    "    balance_ratio = np.max(valid_counts) / np.min(valid_counts)\n",
    "    summary['class_balance'] = {\n",
    "        'balance_ratio': balance_ratio,\n",
    "        'is_balanced': balance_ratio < 5,  # å¦‚æžœæ¯”ä¾‹å°äºŽ5è®¤ä¸ºæ˜¯å¹³è¡¡çš„\n",
    "        'min_class_size': int(np.min(valid_counts)),\n",
    "        'max_class_size': int(np.max(valid_counts))\n",
    "    }\n",
    "    \n",
    "    # å…‰è°±å¯åˆ†æ€§\n",
    "    if len(class_spectra) > 1:\n",
    "        separability_scores = []\n",
    "        class_ids = list(class_spectra.keys())\n",
    "        \n",
    "        for i in range(len(class_ids)):\n",
    "            for j in range(i+1, len(class_ids)):\n",
    "                spectra1 = class_spectra[class_ids[i]]['spectra']\n",
    "                spectra2 = class_spectra[class_ids[j]]['spectra']\n",
    "                angle, _ = calculate_spectral_separability(spectra1, spectra2)\n",
    "                separability_scores.append(angle)\n",
    "        \n",
    "        summary['spectral_separability'] = {\n",
    "            'mean_separability': np.mean(separability_scores),\n",
    "            'min_separability': np.min(separability_scores),\n",
    "            'max_separability': np.max(separability_scores),\n",
    "            'well_separated': np.mean(separability_scores) > 10  # è§’åº¦å¤§äºŽ10åº¦è®¤ä¸ºå¯åˆ†\n",
    "        }\n",
    "    \n",
    "    # æ•°æ®è´¨é‡\n",
    "    total_pixels = hyperspectral_data.size\n",
    "    problem_pixels = (quality_report['missing_data']['nan_pixels'] + \n",
    "                     quality_report['missing_data']['inf_pixels'])\n",
    "    \n",
    "    summary['data_quality'] = {\n",
    "        'quality_score': 100 * (1 - problem_pixels / total_pixels),\n",
    "        'has_missing_data': problem_pixels > 0,\n",
    "        'noisy_bands_ratio': len(quality_report['band_quality']['noisy_bands']) / bands,\n",
    "        'low_var_bands_ratio': len(quality_report['band_quality']['low_variation_bands']) / bands\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ç”Ÿæˆæ€»ç»“\n",
    "exploration_summary = generate_exploration_summary(\n",
    "    hyperspectral_data, ground_truth, class_info, quality_report, class_spectra\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ æ•°æ®æŽ¢ç´¢æ€»ç»“æŠ¥å‘Š\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nðŸ“Š åŸºæœ¬ä¿¡æ¯:\")\n",
    "basic = exploration_summary['basic_info']\n",
    "print(f\"   ç©ºé—´å°ºå¯¸: {basic['spatial_size']} åƒç´ \")\n",
    "print(f\"   å…‰è°±æ³¢æ®µ: {basic['spectral_bands']} ä¸ª\")\n",
    "print(f\"   æ•°æ®å¤§å°: {basic['data_size_mb']:.1f} MB\")\n",
    "print(f\"   ç±»åˆ«æ•°é‡: {basic['num_classes']} ä¸ª\")\n",
    "\n",
    "print(f\"\\nâš–ï¸ ç±»åˆ«å¹³è¡¡æ€§:\")\n",
    "balance = exploration_summary['class_balance']\n",
    "print(f\"   å¹³è¡¡çŠ¶æ€: {'âœ… å¹³è¡¡' if balance['is_balanced'] else 'âš ï¸ ä¸å¹³è¡¡'}\")\n",
    "print(f\"   å¹³è¡¡æ¯”ä¾‹: {balance['balance_ratio']:.1f}:1\")\n",
    "print(f\"   æ ·æœ¬é‡èŒƒå›´: {balance['min_class_size']:,} ~ {balance['max_class_size']:,}\")\n",
    "\n",
    "if 'spectral_separability' in exploration_summary:\n",
    "    print(f\"\\nðŸ”¬ å…‰è°±å¯åˆ†æ€§:\")\n",
    "    sep = exploration_summary['spectral_separability']\n",
    "    print(f\"   å¯åˆ†æ€§çŠ¶æ€: {'âœ… è‰¯å¥½' if sep['well_separated'] else 'âš ï¸ å›°éš¾'}\")\n",
    "    print(f\"   å¹³å‡è§’è·ç¦»: {sep['mean_separability']:.1f}Â°\")\n",
    "    print(f\"   è·ç¦»èŒƒå›´: {sep['min_separability']:.1f}Â° ~ {sep['max_separability']:.1f}Â°\")\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®è´¨é‡:\")\n",
    "quality = exploration_summary['data_quality']\n",
    "print(f\"   è´¨é‡è¯„åˆ†: {quality['quality_score']:.1f}/100\")\n",
    "print(f\"   æ•°æ®å®Œæ•´æ€§: {'âœ… å®Œæ•´' if not quality['has_missing_data'] else 'âš ï¸ æœ‰ç¼ºå¤±'}\")\n",
    "print(f\"   å™ªå£°æ³¢æ®µæ¯”ä¾‹: {quality['noisy_bands_ratio']:.1%}\")\n",
    "print(f\"   ä½Žå˜å¼‚æ³¢æ®µæ¯”ä¾‹: {quality['low_var_bands_ratio']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 æ•°æ®é¢„å¤„ç†å»ºè®®\n",
    "def generate_preprocessing_recommendations(exploration_summary, quality_report):\n",
    "    \"\"\"åŸºäºŽæŽ¢ç´¢ç»“æžœç”Ÿæˆé¢„å¤„ç†å»ºè®®\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # ç±»åˆ«å¹³è¡¡å»ºè®®\n",
    "    if not exploration_summary['class_balance']['is_balanced']:\n",
    "        recommendations.append({\n",
    "            'type': 'ç±»åˆ«å¹³è¡¡',\n",
    "            'priority': 'high',\n",
    "            'suggestion': 'æ•°æ®é›†å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡ï¼Œå»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡ã€è¿‡é‡‡æ ·æˆ–æ¬ é‡‡æ ·æŠ€æœ¯'\n",
    "        })\n",
    "    \n",
    "    # å™ªå£°æ³¢æ®µå¤„ç†\n",
    "    noisy_ratio = exploration_summary['data_quality']['noisy_bands_ratio']\n",
    "    if noisy_ratio > 0.1:\n",
    "        recommendations.append({\n",
    "            'type': 'å™ªå£°å¤„ç†',\n",
    "            'priority': 'medium',\n",
    "            'suggestion': f'æ£€æµ‹åˆ°{noisy_ratio:.1%}çš„å™ªå£°æ³¢æ®µï¼Œå»ºè®®è¿›è¡Œæ³¢æ®µé€‰æ‹©æˆ–å™ªå£°åŽ»é™¤'\n",
    "        })\n",
    "    \n",
    "    # ä½Žå˜å¼‚æ³¢æ®µå¤„ç†\n",
    "    low_var_ratio = exploration_summary['data_quality']['low_var_bands_ratio']\n",
    "    if low_var_ratio > 0.05:\n",
    "        recommendations.append({\n",
    "            'type': 'ç‰¹å¾é€‰æ‹©',\n",
    "            'priority': 'low',\n",
    "            'suggestion': f'æ£€æµ‹åˆ°{low_var_ratio:.1%}çš„ä½Žå˜å¼‚æ³¢æ®µï¼Œå¯è€ƒè™‘åŽ»é™¤ä»¥å‡å°‘å†—ä½™'\n",
    "        })\n",
    "    \n",
    "    # å…‰è°±å¯åˆ†æ€§å»ºè®®\n",
    "    if 'spectral_separability' in exploration_summary:\n",
    "        if not exploration_summary['spectral_separability']['well_separated']:\n",
    "            recommendations.append({\n",
    "                'type': 'ç‰¹å¾å¢žå¼º',\n",
    "                'priority': 'high',\n",
    "                'suggestion': 'å…‰è°±å¯åˆ†æ€§è¾ƒä½Žï¼Œå»ºè®®æå–æ¤è¢«æŒ‡æ•°ã€çº¹ç†ç‰¹å¾ç­‰é«˜çº§ç‰¹å¾'\n",
    "            })\n",
    "    \n",
    "    # æ•°æ®å½’ä¸€åŒ–å»ºè®®\n",
    "    value_range = quality_report['value_range']\n",
    "    if value_range['max_value'] > 1 or value_range['min_value'] < 0:\n",
    "        recommendations.append({\n",
    "            'type': 'æ•°æ®å½’ä¸€åŒ–',\n",
    "            'priority': 'medium',\n",
    "            'suggestion': 'æ•°æ®å€¼åŸŸè¶…å‡º[0,1]èŒƒå›´ï¼Œå»ºè®®è¿›è¡Œå½’ä¸€åŒ–æˆ–æ ‡å‡†åŒ–å¤„ç†'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# ç”Ÿæˆå»ºè®®\n",
    "recommendations = generate_preprocessing_recommendations(exploration_summary, quality_report)\n",
    "\n",
    "print(\"\\nðŸ’¡ é¢„å¤„ç†å»ºè®®:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if recommendations:\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        priority_icon = {'high': 'ðŸ”´', 'medium': 'ðŸŸ¡', 'low': 'ðŸŸ¢'}\n",
    "        print(f\"\\n{i}. {rec['type']} {priority_icon.get(rec['priority'], 'âšª')}\")\n",
    "        print(f\"   {rec['suggestion']}\")\nelse:\n",
    "    print(\"\\nâœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šé¢„å¤„ç†\")\n",
    "\n",
    "print(\"\\nðŸ“š ä¸‹ä¸€æ­¥å»ºè®®:\")\n",
    "print(\"   1. æ ¹æ®ä»¥ä¸Šå»ºè®®è¿›è¡Œæ•°æ®é¢„å¤„ç†\")\n",
    "print(\"   2. æŸ¥çœ‹ '02_é¢„å¤„ç†æµç¨‹.ipynb' äº†è§£å…·ä½“å®žçŽ°\")\n",
    "print(\"   3. è¿›è¡Œç‰¹å¾å·¥ç¨‹å’Œæ¨¡åž‹è®­ç»ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ä¿å­˜æŽ¢ç´¢ç»“æžœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æŽ¢ç´¢ç»“æžœ\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "output_dir = Path('../output/data_exploration')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜æŽ¢ç´¢æ€»ç»“\n",
    "exploration_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_info': {\n",
    "        'data_path': str(data_dir),\n",
    "        'data_shape': hyperspectral_data.shape,\n",
    "        'wavelength_range': [float(wavelengths[0]), float(wavelengths[-1])],\n",
    "        'num_classes': len(class_info)\n",
    "    },\n",
    "    'exploration_summary': exploration_summary,\n",
    "    'quality_report': quality_report,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "# ä¿å­˜ä¸ºJSONæ–‡ä»¶\n",
    "with open(output_dir / 'exploration_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(exploration_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ä¿å­˜æ¤è¢«æŒ‡æ•°\n",
    "np.savez_compressed(output_dir / 'vegetation_indices.npz', **vegetation_indices)\n",
    "\n",
    "# ä¿å­˜ç±»åˆ«å…‰è°±ç‰¹å¾\n",
    "spectral_data = {}\n",
    "for class_id, spec_data in class_spectra.items():\n",
    "    spectral_data[f'class_{class_id}_mean'] = spec_data['mean']\n",
    "    spectral_data[f'class_{class_id}_std'] = spec_data['std']\n",
    "\n",
    "np.savez_compressed(output_dir / 'class_spectra.npz', \n",
    "                   wavelengths=wavelengths, **spectral_data)\n",
    "\n",
    "print(f\"ðŸ’¾ æŽ¢ç´¢ç»“æžœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "print(f\"   - exploration_report.json: å®Œæ•´æŽ¢ç´¢æŠ¥å‘Š\")\n",
    "print(f\"   - vegetation_indices.npz: æ¤è¢«æŒ‡æ•°æ•°æ®\")\n",
    "print(f\"   - class_spectra.npz: ç±»åˆ«å…‰è°±ç‰¹å¾\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ æ•°æ®æŽ¢ç´¢å®Œæˆ!\n",
    "\n",
    "é€šè¿‡æœ¬ç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å®Œæˆäº†å¯¹æ¹¿åœ°é«˜å…‰è°±æ•°æ®çš„å…¨é¢æŽ¢ç´¢åˆ†æžï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "### âœ… ä¸»è¦æˆæžœï¼š\n",
    "1. **æ•°æ®åŸºæœ¬ä¿¡æ¯**: äº†è§£äº†æ•°æ®çš„ç»´åº¦ã€å¤§å°å’ŒåŸºæœ¬ç»Ÿè®¡ç‰¹æ€§\n",
    "2. **å¯è§†åŒ–åˆ†æž**: åˆ›å»ºäº†RGBå›¾åƒã€æ ‡ç­¾å›¾å’Œç±»åˆ«åˆ†å¸ƒå›¾\n",
    "3. **å…‰è°±ç‰¹å¾åˆ†æž**: æå–å¹¶åˆ†æžäº†å„ç±»åˆ«çš„å…‰è°±ç‰¹å¾æ›²çº¿\n",
    "4. **å¯åˆ†æ€§è¯„ä¼°**: è®¡ç®—äº†ç±»åˆ«é—´çš„å…‰è°±å¯åˆ†æ€§\n",
    "5. **æ¤è¢«æŒ‡æ•°è®¡ç®—**: è®¡ç®—äº†NDVIã€EVIã€SAVIã€NDWIç­‰å…³é”®æŒ‡æ•°\n",
    "6. **æ•°æ®è´¨é‡è¯„ä¼°**: å…¨é¢è¯„ä¼°äº†æ•°æ®è´¨é‡å¹¶è¯†åˆ«æ½œåœ¨é—®é¢˜\n",
    "7. **é¢„å¤„ç†å»ºè®®**: åŸºäºŽåˆ†æžç»“æžœæä¾›äº†é’ˆå¯¹æ€§çš„é¢„å¤„ç†å»ºè®®\n",
    "\n",
    "### ðŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š\n",
    "- **02_é¢„å¤„ç†æµç¨‹.ipynb**: å­¦ä¹ æ•°æ®é¢„å¤„ç†çš„å…·ä½“æ–¹æ³•\n",
    "- **03_ç‰¹å¾å·¥ç¨‹.ipynb**: æ·±å…¥äº†è§£ç‰¹å¾æå–å’Œå·¥ç¨‹\n",
    "- **04_æ¨¡åž‹è®­ç»ƒ.ipynb**: æŽŒæ¡æ¨¡åž‹è®­ç»ƒå’Œè°ƒä¼˜\n",
    "- **05_ç»“æžœåˆ†æž.ipynb**: å­¦ä¹ ç»“æžœåˆ†æžå’Œè¯„ä¼°\n",
    "- **06_æ™¯è§‚åˆ†æž.ipynb**: äº†è§£æ™¯è§‚æ ¼å±€åˆ†æžæ–¹æ³•\n",
    "\n",
    "### ðŸ’¡ å…³é”®æ”¶èŽ·ï¼š\n",
    "- é«˜å…‰è°±æ•°æ®åŒ…å«ä¸°å¯Œçš„å…‰è°±ä¿¡æ¯ï¼Œæ¯ä¸ªåœ°ç‰©ç±»åž‹éƒ½æœ‰ç‹¬ç‰¹çš„å…‰è°±ç‰¹å¾\n",
    "- æ¤è¢«æŒ‡æ•°æ˜¯åŒºåˆ†ä¸åŒåœ°ç‰©ç±»åž‹çš„é‡è¦ç‰¹å¾\n",
    "- æ•°æ®è´¨é‡è¯„ä¼°æ˜¯æˆåŠŸåˆ†ç±»çš„é‡è¦å‰æ\n",
    "- ç±»åˆ«å¹³è¡¡æ€§å’Œå…‰è°±å¯åˆ†æ€§ç›´æŽ¥å½±å“åˆ†ç±»æ•ˆæžœ\n",
    "\n",
    "ç»§ç»­æŽ¢ç´¢æ›´å¤šé«˜çº§åŠŸèƒ½ï¼Œç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}