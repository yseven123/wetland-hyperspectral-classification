#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸»å¤„ç†æµæ°´çº¿
Main Processing Pipeline

é›†æˆæ•°æ®å¤„ç†ã€ç‰¹å¾æå–ã€åˆ†ç±»å’Œåå¤„ç†çš„å®Œæ•´æµæ°´çº¿

ä½œè€…: Wetland Research Team
"""

import os
import time
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass, field
import logging
from datetime import datetime
import traceback

import numpy as np
import pandas as pd
from tqdm import tqdm

from .config import Config
from .utils.logger import Logger
from .utils.io_utils import IOUtils
from .utils.visualization import VisualizationUtils

logger = logging.getLogger(__name__)


@dataclass
class PipelineResults:
    """æµæ°´çº¿æ‰§è¡Œç»“æœ"""
    success: bool = False
    message: str = ""
    processing_time: float = 0.0
    
    # æ•°æ®ä¿¡æ¯
    input_shape: Optional[Tuple[int, ...]] = None
    output_shape: Optional[Tuple[int, ...]] = None
    num_samples: int = 0
    
    # åˆ†ç±»ç»“æœ
    classification_map: Optional[np.ndarray] = None
    probability_maps: Optional[np.ndarray] = None
    confidence_map: Optional[np.ndarray] = None
    
    # ç²¾åº¦è¯„ä¼°
    accuracy_metrics: Dict[str, float] = field(default_factory=dict)
    confusion_matrix: Optional[np.ndarray] = None
    classification_report: Optional[Dict[str, Any]] = None
    
    # æ™¯è§‚åˆ†æ
    landscape_metrics: Dict[str, float] = field(default_factory=dict)
    
    # æ–‡ä»¶è·¯å¾„
    output_files: Dict[str, str] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'success': self.success,
            'message': self.message,
            'processing_time': self.processing_time,
            'input_shape': self.input_shape,
            'output_shape': self.output_shape,
            'num_samples': self.num_samples,
            'accuracy_metrics': self.accuracy_metrics,
            'landscape_metrics': self.landscape_metrics,
            'output_files': self.output_files
        }


class Pipeline:
    """æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ä¸»å¤„ç†æµæ°´çº¿
    
    é›†æˆå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š
    1. æ•°æ®åŠ è½½å’ŒéªŒè¯
    2. é¢„å¤„ç† (è¾å°„æ ¡æ­£ã€å¤§æ°”æ ¡æ­£ã€å‡ ä½•æ ¡æ­£ã€å™ªå£°å»é™¤)
    3. ç‰¹å¾æå– (å…‰è°±ç‰¹å¾ã€æ¤è¢«æŒ‡æ•°ã€çº¹ç†ç‰¹å¾ã€ç©ºé—´ç‰¹å¾)
    4. åˆ†ç±»é¢„æµ‹ (ä¼ ç»ŸMLã€æ·±åº¦å­¦ä¹ ã€é›†æˆå­¦ä¹ )
    5. åå¤„ç† (ç©ºé—´æ»¤æ³¢ã€å½¢æ€å­¦æ“ä½œã€ä¸€è‡´æ€§æ£€æŸ¥)
    6. æ™¯è§‚åˆ†æ (æ™¯è§‚æŒ‡æ•°ã€è¿é€šæ€§åˆ†æ)
    7. ç²¾åº¦è¯„ä¼°å’Œç»“æœè¾“å‡º
    """
    
    def __init__(self, config: Union[Config, str, Path]):
        """åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            config: é…ç½®å¯¹è±¡æˆ–é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        if isinstance(config, (str, Path)):
            self.config = Config.from_file(config)
        elif isinstance(config, Config):
            self.config = config
        else:
            raise ValueError("config must be Config object or file path")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self._setup_output_dir()
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # éªŒè¯ç¯å¢ƒ
        self._validate_environment()
        
        logger.info("Pipeline initialized successfully")
    
    def run(self,
            input_path: Union[str, Path],
            ground_truth: Optional[Union[str, Path]] = None,
            output_dir: Optional[Union[str, Path]] = None,
            model_type: Optional[str] = None,
            **kwargs) -> PipelineResults:
        """æ‰§è¡Œå®Œæ•´çš„åˆ†ç±»æµæ°´çº¿
        
        Args:
            input_path: è¾“å…¥é«˜å…‰è°±æ•°æ®è·¯å¾„
            ground_truth: åœ°é¢çœŸå€¼æ•°æ®è·¯å¾„ (å¯é€‰)
            output_dir: è¾“å‡ºç›®å½• (å¯é€‰)
            model_type: æ¨¡å‹ç±»å‹ (å¯é€‰)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            PipelineResults: å¤„ç†ç»“æœ
        """
        start_time = time.time()
        results = PipelineResults()
        
        try:
            logger.info("Starting classification pipeline")
            logger.info(f"Input: {input_path}")
            logger.info(f"Ground truth: {ground_truth}")
            logger.info(f"Model type: {model_type or self.config.get('classification.default_classifier')}")
            
            # 1. æ•°æ®åŠ è½½å’ŒéªŒè¯
            logger.info("Step 1: Loading and validating data")
            hyperspectral_data, metadata = self._load_and_validate_data(input_path)
            results.input_shape = hyperspectral_data.shape
            logger.info(f"Data shape: {hyperspectral_data.shape}")
            
            # åŠ è½½è®­ç»ƒæ ·æœ¬
            train_data, val_data, test_data = None, None, None
            if ground_truth:
                logger.info("Loading ground truth data")
                train_data, val_data, test_data = self._load_ground_truth(ground_truth, hyperspectral_data)
                results.num_samples = len(train_data[0]) if train_data else 0
                logger.info(f"Training samples: {results.num_samples}")
            
            # 2. é¢„å¤„ç†
            logger.info("Step 2: Preprocessing")
            preprocessed_data = self._preprocess_data(hyperspectral_data, metadata)
            logger.info("Preprocessing completed")
            
            # 3. ç‰¹å¾æå–
            logger.info("Step 3: Feature extraction")
            features = self._extract_features(preprocessed_data)
            logger.info(f"Features shape: {features.shape}")
            
            # 4. åˆ†ç±»é¢„æµ‹
            logger.info("Step 4: Classification")
            classifier = self._get_classifier(model_type)
            
            if train_data is not None:
                # è®­ç»ƒæ¨¡å¼
                logger.info("Training classifier")
                classifier = self._train_classifier(classifier, train_data, val_data, features)
                
                # é¢„æµ‹
                logger.info("Predicting classification")
                classification_map, probability_maps = self._predict_classification(
                    classifier, features
                )
                
                # ç²¾åº¦è¯„ä¼°
                if test_data is not None:
                    logger.info("Evaluating accuracy")
                    results.accuracy_metrics, results.confusion_matrix, results.classification_report = \
                        self._evaluate_accuracy(classifier, test_data, features)
            else:
                # é¢„æµ‹æ¨¡å¼ (ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)
                logger.info("Loading pretrained model")
                classifier = self._load_pretrained_model(classifier)
                
                logger.info("Predicting classification")
                classification_map, probability_maps = self._predict_classification(
                    classifier, features
                )
            
            results.classification_map = classification_map
            results.probability_maps = probability_maps
            results.output_shape = classification_map.shape
            
            # 5. åå¤„ç†
            logger.info("Step 5: Post-processing")
            processed_map = self._postprocess_results(classification_map)
            results.classification_map = processed_map
            
            # è®¡ç®—ç½®ä¿¡åº¦
            if probability_maps is not None:
                results.confidence_map = self._calculate_confidence(probability_maps)
            
            # 6. æ™¯è§‚åˆ†æ
            logger.info("Step 6: Landscape analysis")
            results.landscape_metrics = self._analyze_landscape(processed_map)
            
            # 7. ç»“æœè¾“å‡º
            logger.info("Step 7: Saving results")
            output_dir = output_dir or self.output_dir
            results.output_files = self._save_results(
                processed_map, probability_maps, results.confidence_map,
                metadata, results.accuracy_metrics, results.landscape_metrics,
                output_dir
            )
            
            # 8. å¯è§†åŒ–
            logger.info("Step 8: Generating visualizations")
            self._generate_visualizations(
                hyperspectral_data, processed_map, probability_maps,
                results.accuracy_metrics, results.landscape_metrics,
                output_dir
            )
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            results.processing_time = time.time() - start_time
            results.success = True
            results.message = "Pipeline completed successfully"
            
            logger.info(f"Pipeline completed in {results.processing_time:.2f} seconds")
            self._print_summary(results)
            
        except Exception as e:
            results.processing_time = time.time() - start_time
            results.success = False
            results.message = f"Pipeline failed: {str(e)}"
            
            logger.error(f"Pipeline failed: {str(e)}")
            logger.error(traceback.format_exc())
        
        return results
    
    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        from .data import DataLoader, DataValidator, DataAugmentation
        from .preprocessing import (
            RadiometricCorrector, AtmosphericCorrector,
            GeometricCorrector, NoiseReducer
        )
        from .features import (
            SpectralFeatureExtractor, VegetationIndexCalculator,
            TextureFeatureExtractor, SpatialFeatureExtractor
        )
        from .classification import (
            TraditionalClassifier, DeepLearningClassifier, EnsembleClassifier
        )
        from .postprocessing import (
            SpatialFilter, MorphologyProcessor, ConsistencyChecker
        )
        from .landscape import LandscapeMetrics, ConnectivityAnalyzer
        from .evaluation import MetricsCalculator, CrossValidator, UncertaintyAnalyzer
        
        # æ•°æ®ç»„ä»¶
        self.data_loader = DataLoader(self.config)
        self.data_validator = DataValidator(self.config)
        self.data_augmentation = DataAugmentation(self.config)
        
        # é¢„å¤„ç†ç»„ä»¶
        self.radiometric_corrector = RadiometricCorrector(self.config)
        self.atmospheric_corrector = AtmosphericCorrector(self.config)
        self.geometric_corrector = GeometricCorrector(self.config)
        self.noise_reducer = NoiseReducer(self.config)
        
        # ç‰¹å¾æå–ç»„ä»¶
        self.spectral_extractor = SpectralFeatureExtractor(self.config)
        self.vegetation_calculator = VegetationIndexCalculator(self.config)
        self.texture_extractor = TextureFeatureExtractor(self.config)
        self.spatial_extractor = SpatialFeatureExtractor(self.config)
        
        # åˆ†ç±»ç»„ä»¶
        self.traditional_classifier = TraditionalClassifier(self.config)
        self.deep_learning_classifier = DeepLearningClassifier(self.config)
        self.ensemble_classifier = EnsembleClassifier(self.config)
        
        # åå¤„ç†ç»„ä»¶
        self.spatial_filter = SpatialFilter(self.config)
        self.morphology_processor = MorphologyProcessor(self.config)
        self.consistency_checker = ConsistencyChecker(self.config)
        
        # æ™¯è§‚åˆ†æç»„ä»¶
        self.landscape_metrics = LandscapeMetrics(self.config)
        self.connectivity_analyzer = ConnectivityAnalyzer(self.config)
        
        # è¯„ä¼°ç»„ä»¶
        self.metrics_calculator = MetricsCalculator(self.config)
        self.cross_validator = CrossValidator(self.config)
        self.uncertainty_analyzer = UncertaintyAnalyzer(self.config)
        
        # å·¥å…·ç»„ä»¶
        self.io_utils = IOUtils(self.config)
        self.viz_utils = VisualizationUtils(self.config)
    
    def _setup_output_dir(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        base_dir = self.config.get('output.base_dir', 'output/')
        
        if self.config.get('output.create_timestamp_dir', True):
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.output_dir = Path(base_dir) / f"classification_{timestamp}"
        else:
            self.output_dir = Path(base_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Output directory: {self.output_dir}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_dir / "pipeline.log"
        Logger.setup_logging(
            level=self.config.get('logging.level', 'INFO'),
            log_file=str(log_file)
        )
    
    def _validate_environment(self):
        """éªŒè¯è¿è¡Œç¯å¢ƒ"""
        # éªŒè¯è·¯å¾„
        self.config.validate_paths()
        
        # æ£€æŸ¥ä¾èµ–
        try:
            import torch
            import sklearn
            import rasterio
            logger.info("Required dependencies available")
        except ImportError as e:
            logger.warning(f"Missing dependency: {e}")
    
    def _load_and_validate_data(self, input_path: Union[str, Path]) -> Tuple[np.ndarray, Dict[str, Any]]:
        """åŠ è½½å’ŒéªŒè¯æ•°æ®"""
        # åŠ è½½æ•°æ®
        hyperspectral_data, metadata = self.data_loader.load_hyperspectral(input_path)
        
        # éªŒè¯æ•°æ®
        is_valid, issues = self.data_validator.validate_hyperspectral(hyperspectral_data, metadata)
        
        if not is_valid:
            logger.warning(f"Data validation issues: {issues}")
        
        return hyperspectral_data, metadata
    
    def _load_ground_truth(self, ground_truth_path: Union[str, Path], 
                          hyperspectral_data: np.ndarray) -> Tuple[Tuple, Tuple, Tuple]:
        """åŠ è½½åœ°é¢çœŸå€¼æ•°æ®"""
        # åŠ è½½æ ·æœ¬æ•°æ®
        samples = self.data_loader.load_samples(ground_truth_path)
        
        # æå–å…‰è°±å’Œæ ‡ç­¾
        spectra, labels = self.data_loader.extract_spectra_and_labels(
            hyperspectral_data, samples
        )
        
        # æ•°æ®åˆ†å‰²
        train_data, val_data, test_data = self.data_loader.split_data(
            spectra, labels, 
            train_ratio=self.config.get('data.samples.train_ratio', 0.7),
            val_ratio=self.config.get('data.samples.val_ratio', 0.15),
            test_ratio=self.config.get('data.samples.test_ratio', 0.15),
            stratify=self.config.get('data.samples.stratify', True)
        )
        
        return train_data, val_data, test_data
    
    def _preprocess_data(self, hyperspectral_data: np.ndarray, 
                        metadata: Dict[str, Any]) -> np.ndarray:
        """é¢„å¤„ç†æ•°æ®"""
        processed_data = hyperspectral_data.copy()
        
        # è¾å°„æ ¡æ­£
        if self.config.get('preprocessing.radiometric.enabled', True):
            processed_data = self.radiometric_corrector.correct(processed_data, metadata)
        
        # å¤§æ°”æ ¡æ­£
        if self.config.get('preprocessing.atmospheric.enabled', True):
            processed_data = self.atmospheric_corrector.correct(processed_data, metadata)
        
        # å‡ ä½•æ ¡æ­£
        if self.config.get('preprocessing.geometric.enabled', True):
            processed_data = self.geometric_corrector.correct(processed_data, metadata)
        
        # å™ªå£°å»é™¤
        if self.config.get('preprocessing.noise_reduction.enabled', True):
            processed_data = self.noise_reducer.reduce_noise(processed_data)
        
        return processed_data
    
    def _extract_features(self, hyperspectral_data: np.ndarray) -> np.ndarray:
        """æå–ç‰¹å¾"""
        features_list = []
        
        # å…‰è°±ç‰¹å¾
        if self.config.get('features.spectral.enabled', True):
            spectral_features = self.spectral_extractor.extract(hyperspectral_data)
            features_list.append(spectral_features)
        
        # æ¤è¢«æŒ‡æ•°
        if self.config.get('features.vegetation_indices.enabled', True):
            vegetation_indices = self.vegetation_calculator.calculate(hyperspectral_data)
            features_list.append(vegetation_indices)
        
        # çº¹ç†ç‰¹å¾
        if self.config.get('features.texture.enabled', True):
            texture_features = self.texture_extractor.extract(hyperspectral_data)
            features_list.append(texture_features)
        
        # ç©ºé—´ç‰¹å¾
        if self.config.get('features.spatial.enabled', True):
            spatial_features = self.spatial_extractor.extract(hyperspectral_data)
            features_list.append(spatial_features)
        
        # åˆå¹¶ç‰¹å¾
        if features_list:
            features = np.concatenate(features_list, axis=-1)
        else:
            features = hyperspectral_data
        
        return features
    
    def _get_classifier(self, model_type: Optional[str]):
        """è·å–åˆ†ç±»å™¨"""
        model_type = model_type or self.config.get('classification.default_classifier')
        
        if model_type in ['svm', 'random_forest', 'xgboost', 'knn']:
            return self.traditional_classifier
        elif model_type in ['cnn_3d', 'hybrid_cnn', 'vision_transformer', 'resnet_hs']:
            return self.deep_learning_classifier
        elif model_type in ['voting_ensemble', 'stacking_ensemble', 'weighted_ensemble']:
            return self.ensemble_classifier
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    def _train_classifier(self, classifier, train_data: Tuple, 
                         val_data: Tuple, features: np.ndarray):
        """è®­ç»ƒåˆ†ç±»å™¨"""
        return classifier.train(train_data, val_data, features)
    
    def _load_pretrained_model(self, classifier):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        return classifier.load_pretrained()
    
    def _predict_classification(self, classifier, features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """åˆ†ç±»é¢„æµ‹"""
        return classifier.predict(features)
    
    def _evaluate_accuracy(self, classifier, test_data: Tuple, 
                          features: np.ndarray) -> Tuple[Dict[str, float], np.ndarray, Dict[str, Any]]:
        """ç²¾åº¦è¯„ä¼°"""
        return self.metrics_calculator.evaluate(classifier, test_data, features)
    
    def _postprocess_results(self, classification_map: np.ndarray) -> np.ndarray:
        """åå¤„ç†ç»“æœ"""
        processed_map = classification_map.copy()
        
        # ç©ºé—´æ»¤æ³¢
        if self.config.get('postprocessing.spatial_filter.enabled', True):
            processed_map = self.spatial_filter.filter(processed_map)
        
        # å½¢æ€å­¦æ“ä½œ
        if self.config.get('postprocessing.morphology.enabled', True):
            processed_map = self.morphology_processor.process(processed_map)
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        if self.config.get('postprocessing.consistency.enabled', True):
            processed_map = self.consistency_checker.check(processed_map)
        
        return processed_map
    
    def _calculate_confidence(self, probability_maps: np.ndarray) -> np.ndarray:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # ä½¿ç”¨æœ€å¤§æ¦‚ç‡ä½œä¸ºç½®ä¿¡åº¦
        confidence_map = np.max(probability_maps, axis=-1)
        return confidence_map
    
    def _analyze_landscape(self, classification_map: np.ndarray) -> Dict[str, float]:
        """æ™¯è§‚åˆ†æ"""
        metrics = {}
        
        # æ™¯è§‚æŒ‡æ•°
        if self.config.get('landscape.metrics.enabled', True):
            landscape_metrics = self.landscape_metrics.calculate(classification_map)
            metrics.update(landscape_metrics)
        
        # è¿é€šæ€§åˆ†æ
        if self.config.get('landscape.connectivity.enabled', True):
            connectivity_metrics = self.connectivity_analyzer.analyze(classification_map)
            metrics.update(connectivity_metrics)
        
        return metrics
    
    def _save_results(self, classification_map: np.ndarray, 
                     probability_maps: Optional[np.ndarray],
                     confidence_map: Optional[np.ndarray],
                     metadata: Dict[str, Any],
                     accuracy_metrics: Dict[str, float],
                     landscape_metrics: Dict[str, float],
                     output_dir: Path) -> Dict[str, str]:
        """ä¿å­˜ç»“æœ"""
        output_files = {}
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        classification_file = output_dir / "classification_map.tif"
        self.io_utils.save_raster(classification_map, classification_file, metadata)
        output_files['classification_map'] = str(classification_file)
        
        # ä¿å­˜æ¦‚ç‡å›¾
        if probability_maps is not None:
            probability_file = output_dir / "probability_maps.tif"
            self.io_utils.save_raster(probability_maps, probability_file, metadata)
            output_files['probability_maps'] = str(probability_file)
        
        # ä¿å­˜ç½®ä¿¡åº¦å›¾
        if confidence_map is not None:
            confidence_file = output_dir / "confidence_map.tif"
            self.io_utils.save_raster(confidence_map, confidence_file, metadata)
            output_files['confidence_map'] = str(confidence_file)
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        report = {
            'accuracy_metrics': accuracy_metrics,
            'landscape_metrics': landscape_metrics,
            'processing_config': self.config.to_dict()
        }
        
        report_file = output_dir / "classification_report.json"
        self.io_utils.save_json(report, report_file)
        output_files['report'] = str(report_file)
        
        return output_files
    
    def _generate_visualizations(self, hyperspectral_data: np.ndarray,
                               classification_map: np.ndarray,
                               probability_maps: Optional[np.ndarray],
                               accuracy_metrics: Dict[str, float],
                               landscape_metrics: Dict[str, float],
                               output_dir: Path):
        """ç”Ÿæˆå¯è§†åŒ–"""
        # åˆ†ç±»ç»“æœå¯è§†åŒ–
        self.viz_utils.plot_classification_map(
            classification_map, 
            save_path=output_dir / "classification_visualization.png"
        )
        
        # RGBå¤åˆå›¾
        rgb_image = self.viz_utils.create_rgb_composite(hyperspectral_data)
        self.viz_utils.save_image(rgb_image, output_dir / "rgb_composite.png")
        
        # ç²¾åº¦è¯„ä¼°å›¾è¡¨
        if accuracy_metrics:
            self.viz_utils.plot_accuracy_metrics(
                accuracy_metrics,
                save_path=output_dir / "accuracy_metrics.png"
            )
        
        # æ™¯è§‚æŒ‡æ•°å›¾è¡¨
        if landscape_metrics:
            self.viz_utils.plot_landscape_metrics(
                landscape_metrics,
                save_path=output_dir / "landscape_metrics.png"
            )
        
        # æ¦‚ç‡åˆ†å¸ƒå›¾
        if probability_maps is not None:
            self.viz_utils.plot_probability_distribution(
                probability_maps,
                save_path=output_dir / "probability_distribution.png"
            )
    
    def _print_summary(self, results: PipelineResults):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸŒ¿ æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ç»“æœæ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ“Š å¤„ç†çŠ¶æ€: {'âœ… æˆåŠŸ' if results.success else 'âŒ å¤±è´¥'}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {results.processing_time:.2f} ç§’")
        
        if results.input_shape:
            print(f"ğŸ“¥ è¾“å…¥å°ºå¯¸: {results.input_shape}")
        if results.output_shape:
            print(f"ğŸ“¤ è¾“å‡ºå°ºå¯¸: {results.output_shape}")
        if results.num_samples > 0:
            print(f"ğŸ¯ è®­ç»ƒæ ·æœ¬: {results.num_samples}")
        
        if results.accuracy_metrics:
            print("\nğŸ“ˆ ç²¾åº¦æŒ‡æ ‡:")
            for metric, value in results.accuracy_metrics.items():
                print(f"  {metric}: {value:.3f}")
        
        if results.landscape_metrics:
            print("\nğŸŒ æ™¯è§‚æŒ‡æ ‡:")
            for metric, value in list(results.landscape_metrics.items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"  {metric}: {value:.3f}")
        
        if results.output_files:
            print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
            for file_type, file_path in results.output_files.items():
                print(f"  {file_type}: {file_path}")
        
        print("="*60)