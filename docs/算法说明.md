# 湿地高光谱分类系统算法说明
## Wetland Hyperspectral Classification System - Algorithm Documentation

### 📋 目录

1. [算法概述](#算法概述)
2. [数据预处理算法](#数据预处理算法)
3. [特征提取算法](#特征提取算法)
4. [传统机器学习算法](#传统机器学习算法)
5. [深度学习算法](#深度学习算法)
6. [集成学习算法](#集成学习算法)
7. [后处理算法](#后处理算法)
8. [景观分析算法](#景观分析算法)
9. [评估算法](#评估算法)
10. [算法比较与选择](#算法比较与选择)

---

## 🎯 算法概述

### 系统算法架构

湿地高光谱分类系统采用多层次、模块化的算法架构，包含以下主要组件：

```
输入：高光谱数据 (H×W×B)
         ↓
    数据预处理层
    ├── 辐射定标
    ├── 大气校正  
    ├── 几何校正
    └── 噪声去除
         ↓
    特征提取层
    ├── 光谱特征
    ├── 植被指数
    ├── 纹理特征
    └── 空间特征
         ↓
    分类算法层
    ├── 传统机器学习 (SVM, RF, XGBoost)
    ├── 深度学习 (3D-CNN, HybridSN, ViT)
    └── 集成学习 (Voting, Stacking)
         ↓
    后处理层
    ├── 空间滤波
    ├── 形态学操作
    └── 一致性检查
         ↓
输出：分类结果图 (H×W)
```

### 算法设计原则

1. **精度优先**: 确保分类精度满足实际应用需求
2. **效率平衡**: 在精度和计算效率间找到最佳平衡
3. **鲁棒性强**: 对噪声和异常值具有良好的抗干扰能力
4. **可扩展性**: 支持新算法的快速集成和部署
5. **自适应性**: 能够根据数据特点自动调整参数

---

## 🔧 数据预处理算法

### 1. 辐射定标算法

#### 算法原理
将原始DN值转换为物理意义的辐射亮度值。

#### 数学模型
```
L = (DN - DN_dark) × Gain + Offset
```

其中：
- L: 辐射亮度 (W/m²/sr/μm)
- DN: 数字量化值
- DN_dark: 暗电流值
- Gain: 增益系数
- Offset: 偏移量

#### 实现算法
```python
def radiometric_calibration(dn_values, gain, offset, dark_current=0):
    """
    辐射定标算法实现
    
    Args:
        dn_values: 原始DN值 (H, W, B)
        gain: 增益系数 (B,)
        offset: 偏移量 (B,)
        dark_current: 暗电流值 (B,)
    
    Returns:
        radiance: 辐射亮度值 (H, W, B)
    """
    # 去除暗电流
    corrected_dn = dn_values - dark_current
    
    # 应用增益和偏移
    radiance = corrected_dn * gain + offset
    
    # 确保数值为正
    radiance = np.maximum(radiance, 0)
    
    return radiance
```

### 2. 大气校正算法

#### FLAASH算法
基于MODTRAN辐射传输模型的大气校正算法。

#### 算法原理
大气校正的核心方程：
```
ρ = π × (L - L_p) / (τ × E_sun × cos(θ) / π + E_down)
```

其中：
- ρ: 地表反射率
- L: 传感器接收的辐射亮度
- L_p: 大气程辐射
- τ: 大气透过率
- E_sun: 太阳辐照度
- E_down: 下行漫射辐照度
- θ: 太阳天顶角

#### 实现算法
```python
def atmospheric_correction_flaash(radiance, solar_irradiance, 
                                 solar_zenith, atmospheric_params):
    """
    FLAASH大气校正算法
    
    Args:
        radiance: 辐射亮度 (H, W, B)
        solar_irradiance: 太阳辐照度 (B,)
        solar_zenith: 太阳天顶角
        atmospheric_params: 大气参数字典
    
    Returns:
        reflectance: 地表反射率 (H, W, B)
    """
    # 计算大气参数
    path_radiance = atmospheric_params['path_radiance']
    transmittance = atmospheric_params['transmittance']
    downwelling = atmospheric_params['downwelling']
    
    # 大气校正计算
    cos_solar_zenith = np.cos(np.radians(solar_zenith))
    solar_term = solar_irradiance * cos_solar_zenith / np.pi
    
    numerator = np.pi * (radiance - path_radiance)
    denominator = transmittance * solar_term + downwelling
    
    reflectance = numerator / (denominator + 1e-8)
    
    # 限制反射率范围
    reflectance = np.clip(reflectance, 0, 1)
    
    return reflectance
```

### 3. 噪声去除算法

#### 最小噪声分离变换 (MNF)
基于信噪比分离的降维算法。

#### 算法原理
1. 估计噪声协方差矩阵
2. 计算信号协方差矩阵
3. 求解广义特征值问题
4. 选择高信噪比成分

#### 实现算法
```python
def minimum_noise_fraction(data, n_components=50):
    """
    最小噪声分离变换
    
    Args:
        data: 高光谱数据 (N_pixels, N_bands)
        n_components: 保留的成分数
    
    Returns:
        transformed_data: 变换后的数据
        eigenvalues: 特征值
        eigenvectors: 特征向量
    """
    # 估计噪声协方差矩阵
    noise_cov = estimate_noise_covariance(data)
    
    # 计算数据协方差矩阵
    data_cov = np.cov(data.T)
    
    # 求解广义特征值问题
    eigenvalues, eigenvectors = scipy.linalg.eigh(data_cov, noise_cov)
    
    # 按特征值降序排列
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # 选择前n_components个成分
    transform_matrix = eigenvectors[:, :n_components]
    transformed_data = data @ transform_matrix
    
    return transformed_data, eigenvalues, eigenvectors
```

---

## 🎨 特征提取算法

### 1. 光谱特征提取

#### 光谱导数
用于突出光谱特征和减少环境影响。

#### 一阶导数
```python
def first_derivative(spectrum):
    """计算一阶光谱导数"""
    return np.gradient(spectrum)

def second_derivative(spectrum):
    """计算二阶光谱导数"""
    first_deriv = np.gradient(spectrum)
    return np.gradient(first_deriv)
```

#### 连续统去除 (Continuum Removal)
```python
def continuum_removal(spectrum):
    """
    连续统去除算法
    
    Args:
        spectrum: 光谱曲线 (N_bands,)
    
    Returns:
        cr_spectrum: 连续统去除后的光谱
    """
    # 计算凸包
    hull = ConvexHull(np.column_stack([range(len(spectrum)), spectrum]))
    hull_points = hull.vertices
    
    # 构建连续统线
    continuum = np.interp(range(len(spectrum)), 
                         hull_points, spectrum[hull_points])
    
    # 连续统去除
    cr_spectrum = spectrum / (continuum + 1e-8)
    
    return cr_spectrum
```

### 2. 植被指数算法

#### 归一化差异植被指数 (NDVI)
```python
def calculate_ndvi(nir, red):
    """
    NDVI = (NIR - Red) / (NIR + Red)
    
    Args:
        nir: 近红外波段
        red: 红光波段
    
    Returns:
        ndvi: NDVI指数
    """
    return (nir - red) / (nir + red + 1e-8)
```

#### 增强植被指数 (EVI)
```python
def calculate_evi(nir, red, blue, L=1, C1=6, C2=7.5, G=2.5):
    """
    EVI = G × (NIR - Red) / (NIR + C1×Red - C2×Blue + L)
    
    Args:
        nir: 近红外波段
        red: 红光波段  
        blue: 蓝光波段
        L, C1, C2, G: EVI参数
    
    Returns:
        evi: EVI指数
    """
    numerator = G * (nir - red)
    denominator = nir + C1 * red - C2 * blue + L
    return numerator / (denominator + 1e-8)
```

#### 红边位置 (Red Edge Position)
```python
def red_edge_position(spectrum, wavelengths):
    """
    计算红边位置
    
    Args:
        spectrum: 光谱数据
        wavelengths: 波长数组
    
    Returns:
        rep: 红边位置 (nm)
    """
    # 红边范围 (690-740 nm)
    red_edge_mask = (wavelengths >= 690) & (wavelengths <= 740)
    red_edge_spectrum = spectrum[red_edge_mask]
    red_edge_wavelengths = wavelengths[red_edge_mask]
    
    # 计算一阶导数
    derivative = np.gradient(red_edge_spectrum)
    
    # 找到最大导数位置
    max_deriv_idx = np.argmax(derivative)
    rep = red_edge_wavelengths[max_deriv_idx]
    
    return rep
```

### 3. 纹理特征算法

#### 灰度共生矩阵 (GLCM)
```python
def calculate_glcm_features(image, distances=[1], angles=[0, 45, 90, 135]):
    """
    计算GLCM纹理特征
    
    Args:
        image: 输入图像
        distances: 像素距离列表
        angles: 方向角度列表
    
    Returns:
        features: 纹理特征字典
    """
    from skimage.feature import graycomatrix, graycoprops
    
    # 计算GLCM矩阵
    glcm = graycomatrix(image, distances, angles, 
                       levels=256, symmetric=True, normed=True)
    
    # 计算纹理特征
    features = {}
    features['contrast'] = graycoprops(glcm, 'contrast').mean()
    features['dissimilarity'] = graycoprops(glcm, 'dissimilarity').mean()
    features['homogeneity'] = graycoprops(glcm, 'homogeneity').mean()
    features['energy'] = graycoprops(glcm, 'energy').mean()
    features['correlation'] = graycoprops(glcm, 'correlation').mean()
    features['ASM'] = graycoprops(glcm, 'ASM').mean()
    
    return features
```

#### 局部二值模式 (LBP)
```python
def local_binary_pattern(image, radius=3, n_points=24):
    """
    计算局部二值模式
    
    Args:
        image: 输入图像
        radius: 圆形邻域半径
        n_points: 采样点数
    
    Returns:
        lbp: LBP特征图
    """
    from skimage.feature import local_binary_pattern as skimage_lbp
    
    lbp = skimage_lbp(image, n_points, radius, method='uniform')
    
    # 计算LBP直方图
    hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, 
                          range=(0, n_points + 2))
    
    # 归一化
    hist = hist.astype(float) / (hist.sum() + 1e-8)
    
    return lbp, hist
```

---

## 🤖 传统机器学习算法

### 1. 支持向量机 (SVM)

#### 算法原理
SVM通过寻找最优超平面来分离不同类别。对于非线性问题，使用核函数映射到高维空间。

#### 核函数
1. **线性核**: K(x, y) = x^T y
2. **多项式核**: K(x, y) = (γx^T y + r)^d
3. **RBF核**: K(x, y) = exp(-γ||x - y||²)
4. **Sigmoid核**: K(x, y) = tanh(γx^T y + r)

#### 优化目标
```
min (1/2)||w||² + C∑ξᵢ
约束条件: yᵢ(w^T φ(xᵢ) + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0
```

#### 实现代码
```python
class HyperspectralSVM:
    def __init__(self, kernel='rbf', C=1.0, gamma='scale'):
        """
        高光谱SVM分类器
        
        Args:
            kernel: 核函数类型
            C: 惩罚参数
            gamma: 核函数参数
        """
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.model = SVC(kernel=kernel, C=C, gamma=gamma, 
                        probability=True, random_state=42)
    
    def fit(self, X, y):
        """训练SVM模型"""
        # 数据标准化
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # 训练模型
        self.model.fit(X_scaled, y)
        
        return self
    
    def predict(self, X):
        """预测类别"""
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
```

### 2. 随机森林 (Random Forest)

#### 算法原理
随机森林是基于决策树的集成学习算法，通过Bootstrap采样和随机特征选择来减少过拟合。

#### Bagging策略
1. 从训练集中有放回抽样生成新的训练集
2. 在每个节点随机选择部分特征进行分裂
3. 构建多个决策树
4. 通过投票或平均进行预测

#### 实现代码
```python
class HyperspectralRandomForest:
    def __init__(self, n_estimators=100, max_depth=None, 
                 max_features='sqrt', random_state=42):
        """
        高光谱随机森林分类器
        
        Args:
            n_estimators: 决策树数量
            max_depth: 最大深度
            max_features: 每次分裂考虑的最大特征数
        """
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            max_features=max_features,
            random_state=random_state,
            n_jobs=-1
        )
    
    def fit(self, X, y):
        """训练随机森林模型"""
        self.model.fit(X, y)
        return self
    
    def predict(self, X):
        """预测类别"""
        return self.model.predict(X)
    
    def feature_importance(self):
        """获取特征重要性"""
        return self.model.feature_importances_
```

### 3. 极端梯度提升 (XGBoost)

#### 算法原理
XGBoost是基于梯度提升的集成学习算法，通过逐步添加弱学习器来优化目标函数。

#### 目标函数
```
Obj = ∑L(yᵢ, ŷᵢ) + ∑Ω(fₖ)
```

其中：
- L: 损失函数
- Ω: 正则化项
- fₖ: 第k个弱学习器

#### 实现代码
```python
class HyperspectralXGBoost:
    def __init__(self, n_estimators=100, learning_rate=0.1, 
                 max_depth=6, subsample=0.8):
        """
        高光谱XGBoost分类器
        
        Args:
            n_estimators: 弱学习器数量
            learning_rate: 学习率
            max_depth: 最大深度
            subsample: 子采样比例
        """
        self.model = XGBClassifier(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            subsample=subsample,
            random_state=42,
            n_jobs=-1
        )
    
    def fit(self, X, y, eval_set=None, early_stopping_rounds=None):
        """训练XGBoost模型"""
        self.model.fit(X, y, eval_set=eval_set, 
                      early_stopping_rounds=early_stopping_rounds,
                      verbose=False)
        return self
    
    def predict(self, X):
        """预测类别"""
        return self.model.predict(X)
```

---

## 🧠 深度学习算法

### 1. 3D卷积神经网络 (3D-CNN)

#### 算法原理
3D-CNN能够同时提取高光谱数据的空间和光谱特征，通过3D卷积核在三个维度上进行特征提取。

#### 网络架构
```python
class HyperspectralCNN3D(nn.Module):
    def __init__(self, input_channels, num_classes, patch_size=9):
        """
        3D-CNN网络架构
        
        Args:
            input_channels: 输入光谱波段数
            num_classes: 类别数
            patch_size: 图像块大小
        """
        super(HyperspectralCNN3D, self).__init__()
        
        # 3D卷积层
        self.conv3d1 = nn.Conv3d(1, 8, kernel_size=(7, 3, 3), padding=(0, 1, 1))
        self.conv3d2 = nn.Conv3d(8, 16, kernel_size=(5, 3, 3), padding=(0, 1, 1))
        self.conv3d3 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=(0, 1, 1))
        
        # 批归一化
        self.bn1 = nn.BatchNorm3d(8)
        self.bn2 = nn.BatchNorm3d(16)
        self.bn3 = nn.BatchNorm3d(32)
        
        # Dropout
        self.dropout = nn.Dropout3d(0.4)
        
        # 全连接层
        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(32, num_classes)
        
    def forward(self, x):
        # x shape: (batch_size, 1, bands, height, width)
        x = F.relu(self.bn1(self.conv3d1(x)))
        x = F.relu(self.bn2(self.conv3d2(x)))
        x = F.relu(self.bn3(self.conv3d3(x)))
        
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x
```

### 2. 混合卷积网络 (HybridSN)

#### 算法原理
HybridSN结合3D和2D卷积，先用3D卷积提取光谱-空间特征，再用2D卷积进一步提取空间特征。

#### 网络架构
```python
class HybridSN(nn.Module):
    def __init__(self, input_channels, num_classes, patch_size=25):
        super(HybridSN, self).__init__()
        
        # 3D卷积分支
        self.conv3d1 = nn.Conv3d(1, 8, kernel_size=(7, 3, 3))
        self.conv3d2 = nn.Conv3d(8, 16, kernel_size=(5, 3, 3))
        self.conv3d3 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3))
        
        # 2D卷积分支
        self.conv2d1 = nn.Conv2d(32 * (input_channels - 12), 64, kernel_size=3)
        self.conv2d2 = nn.Conv2d(64, 128, kernel_size=3)
        
        # 全连接层
        self.fc1 = nn.Linear(128 * (patch_size - 8) * (patch_size - 8), 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)
        
        self.dropout = nn.Dropout(0.4)
        
    def forward(self, x):
        # 3D卷积部分
        x = F.relu(self.conv3d1(x))
        x = F.relu(self.conv3d2(x))
        x = F.relu(self.conv3d3(x))
        
        # 重塑为2D
        x = x.view(x.size(0), -1, x.size(3), x.size(4))
        
        # 2D卷积部分
        x = F.relu(self.conv2d1(x))
        x = F.relu(self.conv2d2(x))
        
        # 全连接部分
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x
```

### 3. Vision Transformer (ViT)

#### 算法原理
ViT将图像分割为patches，通过自注意力机制学习全局特征关系。

#### 核心组件
1. **Patch Embedding**: 将图像块转换为词向量
2. **Position Encoding**: 添加位置信息
3. **Multi-Head Attention**: 多头自注意力机制
4. **Feed Forward Network**: 前馈神经网络

#### 实现代码
```python
class HyperspectralViT(nn.Module):
    def __init__(self, input_channels, num_classes, patch_size=8, 
                 dim=512, depth=6, heads=8, mlp_dim=1024):
        super(HyperspectralViT, self).__init__()
        
        self.patch_size = patch_size
        self.dim = dim
        
        # Patch Embedding
        self.patch_embed = nn.Conv2d(input_channels, dim, 
                                   kernel_size=patch_size, stride=patch_size)
        
        # Position Embedding
        self.pos_embed = nn.Parameter(torch.randn(1, 1000, dim))
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        
        # Classification Head
        self.norm = nn.LayerNorm(dim)
        self.fc = nn.Linear(dim, num_classes)
        
    def forward(self, x):
        # Patch Embedding
        x = self.patch_embed(x)  # (B, dim, H/p, W/p)
        x = x.flatten(2).transpose(1, 2)  # (B, N, dim)
        
        # Add Position Embedding
        x = x + self.pos_embed[:, :x.size(1)]
        
        # Transformer Encoding
        x = self.transformer(x)
        
        # Global Average Pooling
        x = x.mean(dim=1)
        
        # Classification
        x = self.norm(x)
        x = self.fc(x)
        
        return x
```

---

## 🔗 集成学习算法

### 1. 投票集成 (Voting Ensemble)

#### 算法原理
通过多个基分类器的投票来做最终决策。

#### 硬投票
```python
def hard_voting_predict(predictions_list):
    """
    硬投票预测
    
    Args:
        predictions_list: 基分类器预测结果列表
    
    Returns:
        final_predictions: 投票结果
    """
    predictions_array = np.array(predictions_list)
    final_predictions = []
    
    for i in range(predictions_array.shape[1]):
        # 统计每个类别的票数
        votes = predictions_array[:, i]
        final_pred = np.bincount(votes).argmax()
        final_predictions.append(final_pred)
    
    return np.array(final_predictions)
```

#### 软投票
```python
def soft_voting_predict(probabilities_list, weights=None):
    """
    软投票预测
    
    Args:
        probabilities_list: 基分类器概率预测列表
        weights: 权重
    
    Returns:
        final_predictions: 加权投票结果
    """
    if weights is None:
        weights = np.ones(len(probabilities_list)) / len(probabilities_list)
    
    # 加权平均概率
    weighted_probs = np.zeros_like(probabilities_list[0])
    for i, probs in enumerate(probabilities_list):
        weighted_probs += weights[i] * probs
    
    # 选择最大概率的类别
    final_predictions = np.argmax(weighted_probs, axis=1)
    
    return final_predictions, weighted_probs
```

### 2. 堆叠集成 (Stacking)

#### 算法原理
使用元学习器学习如何最优地组合基分类器的输出。

#### 实现代码
```python
class StackingEnsemble:
    def __init__(self, base_classifiers, meta_classifier):
        """
        堆叠集成分类器
        
        Args:
            base_classifiers: 基分类器列表
            meta_classifier: 元分类器
        """
        self.base_classifiers = base_classifiers
        self.meta_classifier = meta_classifier
        
    def fit(self, X, y, cv=5):
        """训练堆叠集成模型"""
        from sklearn.model_selection import cross_val_predict
        
        # 训练基分类器并生成元特征
        meta_features = []
        
        for clf in self.base_classifiers:
            # 交叉验证预测
            cv_pred = cross_val_predict(clf, X, y, cv=cv, method='predict_proba')
            meta_features.append(cv_pred)
            
            # 在全部数据上训练
            clf.fit(X, y)
        
        # 组合元特征
        meta_X = np.column_stack(meta_features)
        
        # 训练元分类器
        self.meta_classifier.fit(meta_X, y)
        
        return self
    
    def predict(self, X):
        """预测"""
        # 基分类器预测
        base_predictions = []
        for clf in self.base_classifiers:
            pred = clf.predict_proba(X)
            base_predictions.append(pred)
        
        # 组合预测结果
        meta_X = np.column_stack(base_predictions)
        
        # 元分类器预测
        final_pred = self.meta_classifier.predict(meta_X)
        
        return final_pred
```

---

## 🔄 后处理算法

### 1. 空间滤波算法

#### 多数滤波 (Majority Filter)
```python
def majority_filter(classification_map, window_size=3):
    """
    多数滤波算法
    
    Args:
        classification_map: 分类结果图
        window_size: 滤波窗口大小
    
    Returns:
        filtered_map: 滤波后的分类图
    """
    from scipy import ndimage
    
    filtered_map = np.copy(classification_map)
    height, width = classification_map.shape
    
    pad_size = window_size // 2
    padded_map = np.pad(classification_map, pad_size, mode='reflect')
    
    for i in range(height):
        for j in range(width):
            # 提取窗口
            window = padded_map[i:i+window_size, j:j+window_size]
            
            # 计算多数类别
            unique, counts = np.unique(window, return_counts=True)
            majority_class = unique[np.argmax(counts)]
            
            filtered_map[i, j] = majority_class
    
    return filtered_map
```

### 2. 形态学操作

#### 开运算和闭运算
```python
def morphological_operations(classification_map, operation='opening', 
                           kernel_size=3, iterations=1):
    """
    形态学操作
    
    Args:
        classification_map: 分类结果图
        operation: 操作类型 ('opening', 'closing', 'erosion', 'dilation')
        kernel_size: 结构元素大小
        iterations: 迭代次数
    
    Returns:
        processed_map: 处理后的分类图
    """
    from scipy import ndimage
    
    # 创建结构元素
    kernel = np.ones((kernel_size, kernel_size))
    
    processed_map = np.copy(classification_map)
    
    if operation == 'erosion':
        processed_map = ndimage.binary_erosion(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    elif operation == 'dilation':
        processed_map = ndimage.binary_dilation(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    elif operation == 'opening':
        processed_map = ndimage.binary_opening(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    elif operation == 'closing':
        processed_map = ndimage.binary_closing(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    
    return processed_map
```

---

## 🌿 景观分析算法

### 1. 景观指数计算

#### 香农多样性指数
```python
def shannon_diversity_index(classification_map):
    """
    计算香农多样性指数
    
    H = -∑(p_i × ln(p_i))
    
    Args:
        classification_map: 分类结果图
    
    Returns:
        shannon_index: 香农多样性指数
    """
    # 计算各类别比例
    unique_classes, counts = np.unique(classification_map, return_counts=True)
    proportions = counts / counts.sum()
    
    # 计算香农指数
    shannon_index = -np.sum(proportions * np.log(proportions + 1e-8))
    
    return shannon_index
```

#### 聚集指数
```python
def aggregation_index(classification_map):
    """
    计算聚集指数
    
    AI = 100 × (g_ii / max_g_ii)
    
    Args:
        classification_map: 分类结果图
    
    Returns:
        ai: 聚集指数
    """
    height, width = classification_map.shape
    total_adjacencies = 0
    same_class_adjacencies = 0
    
    # 计算相邻像素对
    for i in range(height):
        for j in range(width):
            current_class = classification_map[i, j]
            
            # 检查右邻居
            if j < width - 1:
                total_adjacencies += 1
                if classification_map[i, j + 1] == current_class:
                    same_class_adjacencies += 1
            
            # 检查下邻居  
            if i < height - 1:
                total_adjacencies += 1
                if classification_map[i + 1, j] == current_class:
                    same_class_adjacencies += 1
    
    # 计算聚集指数
    ai = 100 * same_class_adjacencies / total_adjacencies if total_adjacencies > 0 else 0
    
    return ai
```

### 2. 连通性分析

#### 连通组件标记
```python
def connectivity_analysis(classification_map, class_id, connectivity=8):
    """
    连通性分析
    
    Args:
        classification_map: 分类结果图
        class_id: 目标类别ID
        connectivity: 连通性 (4 或 8)
    
    Returns:
        results: 连通性分析结果
    """
    from scipy import ndimage
    
    # 创建二值图
    binary_map = (classification_map == class_id).astype(int)
    
    # 连通组件标记
    if connectivity == 4:
        structure = ndimage.generate_binary_structure(2, 1)
    else:  # connectivity == 8
        structure = ndimage.generate_binary_structure(2, 2)
    
    labeled_map, num_features = ndimage.label(binary_map, structure=structure)
    
    # 计算各连通组件的大小
    component_sizes = ndimage.sum(binary_map, labeled_map, 
                                 range(1, num_features + 1))
    
    results = {
        'num_components': num_features,
        'component_sizes': component_sizes,
        'largest_component': np.max(component_sizes) if num_features > 0 else 0,
        'mean_component_size': np.mean(component_sizes) if num_features > 0 else 0,
        'labeled_map': labeled_map
    }
    
    return results
```

---

## 📊 评估算法

### 1. 精度评估

#### 混淆矩阵
```python
def confusion_matrix(y_true, y_pred, num_classes):
    """
    计算混淆矩阵
    
    Args:
        y_true: 真实标签
        y_pred: 预测标签
        num_classes: 类别数
    
    Returns:
        cm: 混淆矩阵
    """
    cm = np.zeros((num_classes, num_classes), dtype=int)
    
    for true_label, pred_label in zip(y_true, y_pred):
        cm[true_label, pred_label] += 1
    
    return cm
```

#### Kappa系数
```python
def kappa_coefficient(y_true, y_pred):
    """
    计算Kappa系数
    
    κ = (p_o - p_e) / (1 - p_e)
    
    Args:
        y_true: 真实标签
        y_pred: 预测标签
    
    Returns:
        kappa: Kappa系数
    """
    from sklearn.metrics import confusion_matrix
    
    cm = confusion_matrix(y_true, y_pred)
    n = np.sum(cm)
    
    # 观测一致性
    p_o = np.trace(cm) / n
    
    # 期望一致性
    row_sums = np.sum(cm, axis=1)
    col_sums = np.sum(cm, axis=0)
    p_e = np.sum(row_sums * col_sums) / (n * n)
    
    # Kappa系数
    kappa = (p_o - p_e) / (1 - p_e) if p_e != 1 else 0
    
    return kappa
```

### 2. 统计显著性检验

#### McNemar检验
```python
def mcnemar_test(y_true, y_pred1, y_pred2):
    """
    McNemar检验比较两个分类器
    
    Args:
        y_true: 真实标签
        y_pred1: 分类器1预测
        y_pred2: 分类器2预测
    
    Returns:
        statistic: 检验统计量
        p_value: p值
    """
    # 构建2x2列联表
    correct1 = (y_pred1 == y_true)
    correct2 = (y_pred2 == y_true)
    
    a = np.sum(correct1 & correct2)      # 都正确
    b = np.sum(correct1 & ~correct2)     # 1正确，2错误
    c = np.sum(~correct1 & correct2)     # 1错误，2正确
    d = np.sum(~correct1 & ~correct2)    # 都错误
    
    # McNemar统计量
    statistic = (abs(b - c) - 1) ** 2 / (b + c) if (b + c) > 0 else 0
    
    # p值 (卡方分布)
    from scipy.stats import chi2
    p_value = 1 - chi2.cdf(statistic, df=1)
    
    return statistic, p_value
```

---

## 📈 算法比较与选择

### 1. 算法性能对比

#### 计算复杂度
| 算法 | 训练复杂度 | 预测复杂度 | 内存复杂度 |
|------|------------|------------|------------|
| SVM | O(n³) | O(sv×d) | O(sv×d) |
| Random Forest | O(n×log n×d×t) | O(t×log n) | O(t×n×d) |
| XGBoost | O(n×d×t) | O(t×log n) | O(n×d) |
| 3D-CNN | O(epochs×n×operations) | O(operations) | O(model_size) |
| HybridSN | O(epochs×n×operations) | O(operations) | O(model_size) |

其中：
- n: 样本数
- d: 特征维度
- t: 树的数量
- sv: 支持向量数

#### 适用场景
```python
def algorithm_selection_guide(dataset_size, feature_dim, 
                            computational_budget, accuracy_requirement):
    """
    算法选择指南
    
    Args:
        dataset_size: 数据集大小
        feature_dim: 特征维度
        computational_budget: 计算预算 ('low', 'medium', 'high')
        accuracy_requirement: 精度要求 ('low', 'medium', 'high')
    
    Returns:
        recommended_algorithms: 推荐算法列表
    """
    recommendations = []
    
    if computational_budget == 'low':
        if dataset_size < 10000:
            recommendations.extend(['SVM', 'Random Forest'])
        else:
            recommendations.append('Random Forest')
    
    elif computational_budget == 'medium':
        recommendations.extend(['Random Forest', 'XGBoost'])
        if dataset_size > 5000:
            recommendations.append('3D-CNN')
    
    elif computational_budget == 'high':
        if accuracy_requirement == 'high':
            recommendations.extend(['HybridSN', 'Vision Transformer', 'Ensemble'])
        else:
            recommendations.extend(['3D-CNN', 'XGBoost'])
    
    # 根据特征维度调整
    if feature_dim > 200:
        if 'SVM' in recommendations:
            recommendations.remove('SVM')  # SVM在高维特征时性能下降
    
    return recommendations
```

### 2. 超参数优化策略

#### 贝叶斯优化
```python
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical

def bayesian_optimization(model_type, X_train, y_train, X_val, y_val):
    """
    贝叶斯超参数优化
    
    Args:
        model_type: 模型类型
        X_train, y_train: 训练数据
        X_val, y_val: 验证数据
    
    Returns:
        best_params: 最优参数
    """
    def objective(params):
        if model_type == 'svm':
            C, gamma = params
            model = SVC(C=C, gamma=gamma)
        elif model_type == 'random_forest':
            n_estimators, max_depth = params
            model = RandomForestClassifier(
                n_estimators=n_estimators, 
                max_depth=max_depth
            )
        
        model.fit(X_train, y_train)
        predictions = model.predict(X_val)
        accuracy = np.mean(predictions == y_val)
        
        return -accuracy  # 最小化负精度
    
    if model_type == 'svm':
        space = [Real(0.1, 100, prior='log-uniform'),    # C
                Real(1e-6, 1e-1, prior='log-uniform')]   # gamma
    elif model_type == 'random_forest':
        space = [Integer(50, 500),     # n_estimators
                Integer(5, 50)]        # max_depth
    
    result = gp_minimize(objective, space, n_calls=50, random_state=42)
    
    return result.x
```

### 3. 模型集成策略

#### 动态权重集成
```python
def dynamic_ensemble_weights(classifiers, X_val, y_val):
    """
    基于验证集性能动态计算集成权重
    
    Args:
        classifiers: 基分类器列表
        X_val, y_val: 验证数据
    
    Returns:
        weights: 归一化权重
    """
    accuracies = []
    
    for clf in classifiers:
        predictions = clf.predict(X_val)
        accuracy = np.mean(predictions == y_val)
        accuracies.append(accuracy)
    
    # 使用softmax计算权重
    accuracies = np.array(accuracies)
    exp_acc = np.exp(accuracies * 5)  # 温度参数=5
    weights = exp_acc / np.sum(exp_acc)
    
    return weights
```

---

## 💡 算法优化建议

### 1. 数据层面优化
- **数据增强**: 旋转、翻转、噪声添加
- **样本平衡**: SMOTE、ADASYN等过采样技术
- **特征选择**: 相关性分析、递归特征消除
- **降维技术**: PCA、MNF、ICA

### 2. 模型层面优化
- **正则化**: L1/L2正则化、Dropout
- **批归一化**: 加速训练收敛
- **学习率调度**: 余弦退火、步长衰减
- **早停策略**: 防止过拟合

### 3. 计算层面优化
- **批处理**: 提高GPU利用率
- **混合精度**: 减少内存使用
- **模型量化**: 压缩模型大小
- **知识蒸馏**: 小模型学习大模型

### 4. 工程层面优化
- **模型缓存**: 避免重复计算
- **流水线并行**: 提高处理效率
- **分布式训练**: 大规模数据处理
- **模型部署**: ONNX、TensorRT优化

---

## 📞 技术支持

### 算法相关问题
- 📧 **算法咨询**: algorithm-support@example.com
- 📚 **技术文档**: [算法详细文档](https://algorithm-docs.example.com)
- 💻 **代码示例**: [GitHub算法示例](https://github.com/yourusername/algorithm-examples)
- 🔬 **研究论文**: [相关论文列表](https://papers.example.com)

### 参考文献

1. Melgani, F., & Bruzzone, L. (2004). Classification of hyperspectral remote sensing images with support vector machines. IEEE Transactions on Geoscience and Remote Sensing, 42(8), 1778-1790.

2. Belgiu, M., & Drăguţ, L. (2016). Random forest in remote sensing: A review of applications and future directions. ISPRS Journal of Photogrammetry and Remote Sensing, 114, 24-31.

3. Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., & Benediktsson, J. A. (2019). Deep learning for hyperspectral image classification: An overview. IEEE Transactions on Geoscience and Remote Sensing, 57(9), 6690-6709.

4. Roy, S. K., Krishna, G., Dubey, S. R., & Chaudhuri, B. B. (2020). HybridSN: Exploring 3-D–2-D CNN feature hierarchy for hyperspectral image classification. IEEE Geoscience and Remote Sensing Letters, 17(2), 277-281.

---

*本算法文档持续更新中，最后更新时间: 2024年6月30日*