# æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ç³»ç»Ÿç®—æ³•è¯´æ˜
## Wetland Hyperspectral Classification System - Algorithm Documentation

### ğŸ“‹ ç›®å½•

1. [ç®—æ³•æ¦‚è¿°](#ç®—æ³•æ¦‚è¿°)
2. [æ•°æ®é¢„å¤„ç†ç®—æ³•](#æ•°æ®é¢„å¤„ç†ç®—æ³•)
3. [ç‰¹å¾æå–ç®—æ³•](#ç‰¹å¾æå–ç®—æ³•)
4. [ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•](#ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•)
5. [æ·±åº¦å­¦ä¹ ç®—æ³•](#æ·±åº¦å­¦ä¹ ç®—æ³•)
6. [é›†æˆå­¦ä¹ ç®—æ³•](#é›†æˆå­¦ä¹ ç®—æ³•)
7. [åå¤„ç†ç®—æ³•](#åå¤„ç†ç®—æ³•)
8. [æ™¯è§‚åˆ†æç®—æ³•](#æ™¯è§‚åˆ†æç®—æ³•)
9. [è¯„ä¼°ç®—æ³•](#è¯„ä¼°ç®—æ³•)
10. [ç®—æ³•æ¯”è¾ƒä¸é€‰æ‹©](#ç®—æ³•æ¯”è¾ƒä¸é€‰æ‹©)

---

## ğŸ¯ ç®—æ³•æ¦‚è¿°

### ç³»ç»Ÿç®—æ³•æ¶æ„

æ¹¿åœ°é«˜å…‰è°±åˆ†ç±»ç³»ç»Ÿé‡‡ç”¨å¤šå±‚æ¬¡ã€æ¨¡å—åŒ–çš„ç®—æ³•æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š

```
è¾“å…¥ï¼šé«˜å…‰è°±æ•°æ® (HÃ—WÃ—B)
         â†“
    æ•°æ®é¢„å¤„ç†å±‚
    â”œâ”€â”€ è¾å°„å®šæ ‡
    â”œâ”€â”€ å¤§æ°”æ ¡æ­£  
    â”œâ”€â”€ å‡ ä½•æ ¡æ­£
    â””â”€â”€ å™ªå£°å»é™¤
         â†“
    ç‰¹å¾æå–å±‚
    â”œâ”€â”€ å…‰è°±ç‰¹å¾
    â”œâ”€â”€ æ¤è¢«æŒ‡æ•°
    â”œâ”€â”€ çº¹ç†ç‰¹å¾
    â””â”€â”€ ç©ºé—´ç‰¹å¾
         â†“
    åˆ†ç±»ç®—æ³•å±‚
    â”œâ”€â”€ ä¼ ç»Ÿæœºå™¨å­¦ä¹  (SVM, RF, XGBoost)
    â”œâ”€â”€ æ·±åº¦å­¦ä¹  (3D-CNN, HybridSN, ViT)
    â””â”€â”€ é›†æˆå­¦ä¹  (Voting, Stacking)
         â†“
    åå¤„ç†å±‚
    â”œâ”€â”€ ç©ºé—´æ»¤æ³¢
    â”œâ”€â”€ å½¢æ€å­¦æ“ä½œ
    â””â”€â”€ ä¸€è‡´æ€§æ£€æŸ¥
         â†“
è¾“å‡ºï¼šåˆ†ç±»ç»“æœå›¾ (HÃ—W)
```

### ç®—æ³•è®¾è®¡åŸåˆ™

1. **ç²¾åº¦ä¼˜å…ˆ**: ç¡®ä¿åˆ†ç±»ç²¾åº¦æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚
2. **æ•ˆç‡å¹³è¡¡**: åœ¨ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡
3. **é²æ£’æ€§å¼º**: å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼å…·æœ‰è‰¯å¥½çš„æŠ—å¹²æ‰°èƒ½åŠ›
4. **å¯æ‰©å±•æ€§**: æ”¯æŒæ–°ç®—æ³•çš„å¿«é€Ÿé›†æˆå’Œéƒ¨ç½²
5. **è‡ªé€‚åº”æ€§**: èƒ½å¤Ÿæ ¹æ®æ•°æ®ç‰¹ç‚¹è‡ªåŠ¨è°ƒæ•´å‚æ•°

---

## ğŸ”§ æ•°æ®é¢„å¤„ç†ç®—æ³•

### 1. è¾å°„å®šæ ‡ç®—æ³•

#### ç®—æ³•åŸç†
å°†åŸå§‹DNå€¼è½¬æ¢ä¸ºç‰©ç†æ„ä¹‰çš„è¾å°„äº®åº¦å€¼ã€‚

#### æ•°å­¦æ¨¡å‹
```
L = (DN - DN_dark) Ã— Gain + Offset
```

å…¶ä¸­ï¼š
- L: è¾å°„äº®åº¦ (W/mÂ²/sr/Î¼m)
- DN: æ•°å­—é‡åŒ–å€¼
- DN_dark: æš—ç”µæµå€¼
- Gain: å¢ç›Šç³»æ•°
- Offset: åç§»é‡

#### å®ç°ç®—æ³•
```python
def radiometric_calibration(dn_values, gain, offset, dark_current=0):
    """
    è¾å°„å®šæ ‡ç®—æ³•å®ç°
    
    Args:
        dn_values: åŸå§‹DNå€¼ (H, W, B)
        gain: å¢ç›Šç³»æ•° (B,)
        offset: åç§»é‡ (B,)
        dark_current: æš—ç”µæµå€¼ (B,)
    
    Returns:
        radiance: è¾å°„äº®åº¦å€¼ (H, W, B)
    """
    # å»é™¤æš—ç”µæµ
    corrected_dn = dn_values - dark_current
    
    # åº”ç”¨å¢ç›Šå’Œåç§»
    radiance = corrected_dn * gain + offset
    
    # ç¡®ä¿æ•°å€¼ä¸ºæ­£
    radiance = np.maximum(radiance, 0)
    
    return radiance
```

### 2. å¤§æ°”æ ¡æ­£ç®—æ³•

#### FLAASHç®—æ³•
åŸºäºMODTRANè¾å°„ä¼ è¾“æ¨¡å‹çš„å¤§æ°”æ ¡æ­£ç®—æ³•ã€‚

#### ç®—æ³•åŸç†
å¤§æ°”æ ¡æ­£çš„æ ¸å¿ƒæ–¹ç¨‹ï¼š
```
Ï = Ï€ Ã— (L - L_p) / (Ï„ Ã— E_sun Ã— cos(Î¸) / Ï€ + E_down)
```

å…¶ä¸­ï¼š
- Ï: åœ°è¡¨åå°„ç‡
- L: ä¼ æ„Ÿå™¨æ¥æ”¶çš„è¾å°„äº®åº¦
- L_p: å¤§æ°”ç¨‹è¾å°„
- Ï„: å¤§æ°”é€è¿‡ç‡
- E_sun: å¤ªé˜³è¾ç…§åº¦
- E_down: ä¸‹è¡Œæ¼«å°„è¾ç…§åº¦
- Î¸: å¤ªé˜³å¤©é¡¶è§’

#### å®ç°ç®—æ³•
```python
def atmospheric_correction_flaash(radiance, solar_irradiance, 
                                 solar_zenith, atmospheric_params):
    """
    FLAASHå¤§æ°”æ ¡æ­£ç®—æ³•
    
    Args:
        radiance: è¾å°„äº®åº¦ (H, W, B)
        solar_irradiance: å¤ªé˜³è¾ç…§åº¦ (B,)
        solar_zenith: å¤ªé˜³å¤©é¡¶è§’
        atmospheric_params: å¤§æ°”å‚æ•°å­—å…¸
    
    Returns:
        reflectance: åœ°è¡¨åå°„ç‡ (H, W, B)
    """
    # è®¡ç®—å¤§æ°”å‚æ•°
    path_radiance = atmospheric_params['path_radiance']
    transmittance = atmospheric_params['transmittance']
    downwelling = atmospheric_params['downwelling']
    
    # å¤§æ°”æ ¡æ­£è®¡ç®—
    cos_solar_zenith = np.cos(np.radians(solar_zenith))
    solar_term = solar_irradiance * cos_solar_zenith / np.pi
    
    numerator = np.pi * (radiance - path_radiance)
    denominator = transmittance * solar_term + downwelling
    
    reflectance = numerator / (denominator + 1e-8)
    
    # é™åˆ¶åå°„ç‡èŒƒå›´
    reflectance = np.clip(reflectance, 0, 1)
    
    return reflectance
```

### 3. å™ªå£°å»é™¤ç®—æ³•

#### æœ€å°å™ªå£°åˆ†ç¦»å˜æ¢ (MNF)
åŸºäºä¿¡å™ªæ¯”åˆ†ç¦»çš„é™ç»´ç®—æ³•ã€‚

#### ç®—æ³•åŸç†
1. ä¼°è®¡å™ªå£°åæ–¹å·®çŸ©é˜µ
2. è®¡ç®—ä¿¡å·åæ–¹å·®çŸ©é˜µ
3. æ±‚è§£å¹¿ä¹‰ç‰¹å¾å€¼é—®é¢˜
4. é€‰æ‹©é«˜ä¿¡å™ªæ¯”æˆåˆ†

#### å®ç°ç®—æ³•
```python
def minimum_noise_fraction(data, n_components=50):
    """
    æœ€å°å™ªå£°åˆ†ç¦»å˜æ¢
    
    Args:
        data: é«˜å…‰è°±æ•°æ® (N_pixels, N_bands)
        n_components: ä¿ç•™çš„æˆåˆ†æ•°
    
    Returns:
        transformed_data: å˜æ¢åçš„æ•°æ®
        eigenvalues: ç‰¹å¾å€¼
        eigenvectors: ç‰¹å¾å‘é‡
    """
    # ä¼°è®¡å™ªå£°åæ–¹å·®çŸ©é˜µ
    noise_cov = estimate_noise_covariance(data)
    
    # è®¡ç®—æ•°æ®åæ–¹å·®çŸ©é˜µ
    data_cov = np.cov(data.T)
    
    # æ±‚è§£å¹¿ä¹‰ç‰¹å¾å€¼é—®é¢˜
    eigenvalues, eigenvectors = scipy.linalg.eigh(data_cov, noise_cov)
    
    # æŒ‰ç‰¹å¾å€¼é™åºæ’åˆ—
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # é€‰æ‹©å‰n_componentsä¸ªæˆåˆ†
    transform_matrix = eigenvectors[:, :n_components]
    transformed_data = data @ transform_matrix
    
    return transformed_data, eigenvalues, eigenvectors
```

---

## ğŸ¨ ç‰¹å¾æå–ç®—æ³•

### 1. å…‰è°±ç‰¹å¾æå–

#### å…‰è°±å¯¼æ•°
ç”¨äºçªå‡ºå…‰è°±ç‰¹å¾å’Œå‡å°‘ç¯å¢ƒå½±å“ã€‚

#### ä¸€é˜¶å¯¼æ•°
```python
def first_derivative(spectrum):
    """è®¡ç®—ä¸€é˜¶å…‰è°±å¯¼æ•°"""
    return np.gradient(spectrum)

def second_derivative(spectrum):
    """è®¡ç®—äºŒé˜¶å…‰è°±å¯¼æ•°"""
    first_deriv = np.gradient(spectrum)
    return np.gradient(first_deriv)
```

#### è¿ç»­ç»Ÿå»é™¤ (Continuum Removal)
```python
def continuum_removal(spectrum):
    """
    è¿ç»­ç»Ÿå»é™¤ç®—æ³•
    
    Args:
        spectrum: å…‰è°±æ›²çº¿ (N_bands,)
    
    Returns:
        cr_spectrum: è¿ç»­ç»Ÿå»é™¤åçš„å…‰è°±
    """
    # è®¡ç®—å‡¸åŒ…
    hull = ConvexHull(np.column_stack([range(len(spectrum)), spectrum]))
    hull_points = hull.vertices
    
    # æ„å»ºè¿ç»­ç»Ÿçº¿
    continuum = np.interp(range(len(spectrum)), 
                         hull_points, spectrum[hull_points])
    
    # è¿ç»­ç»Ÿå»é™¤
    cr_spectrum = spectrum / (continuum + 1e-8)
    
    return cr_spectrum
```

### 2. æ¤è¢«æŒ‡æ•°ç®—æ³•

#### å½’ä¸€åŒ–å·®å¼‚æ¤è¢«æŒ‡æ•° (NDVI)
```python
def calculate_ndvi(nir, red):
    """
    NDVI = (NIR - Red) / (NIR + Red)
    
    Args:
        nir: è¿‘çº¢å¤–æ³¢æ®µ
        red: çº¢å…‰æ³¢æ®µ
    
    Returns:
        ndvi: NDVIæŒ‡æ•°
    """
    return (nir - red) / (nir + red + 1e-8)
```

#### å¢å¼ºæ¤è¢«æŒ‡æ•° (EVI)
```python
def calculate_evi(nir, red, blue, L=1, C1=6, C2=7.5, G=2.5):
    """
    EVI = G Ã— (NIR - Red) / (NIR + C1Ã—Red - C2Ã—Blue + L)
    
    Args:
        nir: è¿‘çº¢å¤–æ³¢æ®µ
        red: çº¢å…‰æ³¢æ®µ  
        blue: è“å…‰æ³¢æ®µ
        L, C1, C2, G: EVIå‚æ•°
    
    Returns:
        evi: EVIæŒ‡æ•°
    """
    numerator = G * (nir - red)
    denominator = nir + C1 * red - C2 * blue + L
    return numerator / (denominator + 1e-8)
```

#### çº¢è¾¹ä½ç½® (Red Edge Position)
```python
def red_edge_position(spectrum, wavelengths):
    """
    è®¡ç®—çº¢è¾¹ä½ç½®
    
    Args:
        spectrum: å…‰è°±æ•°æ®
        wavelengths: æ³¢é•¿æ•°ç»„
    
    Returns:
        rep: çº¢è¾¹ä½ç½® (nm)
    """
    # çº¢è¾¹èŒƒå›´ (690-740 nm)
    red_edge_mask = (wavelengths >= 690) & (wavelengths <= 740)
    red_edge_spectrum = spectrum[red_edge_mask]
    red_edge_wavelengths = wavelengths[red_edge_mask]
    
    # è®¡ç®—ä¸€é˜¶å¯¼æ•°
    derivative = np.gradient(red_edge_spectrum)
    
    # æ‰¾åˆ°æœ€å¤§å¯¼æ•°ä½ç½®
    max_deriv_idx = np.argmax(derivative)
    rep = red_edge_wavelengths[max_deriv_idx]
    
    return rep
```

### 3. çº¹ç†ç‰¹å¾ç®—æ³•

#### ç°åº¦å…±ç”ŸçŸ©é˜µ (GLCM)
```python
def calculate_glcm_features(image, distances=[1], angles=[0, 45, 90, 135]):
    """
    è®¡ç®—GLCMçº¹ç†ç‰¹å¾
    
    Args:
        image: è¾“å…¥å›¾åƒ
        distances: åƒç´ è·ç¦»åˆ—è¡¨
        angles: æ–¹å‘è§’åº¦åˆ—è¡¨
    
    Returns:
        features: çº¹ç†ç‰¹å¾å­—å…¸
    """
    from skimage.feature import graycomatrix, graycoprops
    
    # è®¡ç®—GLCMçŸ©é˜µ
    glcm = graycomatrix(image, distances, angles, 
                       levels=256, symmetric=True, normed=True)
    
    # è®¡ç®—çº¹ç†ç‰¹å¾
    features = {}
    features['contrast'] = graycoprops(glcm, 'contrast').mean()
    features['dissimilarity'] = graycoprops(glcm, 'dissimilarity').mean()
    features['homogeneity'] = graycoprops(glcm, 'homogeneity').mean()
    features['energy'] = graycoprops(glcm, 'energy').mean()
    features['correlation'] = graycoprops(glcm, 'correlation').mean()
    features['ASM'] = graycoprops(glcm, 'ASM').mean()
    
    return features
```

#### å±€éƒ¨äºŒå€¼æ¨¡å¼ (LBP)
```python
def local_binary_pattern(image, radius=3, n_points=24):
    """
    è®¡ç®—å±€éƒ¨äºŒå€¼æ¨¡å¼
    
    Args:
        image: è¾“å…¥å›¾åƒ
        radius: åœ†å½¢é‚»åŸŸåŠå¾„
        n_points: é‡‡æ ·ç‚¹æ•°
    
    Returns:
        lbp: LBPç‰¹å¾å›¾
    """
    from skimage.feature import local_binary_pattern as skimage_lbp
    
    lbp = skimage_lbp(image, n_points, radius, method='uniform')
    
    # è®¡ç®—LBPç›´æ–¹å›¾
    hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, 
                          range=(0, n_points + 2))
    
    # å½’ä¸€åŒ–
    hist = hist.astype(float) / (hist.sum() + 1e-8)
    
    return lbp, hist
```

---

## ğŸ¤– ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•

### 1. æ”¯æŒå‘é‡æœº (SVM)

#### ç®—æ³•åŸç†
SVMé€šè¿‡å¯»æ‰¾æœ€ä¼˜è¶…å¹³é¢æ¥åˆ†ç¦»ä¸åŒç±»åˆ«ã€‚å¯¹äºéçº¿æ€§é—®é¢˜ï¼Œä½¿ç”¨æ ¸å‡½æ•°æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ã€‚

#### æ ¸å‡½æ•°
1. **çº¿æ€§æ ¸**: K(x, y) = x^T y
2. **å¤šé¡¹å¼æ ¸**: K(x, y) = (Î³x^T y + r)^d
3. **RBFæ ¸**: K(x, y) = exp(-Î³||x - y||Â²)
4. **Sigmoidæ ¸**: K(x, y) = tanh(Î³x^T y + r)

#### ä¼˜åŒ–ç›®æ ‡
```
min (1/2)||w||Â² + Câˆ‘Î¾áµ¢
çº¦æŸæ¡ä»¶: yáµ¢(w^T Ï†(xáµ¢) + b) â‰¥ 1 - Î¾áµ¢, Î¾áµ¢ â‰¥ 0
```

#### å®ç°ä»£ç 
```python
class HyperspectralSVM:
    def __init__(self, kernel='rbf', C=1.0, gamma='scale'):
        """
        é«˜å…‰è°±SVMåˆ†ç±»å™¨
        
        Args:
            kernel: æ ¸å‡½æ•°ç±»å‹
            C: æƒ©ç½šå‚æ•°
            gamma: æ ¸å‡½æ•°å‚æ•°
        """
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.model = SVC(kernel=kernel, C=C, gamma=gamma, 
                        probability=True, random_state=42)
    
    def fit(self, X, y):
        """è®­ç»ƒSVMæ¨¡å‹"""
        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X_scaled, y)
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
```

### 2. éšæœºæ£®æ— (Random Forest)

#### ç®—æ³•åŸç†
éšæœºæ£®æ—æ˜¯åŸºäºå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡Bootstrapé‡‡æ ·å’Œéšæœºç‰¹å¾é€‰æ‹©æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚

#### Baggingç­–ç•¥
1. ä»è®­ç»ƒé›†ä¸­æœ‰æ”¾å›æŠ½æ ·ç”Ÿæˆæ–°çš„è®­ç»ƒé›†
2. åœ¨æ¯ä¸ªèŠ‚ç‚¹éšæœºé€‰æ‹©éƒ¨åˆ†ç‰¹å¾è¿›è¡Œåˆ†è£‚
3. æ„å»ºå¤šä¸ªå†³ç­–æ ‘
4. é€šè¿‡æŠ•ç¥¨æˆ–å¹³å‡è¿›è¡Œé¢„æµ‹

#### å®ç°ä»£ç 
```python
class HyperspectralRandomForest:
    def __init__(self, n_estimators=100, max_depth=None, 
                 max_features='sqrt', random_state=42):
        """
        é«˜å…‰è°±éšæœºæ£®æ—åˆ†ç±»å™¨
        
        Args:
            n_estimators: å†³ç­–æ ‘æ•°é‡
            max_depth: æœ€å¤§æ·±åº¦
            max_features: æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ•°
        """
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            max_features=max_features,
            random_state=random_state,
            n_jobs=-1
        )
    
    def fit(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        self.model.fit(X, y)
        return self
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        return self.model.predict(X)
    
    def feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        return self.model.feature_importances_
```

### 3. æç«¯æ¢¯åº¦æå‡ (XGBoost)

#### ç®—æ³•åŸç†
XGBoostæ˜¯åŸºäºæ¢¯åº¦æå‡çš„é›†æˆå­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡é€æ­¥æ·»åŠ å¼±å­¦ä¹ å™¨æ¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ã€‚

#### ç›®æ ‡å‡½æ•°
```
Obj = âˆ‘L(yáµ¢, Å·áµ¢) + âˆ‘Î©(fâ‚–)
```

å…¶ä¸­ï¼š
- L: æŸå¤±å‡½æ•°
- Î©: æ­£åˆ™åŒ–é¡¹
- fâ‚–: ç¬¬kä¸ªå¼±å­¦ä¹ å™¨

#### å®ç°ä»£ç 
```python
class HyperspectralXGBoost:
    def __init__(self, n_estimators=100, learning_rate=0.1, 
                 max_depth=6, subsample=0.8):
        """
        é«˜å…‰è°±XGBooståˆ†ç±»å™¨
        
        Args:
            n_estimators: å¼±å­¦ä¹ å™¨æ•°é‡
            learning_rate: å­¦ä¹ ç‡
            max_depth: æœ€å¤§æ·±åº¦
            subsample: å­é‡‡æ ·æ¯”ä¾‹
        """
        self.model = XGBClassifier(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            subsample=subsample,
            random_state=42,
            n_jobs=-1
        )
    
    def fit(self, X, y, eval_set=None, early_stopping_rounds=None):
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        self.model.fit(X, y, eval_set=eval_set, 
                      early_stopping_rounds=early_stopping_rounds,
                      verbose=False)
        return self
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        return self.model.predict(X)
```

---

## ğŸ§  æ·±åº¦å­¦ä¹ ç®—æ³•

### 1. 3Då·ç§¯ç¥ç»ç½‘ç»œ (3D-CNN)

#### ç®—æ³•åŸç†
3D-CNNèƒ½å¤ŸåŒæ—¶æå–é«˜å…‰è°±æ•°æ®çš„ç©ºé—´å’Œå…‰è°±ç‰¹å¾ï¼Œé€šè¿‡3Då·ç§¯æ ¸åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œç‰¹å¾æå–ã€‚

#### ç½‘ç»œæ¶æ„
```python
class HyperspectralCNN3D(nn.Module):
    def __init__(self, input_channels, num_classes, patch_size=9):
        """
        3D-CNNç½‘ç»œæ¶æ„
        
        Args:
            input_channels: è¾“å…¥å…‰è°±æ³¢æ®µæ•°
            num_classes: ç±»åˆ«æ•°
            patch_size: å›¾åƒå—å¤§å°
        """
        super(HyperspectralCNN3D, self).__init__()
        
        # 3Då·ç§¯å±‚
        self.conv3d1 = nn.Conv3d(1, 8, kernel_size=(7, 3, 3), padding=(0, 1, 1))
        self.conv3d2 = nn.Conv3d(8, 16, kernel_size=(5, 3, 3), padding=(0, 1, 1))
        self.conv3d3 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=(0, 1, 1))
        
        # æ‰¹å½’ä¸€åŒ–
        self.bn1 = nn.BatchNorm3d(8)
        self.bn2 = nn.BatchNorm3d(16)
        self.bn3 = nn.BatchNorm3d(32)
        
        # Dropout
        self.dropout = nn.Dropout3d(0.4)
        
        # å…¨è¿æ¥å±‚
        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(32, num_classes)
        
    def forward(self, x):
        # x shape: (batch_size, 1, bands, height, width)
        x = F.relu(self.bn1(self.conv3d1(x)))
        x = F.relu(self.bn2(self.conv3d2(x)))
        x = F.relu(self.bn3(self.conv3d3(x)))
        
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x
```

### 2. æ··åˆå·ç§¯ç½‘ç»œ (HybridSN)

#### ç®—æ³•åŸç†
HybridSNç»“åˆ3Då’Œ2Då·ç§¯ï¼Œå…ˆç”¨3Då·ç§¯æå–å…‰è°±-ç©ºé—´ç‰¹å¾ï¼Œå†ç”¨2Då·ç§¯è¿›ä¸€æ­¥æå–ç©ºé—´ç‰¹å¾ã€‚

#### ç½‘ç»œæ¶æ„
```python
class HybridSN(nn.Module):
    def __init__(self, input_channels, num_classes, patch_size=25):
        super(HybridSN, self).__init__()
        
        # 3Då·ç§¯åˆ†æ”¯
        self.conv3d1 = nn.Conv3d(1, 8, kernel_size=(7, 3, 3))
        self.conv3d2 = nn.Conv3d(8, 16, kernel_size=(5, 3, 3))
        self.conv3d3 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3))
        
        # 2Då·ç§¯åˆ†æ”¯
        self.conv2d1 = nn.Conv2d(32 * (input_channels - 12), 64, kernel_size=3)
        self.conv2d2 = nn.Conv2d(64, 128, kernel_size=3)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * (patch_size - 8) * (patch_size - 8), 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)
        
        self.dropout = nn.Dropout(0.4)
        
    def forward(self, x):
        # 3Då·ç§¯éƒ¨åˆ†
        x = F.relu(self.conv3d1(x))
        x = F.relu(self.conv3d2(x))
        x = F.relu(self.conv3d3(x))
        
        # é‡å¡‘ä¸º2D
        x = x.view(x.size(0), -1, x.size(3), x.size(4))
        
        # 2Då·ç§¯éƒ¨åˆ†
        x = F.relu(self.conv2d1(x))
        x = F.relu(self.conv2d2(x))
        
        # å…¨è¿æ¥éƒ¨åˆ†
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x
```

### 3. Vision Transformer (ViT)

#### ç®—æ³•åŸç†
ViTå°†å›¾åƒåˆ†å‰²ä¸ºpatchesï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ å…¨å±€ç‰¹å¾å…³ç³»ã€‚

#### æ ¸å¿ƒç»„ä»¶
1. **Patch Embedding**: å°†å›¾åƒå—è½¬æ¢ä¸ºè¯å‘é‡
2. **Position Encoding**: æ·»åŠ ä½ç½®ä¿¡æ¯
3. **Multi-Head Attention**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
4. **Feed Forward Network**: å‰é¦ˆç¥ç»ç½‘ç»œ

#### å®ç°ä»£ç 
```python
class HyperspectralViT(nn.Module):
    def __init__(self, input_channels, num_classes, patch_size=8, 
                 dim=512, depth=6, heads=8, mlp_dim=1024):
        super(HyperspectralViT, self).__init__()
        
        self.patch_size = patch_size
        self.dim = dim
        
        # Patch Embedding
        self.patch_embed = nn.Conv2d(input_channels, dim, 
                                   kernel_size=patch_size, stride=patch_size)
        
        # Position Embedding
        self.pos_embed = nn.Parameter(torch.randn(1, 1000, dim))
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        
        # Classification Head
        self.norm = nn.LayerNorm(dim)
        self.fc = nn.Linear(dim, num_classes)
        
    def forward(self, x):
        # Patch Embedding
        x = self.patch_embed(x)  # (B, dim, H/p, W/p)
        x = x.flatten(2).transpose(1, 2)  # (B, N, dim)
        
        # Add Position Embedding
        x = x + self.pos_embed[:, :x.size(1)]
        
        # Transformer Encoding
        x = self.transformer(x)
        
        # Global Average Pooling
        x = x.mean(dim=1)
        
        # Classification
        x = self.norm(x)
        x = self.fc(x)
        
        return x
```

---

## ğŸ”— é›†æˆå­¦ä¹ ç®—æ³•

### 1. æŠ•ç¥¨é›†æˆ (Voting Ensemble)

#### ç®—æ³•åŸç†
é€šè¿‡å¤šä¸ªåŸºåˆ†ç±»å™¨çš„æŠ•ç¥¨æ¥åšæœ€ç»ˆå†³ç­–ã€‚

#### ç¡¬æŠ•ç¥¨
```python
def hard_voting_predict(predictions_list):
    """
    ç¡¬æŠ•ç¥¨é¢„æµ‹
    
    Args:
        predictions_list: åŸºåˆ†ç±»å™¨é¢„æµ‹ç»“æœåˆ—è¡¨
    
    Returns:
        final_predictions: æŠ•ç¥¨ç»“æœ
    """
    predictions_array = np.array(predictions_list)
    final_predictions = []
    
    for i in range(predictions_array.shape[1]):
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„ç¥¨æ•°
        votes = predictions_array[:, i]
        final_pred = np.bincount(votes).argmax()
        final_predictions.append(final_pred)
    
    return np.array(final_predictions)
```

#### è½¯æŠ•ç¥¨
```python
def soft_voting_predict(probabilities_list, weights=None):
    """
    è½¯æŠ•ç¥¨é¢„æµ‹
    
    Args:
        probabilities_list: åŸºåˆ†ç±»å™¨æ¦‚ç‡é¢„æµ‹åˆ—è¡¨
        weights: æƒé‡
    
    Returns:
        final_predictions: åŠ æƒæŠ•ç¥¨ç»“æœ
    """
    if weights is None:
        weights = np.ones(len(probabilities_list)) / len(probabilities_list)
    
    # åŠ æƒå¹³å‡æ¦‚ç‡
    weighted_probs = np.zeros_like(probabilities_list[0])
    for i, probs in enumerate(probabilities_list):
        weighted_probs += weights[i] * probs
    
    # é€‰æ‹©æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
    final_predictions = np.argmax(weighted_probs, axis=1)
    
    return final_predictions, weighted_probs
```

### 2. å †å é›†æˆ (Stacking)

#### ç®—æ³•åŸç†
ä½¿ç”¨å…ƒå­¦ä¹ å™¨å­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°ç»„åˆåŸºåˆ†ç±»å™¨çš„è¾“å‡ºã€‚

#### å®ç°ä»£ç 
```python
class StackingEnsemble:
    def __init__(self, base_classifiers, meta_classifier):
        """
        å †å é›†æˆåˆ†ç±»å™¨
        
        Args:
            base_classifiers: åŸºåˆ†ç±»å™¨åˆ—è¡¨
            meta_classifier: å…ƒåˆ†ç±»å™¨
        """
        self.base_classifiers = base_classifiers
        self.meta_classifier = meta_classifier
        
    def fit(self, X, y, cv=5):
        """è®­ç»ƒå †å é›†æˆæ¨¡å‹"""
        from sklearn.model_selection import cross_val_predict
        
        # è®­ç»ƒåŸºåˆ†ç±»å™¨å¹¶ç”Ÿæˆå…ƒç‰¹å¾
        meta_features = []
        
        for clf in self.base_classifiers:
            # äº¤å‰éªŒè¯é¢„æµ‹
            cv_pred = cross_val_predict(clf, X, y, cv=cv, method='predict_proba')
            meta_features.append(cv_pred)
            
            # åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒ
            clf.fit(X, y)
        
        # ç»„åˆå…ƒç‰¹å¾
        meta_X = np.column_stack(meta_features)
        
        # è®­ç»ƒå…ƒåˆ†ç±»å™¨
        self.meta_classifier.fit(meta_X, y)
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        # åŸºåˆ†ç±»å™¨é¢„æµ‹
        base_predictions = []
        for clf in self.base_classifiers:
            pred = clf.predict_proba(X)
            base_predictions.append(pred)
        
        # ç»„åˆé¢„æµ‹ç»“æœ
        meta_X = np.column_stack(base_predictions)
        
        # å…ƒåˆ†ç±»å™¨é¢„æµ‹
        final_pred = self.meta_classifier.predict(meta_X)
        
        return final_pred
```

---

## ğŸ”„ åå¤„ç†ç®—æ³•

### 1. ç©ºé—´æ»¤æ³¢ç®—æ³•

#### å¤šæ•°æ»¤æ³¢ (Majority Filter)
```python
def majority_filter(classification_map, window_size=3):
    """
    å¤šæ•°æ»¤æ³¢ç®—æ³•
    
    Args:
        classification_map: åˆ†ç±»ç»“æœå›¾
        window_size: æ»¤æ³¢çª—å£å¤§å°
    
    Returns:
        filtered_map: æ»¤æ³¢åçš„åˆ†ç±»å›¾
    """
    from scipy import ndimage
    
    filtered_map = np.copy(classification_map)
    height, width = classification_map.shape
    
    pad_size = window_size // 2
    padded_map = np.pad(classification_map, pad_size, mode='reflect')
    
    for i in range(height):
        for j in range(width):
            # æå–çª—å£
            window = padded_map[i:i+window_size, j:j+window_size]
            
            # è®¡ç®—å¤šæ•°ç±»åˆ«
            unique, counts = np.unique(window, return_counts=True)
            majority_class = unique[np.argmax(counts)]
            
            filtered_map[i, j] = majority_class
    
    return filtered_map
```

### 2. å½¢æ€å­¦æ“ä½œ

#### å¼€è¿ç®—å’Œé—­è¿ç®—
```python
def morphological_operations(classification_map, operation='opening', 
                           kernel_size=3, iterations=1):
    """
    å½¢æ€å­¦æ“ä½œ
    
    Args:
        classification_map: åˆ†ç±»ç»“æœå›¾
        operation: æ“ä½œç±»å‹ ('opening', 'closing', 'erosion', 'dilation')
        kernel_size: ç»“æ„å…ƒç´ å¤§å°
        iterations: è¿­ä»£æ¬¡æ•°
    
    Returns:
        processed_map: å¤„ç†åçš„åˆ†ç±»å›¾
    """
    from scipy import ndimage
    
    # åˆ›å»ºç»“æ„å…ƒç´ 
    kernel = np.ones((kernel_size, kernel_size))
    
    processed_map = np.copy(classification_map)
    
    if operation == 'erosion':
        processed_map = ndimage.binary_erosion(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    elif operation == 'dilation':
        processed_map = ndimage.binary_dilation(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    elif operation == 'opening':
        processed_map = ndimage.binary_opening(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    elif operation == 'closing':
        processed_map = ndimage.binary_closing(
            classification_map, kernel, iterations=iterations
        ).astype(classification_map.dtype)
    
    return processed_map
```

---

## ğŸŒ¿ æ™¯è§‚åˆ†æç®—æ³•

### 1. æ™¯è§‚æŒ‡æ•°è®¡ç®—

#### é¦™å†œå¤šæ ·æ€§æŒ‡æ•°
```python
def shannon_diversity_index(classification_map):
    """
    è®¡ç®—é¦™å†œå¤šæ ·æ€§æŒ‡æ•°
    
    H = -âˆ‘(p_i Ã— ln(p_i))
    
    Args:
        classification_map: åˆ†ç±»ç»“æœå›¾
    
    Returns:
        shannon_index: é¦™å†œå¤šæ ·æ€§æŒ‡æ•°
    """
    # è®¡ç®—å„ç±»åˆ«æ¯”ä¾‹
    unique_classes, counts = np.unique(classification_map, return_counts=True)
    proportions = counts / counts.sum()
    
    # è®¡ç®—é¦™å†œæŒ‡æ•°
    shannon_index = -np.sum(proportions * np.log(proportions + 1e-8))
    
    return shannon_index
```

#### èšé›†æŒ‡æ•°
```python
def aggregation_index(classification_map):
    """
    è®¡ç®—èšé›†æŒ‡æ•°
    
    AI = 100 Ã— (g_ii / max_g_ii)
    
    Args:
        classification_map: åˆ†ç±»ç»“æœå›¾
    
    Returns:
        ai: èšé›†æŒ‡æ•°
    """
    height, width = classification_map.shape
    total_adjacencies = 0
    same_class_adjacencies = 0
    
    # è®¡ç®—ç›¸é‚»åƒç´ å¯¹
    for i in range(height):
        for j in range(width):
            current_class = classification_map[i, j]
            
            # æ£€æŸ¥å³é‚»å±…
            if j < width - 1:
                total_adjacencies += 1
                if classification_map[i, j + 1] == current_class:
                    same_class_adjacencies += 1
            
            # æ£€æŸ¥ä¸‹é‚»å±…  
            if i < height - 1:
                total_adjacencies += 1
                if classification_map[i + 1, j] == current_class:
                    same_class_adjacencies += 1
    
    # è®¡ç®—èšé›†æŒ‡æ•°
    ai = 100 * same_class_adjacencies / total_adjacencies if total_adjacencies > 0 else 0
    
    return ai
```

### 2. è¿é€šæ€§åˆ†æ

#### è¿é€šç»„ä»¶æ ‡è®°
```python
def connectivity_analysis(classification_map, class_id, connectivity=8):
    """
    è¿é€šæ€§åˆ†æ
    
    Args:
        classification_map: åˆ†ç±»ç»“æœå›¾
        class_id: ç›®æ ‡ç±»åˆ«ID
        connectivity: è¿é€šæ€§ (4 æˆ– 8)
    
    Returns:
        results: è¿é€šæ€§åˆ†æç»“æœ
    """
    from scipy import ndimage
    
    # åˆ›å»ºäºŒå€¼å›¾
    binary_map = (classification_map == class_id).astype(int)
    
    # è¿é€šç»„ä»¶æ ‡è®°
    if connectivity == 4:
        structure = ndimage.generate_binary_structure(2, 1)
    else:  # connectivity == 8
        structure = ndimage.generate_binary_structure(2, 2)
    
    labeled_map, num_features = ndimage.label(binary_map, structure=structure)
    
    # è®¡ç®—å„è¿é€šç»„ä»¶çš„å¤§å°
    component_sizes = ndimage.sum(binary_map, labeled_map, 
                                 range(1, num_features + 1))
    
    results = {
        'num_components': num_features,
        'component_sizes': component_sizes,
        'largest_component': np.max(component_sizes) if num_features > 0 else 0,
        'mean_component_size': np.mean(component_sizes) if num_features > 0 else 0,
        'labeled_map': labeled_map
    }
    
    return results
```

---

## ğŸ“Š è¯„ä¼°ç®—æ³•

### 1. ç²¾åº¦è¯„ä¼°

#### æ··æ·†çŸ©é˜µ
```python
def confusion_matrix(y_true, y_pred, num_classes):
    """
    è®¡ç®—æ··æ·†çŸ©é˜µ
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°
    
    Returns:
        cm: æ··æ·†çŸ©é˜µ
    """
    cm = np.zeros((num_classes, num_classes), dtype=int)
    
    for true_label, pred_label in zip(y_true, y_pred):
        cm[true_label, pred_label] += 1
    
    return cm
```

#### Kappaç³»æ•°
```python
def kappa_coefficient(y_true, y_pred):
    """
    è®¡ç®—Kappaç³»æ•°
    
    Îº = (p_o - p_e) / (1 - p_e)
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
    
    Returns:
        kappa: Kappaç³»æ•°
    """
    from sklearn.metrics import confusion_matrix
    
    cm = confusion_matrix(y_true, y_pred)
    n = np.sum(cm)
    
    # è§‚æµ‹ä¸€è‡´æ€§
    p_o = np.trace(cm) / n
    
    # æœŸæœ›ä¸€è‡´æ€§
    row_sums = np.sum(cm, axis=1)
    col_sums = np.sum(cm, axis=0)
    p_e = np.sum(row_sums * col_sums) / (n * n)
    
    # Kappaç³»æ•°
    kappa = (p_o - p_e) / (1 - p_e) if p_e != 1 else 0
    
    return kappa
```

### 2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

#### McNemaræ£€éªŒ
```python
def mcnemar_test(y_true, y_pred1, y_pred2):
    """
    McNemaræ£€éªŒæ¯”è¾ƒä¸¤ä¸ªåˆ†ç±»å™¨
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred1: åˆ†ç±»å™¨1é¢„æµ‹
        y_pred2: åˆ†ç±»å™¨2é¢„æµ‹
    
    Returns:
        statistic: æ£€éªŒç»Ÿè®¡é‡
        p_value: på€¼
    """
    # æ„å»º2x2åˆ—è”è¡¨
    correct1 = (y_pred1 == y_true)
    correct2 = (y_pred2 == y_true)
    
    a = np.sum(correct1 & correct2)      # éƒ½æ­£ç¡®
    b = np.sum(correct1 & ~correct2)     # 1æ­£ç¡®ï¼Œ2é”™è¯¯
    c = np.sum(~correct1 & correct2)     # 1é”™è¯¯ï¼Œ2æ­£ç¡®
    d = np.sum(~correct1 & ~correct2)    # éƒ½é”™è¯¯
    
    # McNemarç»Ÿè®¡é‡
    statistic = (abs(b - c) - 1) ** 2 / (b + c) if (b + c) > 0 else 0
    
    # på€¼ (å¡æ–¹åˆ†å¸ƒ)
    from scipy.stats import chi2
    p_value = 1 - chi2.cdf(statistic, df=1)
    
    return statistic, p_value
```

---

## ğŸ“ˆ ç®—æ³•æ¯”è¾ƒä¸é€‰æ‹©

### 1. ç®—æ³•æ€§èƒ½å¯¹æ¯”

#### è®¡ç®—å¤æ‚åº¦
| ç®—æ³• | è®­ç»ƒå¤æ‚åº¦ | é¢„æµ‹å¤æ‚åº¦ | å†…å­˜å¤æ‚åº¦ |
|------|------------|------------|------------|
| SVM | O(nÂ³) | O(svÃ—d) | O(svÃ—d) |
| Random Forest | O(nÃ—log nÃ—dÃ—t) | O(tÃ—log n) | O(tÃ—nÃ—d) |
| XGBoost | O(nÃ—dÃ—t) | O(tÃ—log n) | O(nÃ—d) |
| 3D-CNN | O(epochsÃ—nÃ—operations) | O(operations) | O(model_size) |
| HybridSN | O(epochsÃ—nÃ—operations) | O(operations) | O(model_size) |

å…¶ä¸­ï¼š
- n: æ ·æœ¬æ•°
- d: ç‰¹å¾ç»´åº¦
- t: æ ‘çš„æ•°é‡
- sv: æ”¯æŒå‘é‡æ•°

#### é€‚ç”¨åœºæ™¯
```python
def algorithm_selection_guide(dataset_size, feature_dim, 
                            computational_budget, accuracy_requirement):
    """
    ç®—æ³•é€‰æ‹©æŒ‡å—
    
    Args:
        dataset_size: æ•°æ®é›†å¤§å°
        feature_dim: ç‰¹å¾ç»´åº¦
        computational_budget: è®¡ç®—é¢„ç®— ('low', 'medium', 'high')
        accuracy_requirement: ç²¾åº¦è¦æ±‚ ('low', 'medium', 'high')
    
    Returns:
        recommended_algorithms: æ¨èç®—æ³•åˆ—è¡¨
    """
    recommendations = []
    
    if computational_budget == 'low':
        if dataset_size < 10000:
            recommendations.extend(['SVM', 'Random Forest'])
        else:
            recommendations.append('Random Forest')
    
    elif computational_budget == 'medium':
        recommendations.extend(['Random Forest', 'XGBoost'])
        if dataset_size > 5000:
            recommendations.append('3D-CNN')
    
    elif computational_budget == 'high':
        if accuracy_requirement == 'high':
            recommendations.extend(['HybridSN', 'Vision Transformer', 'Ensemble'])
        else:
            recommendations.extend(['3D-CNN', 'XGBoost'])
    
    # æ ¹æ®ç‰¹å¾ç»´åº¦è°ƒæ•´
    if feature_dim > 200:
        if 'SVM' in recommendations:
            recommendations.remove('SVM')  # SVMåœ¨é«˜ç»´ç‰¹å¾æ—¶æ€§èƒ½ä¸‹é™
    
    return recommendations
```

### 2. è¶…å‚æ•°ä¼˜åŒ–ç­–ç•¥

#### è´å¶æ–¯ä¼˜åŒ–
```python
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical

def bayesian_optimization(model_type, X_train, y_train, X_val, y_val):
    """
    è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
    
    Args:
        model_type: æ¨¡å‹ç±»å‹
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_val, y_val: éªŒè¯æ•°æ®
    
    Returns:
        best_params: æœ€ä¼˜å‚æ•°
    """
    def objective(params):
        if model_type == 'svm':
            C, gamma = params
            model = SVC(C=C, gamma=gamma)
        elif model_type == 'random_forest':
            n_estimators, max_depth = params
            model = RandomForestClassifier(
                n_estimators=n_estimators, 
                max_depth=max_depth
            )
        
        model.fit(X_train, y_train)
        predictions = model.predict(X_val)
        accuracy = np.mean(predictions == y_val)
        
        return -accuracy  # æœ€å°åŒ–è´Ÿç²¾åº¦
    
    if model_type == 'svm':
        space = [Real(0.1, 100, prior='log-uniform'),    # C
                Real(1e-6, 1e-1, prior='log-uniform')]   # gamma
    elif model_type == 'random_forest':
        space = [Integer(50, 500),     # n_estimators
                Integer(5, 50)]        # max_depth
    
    result = gp_minimize(objective, space, n_calls=50, random_state=42)
    
    return result.x
```

### 3. æ¨¡å‹é›†æˆç­–ç•¥

#### åŠ¨æ€æƒé‡é›†æˆ
```python
def dynamic_ensemble_weights(classifiers, X_val, y_val):
    """
    åŸºäºéªŒè¯é›†æ€§èƒ½åŠ¨æ€è®¡ç®—é›†æˆæƒé‡
    
    Args:
        classifiers: åŸºåˆ†ç±»å™¨åˆ—è¡¨
        X_val, y_val: éªŒè¯æ•°æ®
    
    Returns:
        weights: å½’ä¸€åŒ–æƒé‡
    """
    accuracies = []
    
    for clf in classifiers:
        predictions = clf.predict(X_val)
        accuracy = np.mean(predictions == y_val)
        accuracies.append(accuracy)
    
    # ä½¿ç”¨softmaxè®¡ç®—æƒé‡
    accuracies = np.array(accuracies)
    exp_acc = np.exp(accuracies * 5)  # æ¸©åº¦å‚æ•°=5
    weights = exp_acc / np.sum(exp_acc)
    
    return weights
```

---

## ğŸ’¡ ç®—æ³•ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®å±‚é¢ä¼˜åŒ–
- **æ•°æ®å¢å¼º**: æ—‹è½¬ã€ç¿»è½¬ã€å™ªå£°æ·»åŠ 
- **æ ·æœ¬å¹³è¡¡**: SMOTEã€ADASYNç­‰è¿‡é‡‡æ ·æŠ€æœ¯
- **ç‰¹å¾é€‰æ‹©**: ç›¸å…³æ€§åˆ†æã€é€’å½’ç‰¹å¾æ¶ˆé™¤
- **é™ç»´æŠ€æœ¯**: PCAã€MNFã€ICA

### 2. æ¨¡å‹å±‚é¢ä¼˜åŒ–
- **æ­£åˆ™åŒ–**: L1/L2æ­£åˆ™åŒ–ã€Dropout
- **æ‰¹å½’ä¸€åŒ–**: åŠ é€Ÿè®­ç»ƒæ”¶æ•›
- **å­¦ä¹ ç‡è°ƒåº¦**: ä½™å¼¦é€€ç«ã€æ­¥é•¿è¡°å‡
- **æ—©åœç­–ç•¥**: é˜²æ­¢è¿‡æ‹Ÿåˆ

### 3. è®¡ç®—å±‚é¢ä¼˜åŒ–
- **æ‰¹å¤„ç†**: æé«˜GPUåˆ©ç”¨ç‡
- **æ··åˆç²¾åº¦**: å‡å°‘å†…å­˜ä½¿ç”¨
- **æ¨¡å‹é‡åŒ–**: å‹ç¼©æ¨¡å‹å¤§å°
- **çŸ¥è¯†è’¸é¦**: å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹

### 4. å·¥ç¨‹å±‚é¢ä¼˜åŒ–
- **æ¨¡å‹ç¼“å­˜**: é¿å…é‡å¤è®¡ç®—
- **æµæ°´çº¿å¹¶è¡Œ**: æé«˜å¤„ç†æ•ˆç‡
- **åˆ†å¸ƒå¼è®­ç»ƒ**: å¤§è§„æ¨¡æ•°æ®å¤„ç†
- **æ¨¡å‹éƒ¨ç½²**: ONNXã€TensorRTä¼˜åŒ–

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### ç®—æ³•ç›¸å…³é—®é¢˜
- ğŸ“§ **ç®—æ³•å’¨è¯¢**: algorithm-support@example.com
- ğŸ“š **æŠ€æœ¯æ–‡æ¡£**: [ç®—æ³•è¯¦ç»†æ–‡æ¡£](https://algorithm-docs.example.com)
- ğŸ’» **ä»£ç ç¤ºä¾‹**: [GitHubç®—æ³•ç¤ºä¾‹](https://github.com/yourusername/algorithm-examples)
- ğŸ”¬ **ç ”ç©¶è®ºæ–‡**: [ç›¸å…³è®ºæ–‡åˆ—è¡¨](https://papers.example.com)

### å‚è€ƒæ–‡çŒ®

1. Melgani, F., & Bruzzone, L. (2004). Classification of hyperspectral remote sensing images with support vector machines. IEEE Transactions on Geoscience and Remote Sensing, 42(8), 1778-1790.

2. Belgiu, M., & DrÄƒguÅ£, L. (2016). Random forest in remote sensing: A review of applications and future directions. ISPRS Journal of Photogrammetry and Remote Sensing, 114, 24-31.

3. Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., & Benediktsson, J. A. (2019). Deep learning for hyperspectral image classification: An overview. IEEE Transactions on Geoscience and Remote Sensing, 57(9), 6690-6709.

4. Roy, S. K., Krishna, G., Dubey, S. R., & Chaudhuri, B. B. (2020). HybridSN: Exploring 3-Dâ€“2-D CNN feature hierarchy for hyperspectral image classification. IEEE Geoscience and Remote Sensing Letters, 17(2), 277-281.

---

*æœ¬ç®—æ³•æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæœ€åæ›´æ–°æ—¶é—´: 2024å¹´6æœˆ30æ—¥*